\documentclass[12pt]{amsart}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage[ansinew]{inputenc}
\usepackage{amssymb,amsmath,amsthm,hyperref,mathrsfs,array,graphicx,bbm,enumerate,wasysym,dsfont, tocloft}
%\usepackage{eucal}
\usepackage[all]{xy}
%\usepackage{bbm}
\usepackage{fullpage}
\usepackage{comment}
\usepackage[usenames, dvipsnames]{color}
\usepackage{textcomp}

\usepackage{titlesec}
\titleformat{\section}[display]
{\normalfont\Large\bfseries\center}{\textsc{Section}\,\thesection}{.5em}{}
\titleformat{\subsection}
{\normalfont\Large\center}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
{\normalfont\large}{\thesubsubsection}{1em}{}
\titleformat{\paragraph}[runin]
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titleformat{\subparagraph}[runin]
{\normalfont\normalsize\bfseries}{\thesubparagraph}{1em}{}
\newcommand{\sectionbreak}{\clearpage}
\setlength{\cftsecindent}{3em}

%\usepackage{subcaption}
%%%%%%%%%%%
%\textwidth 173mm \textheight 235mm \topmargin -50pt \oddsidemargin -0.45cm \evensidemargin -0.45cm

%%%%%%%%%%%
\newtheorem{thm}{Theorem}[section]
\newtheorem{exo}{Exercise}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{rem}[thm]{Remark}
\newtheorem{defn}[thm]{Definition}

\newtheorem{eg}[thm]{Example}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{claim}[thm]{Claim}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{con}[thm]{Conjecture}
\newtheorem{qst}[thm]{Question}
\newtheorem{assum}[thm]{Assumption}

\numberwithin{equation}{section}
\DeclareMathOperator{\Tr}{Tr}

\newcommand{\intR}{\tiny{\circledR}\int}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Z}{\mathbb Z}
\newcommand{\R}{\mathbb R}
\renewcommand{\H}{\mathbb H}
\newcommand{\Hh}{\mathcal H}
\newcommand{\Dd}{\mathcal D}
\newcommand{\HD}{\mathcal H_0^1([0,1])}
\newcommand{\N}{\mathbb N}
\newcommand{\Nn}{\mathcal N}
\newcommand{\D}{\mathbb D}
\newcommand{\Lc}{\mathcal L}
\newcommand{\E}{\mathbb E}
\newcommand{\F}{\mathcal F}
\newcommand{\G}{\mathcal G}
\newcommand{\CC}{\mathcal C}
\newcommand{\CB}{\mathbb C}
\newcommand{\Po}{\mathcal P}
\newcommand{\I}[1]{\mathbf{1}_{\left \{#1\right \}}}
\renewcommand{\P}{\mathbb P}
\renewcommand{\1}{\mathbf 1}
\newcommand{\ip}[1]{\langle #1 \rangle}

\newcommand{\ol}{\overline}
\newcommand{\A}{\mathds A}
\newcommand{\ABP}[2]{\rotatebox[origin=c]{180}{$#1\A$}}
\newcommand{\AB}{{\mathpalette\ABP\relax}}
\newcommand{\B}{\mathcal B}
\newcommand{\ED}{\operatorname{EL}}
\newcommand{\M}{\operatorname{M}}

\newcommand{\eps}{\epsilon}
\newcommand{\crad}{\text{crad}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\intt}{\text{Int}}
%\newcommand{\cll}{\text{Cl}}
%\newcommand{\sgn}{\text{sign}}
\newcommand{\CLE}{\text{CLE}}
\newcommand{\ALE}{\text{ALE}}
\renewcommand{\d}{{d}}
\newcommand{\question}[1]{{\color{blue}  #1}}
%\newcommand{\comment}[1]{{\color{red}  #1}}
%\setlength\parindent{0pt}

\title{Analysis IV for physics}
\newcommand{\juhan}[1]{{\color{ForestGreen} #1}} 
\newcommand\nmD[1]{\left\lVert#1\right\rVert_{\nabla}}
\newcommand\nmL[1]{\left\lVert#1\right\rVert_{L^2}}
\newcommand\nmH[2]{\left\lVert#1\right\rVert_{\Hh^{#2}}}
\newcommand\nm[1]{\left\lVert#1\right\rVert}


\author{Juhan Aru}
\begin{document}
\maketitle

%\setcounter{tocdepth}{1}
\setcounter{section}{-1}
%\tableofcontents 
%\makeatletter
%\let\toc@pre\relax
%\let\toc@post\relax
%\makeatother


\section{Introduction and motivation}

As motivation, let us consider the mathematical description of heat transmission on a homogeneous circular rod: the heat equation.

The heat equation on an interval $[0,1)$ (describing the rod) is given by describing the evolution of the temperature profile
$$\frac{\partial u(t,x)}{\partial t} = D \Delta u(t,x)$$ together with some initial condition $u_0(x) = u(0,x)$ and the boundary condition $u(t,0) = u(t,1)$ for all $t \geq 0$ to express that the ends of the rod are connected. Recall that in the 1D case $\Delta f := \frac{\partial^2 f}{\partial x^2}$ and $D > 0$ is the diffusion coefficient.

The revolutionary idea of Fourier was as follows. He noticed empirically that the heat profile over time shows spatially oscillatory behaviour, and thus also motivated by the solution of the wave equation using waves, he proposed to write any solution using spatially oscillating functions like $f_n(x) = \sin (2\pi nx)$ and $g_n(x) = \cos (2\pi n x)$. More precisely, one 
could try to find a solution of the form

$$u(t, x) = \sum_{n \geq 1} s_n(t) \sin(2\pi n x) + \sum_{n \geq 0} c_n(t) \cos(2\pi n x).$$

But now notice that $\Delta f_n = - 4 \pi^2n^2 f_n$ and thus if we try a solution of the form $u_n(x,t) = f_n(x) s_n(t)$ with $f_n$ as above, we obtain an equation 
$$ \frac{\partial s_n(t)}{\partial t} = - 4D \pi^2n^2 s_n(t).$$
This is a well-known ODE that is easily solved: $s_n(t) = \exp(-4D\pi ^2 n ^2 t) s_n(0)$. Similarly for the cos terms we get $c_n(t) = \exp(-4D\pi ^2 n ^2 t) c_n(0)$.

We conclude that it would make sense to propose a solution of the form
$$u(t, x) = \sum_{n \geq 1} s_n(0)\exp(- D4 \pi n^2 t) \sin(2\pi n x) + \sum_{n \geq 0} c_n(0) \exp(- D4 \pi n^2 t) \cos(2\pi n x).$$
Notice that the initial condition then translates to the condition:
$$u_0 =  \sum_{n \geq 1} s_n(0)\sin(2\pi n x) + \sum_{n \geq 0} c_n(0) \cos(2\pi n x).$$
If we do find such $(s_n(0), c_n(0))_{n \geq 0}$, then we may have found at least one solution to the heat equation on the circular rod. 

Now, this may sound very convincing, but on a closer look there are several questions here:
\begin{enumerate}
\item We have infinite sums - do they even converge? When do they converge and in which sense?
\item For which functions $u_0$ does the above-given expansion hold? In other words for which initial conditions can we find a solution by this method? 
\item Are such expansions unique? Are the solutions we find unique?
\item Can one approximate solutions? For example this is relevant when trying to numerically solve the equation. This is a question about convergence - and further, how does the notion of convergence relate to the coefficients $s_n, c_n$?
\item More generally, how should one measure closesness of different initial conditions, different solutions?
\item What happens for non-circular rods, e.g. rods with endpoints in heat-baths? Or in higher dimensions?
\item What about more non-homogeneous case where $D$ is no longer a constant in space? Or when we replace $\Delta$ with more general (linear) operators, including for example also some outside influences?
\end{enumerate}
The aim of this course is to study the right mathematical framework to ask and answer such questions. This will bring us to study function spaces, the Lebesgue integral and spectral theory of linear operators. To see why some of those aspects might enter let us further consider a simplified model.

\subsection{A discrete model}

To understand what we may hope to achieve, let us consider the same problem of heat diffusion but on a discretised space. For example we think that the rod instead is decomposed of $n$ small containers which can exchange heat between its neighbours.

The temperature profile is now given by $u(x,t) : \{0,1,\dots, n\} \to \R$, with the periodicity condition $u(0,t) = u(n,t)$ for all $t \geq 0$.

The evolution is still given by 
$$\frac{\partial u(t,x)}{\partial t} = K \Delta_d u(t,x)$$ together with some initial condition $u_0(x) = u(0,x)$, only instead of the real Laplacian, we have the discrete Laplacian $\Delta_d f(x) := \frac{1}{d_x}\sum_{y \sim x} f(y) - f(x)$, where $y \sim x$ means that $y, x$ are neighbours in the underlying discrete graph and $d_x$ is the number of neighbours of the vertex $x$. In our concrete case we have a circular graph with $n$ vertices and thus $\Delta_d f(x) := \frac{f(y) + f(z) - 2f(x)}{2}$, where $y, z$ denote the neighbouring vertices.

Now notice that the problem is really a system of $n$ coupled ordinary differential equations of second degree and $\Delta_d$ is just a linear operator on $\R^n \to \R^n$. So how do we solve it?

Let use the same steps as above but see that they have a very simple and concrete meaning here:
\begin{itemize}
\item Notice that each $u_t$ can be seen as a vector in $\R^n$ with coordinates and $\Delta_d$ can be seen as a symmetric linear operator on $\R^n$(check it!) 
\item As such $\Delta_d$ can be diagonalized: there is an orthonormal basis $\phi_1, \dots, \phi_n$ and eigenvalues $\lambda_1, \dots, \lambda_n$ such that $\Delta_d \phi_i = \lambda_i \phi_i$. In particular any function $u : \R^n \to \R^n$ can be uniqueley written as $\sum_{i = 1}^n c_i \phi_i$.
\item But now if we write $u_i(t) := c_i(t) \phi_i$, then again each $c_i(t)$ satisfies now a decoupled ODE $$ \frac{\partial c_i(t)}{\partial t} = K \lambda_i c_i(t)$$
and thus has a solution $c_i(0) \exp(K\lambda_i t)$.
\item We conclude a solution by finding $c_i(0)$ by determining the unique expansion $u_0 := \sum_{i=1}^n c_i(0)\phi_i$.
\item Given the uniqueness of the expansion, this solution is also unique.
\item And finally, we can easily compare solutions just using for example the Euclidean norm. For example conclude that if the initial conditions are close, then so will be the solutions at all times $t > 0$. We also know that this distance is equivalently measured using the distances between two sets of coefficients $(c_i)_{i = 1 \dots n}, (\tilde c_i)_{i = 1 \dots n}$ - and here using the Euclidean norm instead of some other norm is important.
\end{itemize}
Hence in this set-up all works super well and would work equally well as long as we have a symmetric linear operator $L$ instead of $\Delta_d$. 

What did we use here? 
\begin{itemize}
\item We used the fact that $\R^n$ is finite-dimensional and thus there exist basis that give unique expansions for each vector
\item We used the fact that $\Delta_d$ is linear and  symmetric and by the spectral theorem can be diagonalised and we can find a basis of eigenvectors 
\item We used implicitly the linearity of the equation
\end{itemize}

None of these facts are clear in our original set-up as the space of functions from $[0,1]$ to $\R$ is no longer finite-dimensional! 

To address those we will have to look at spaces of functions and try to first see which such spaces have a nice structure. For example, which spaces of functions satisfy linearity? Which can be define a norm and talk about orthonormality? For which spaces do we have orthonormal expansions? 
Looking for such nice properties brings us for example to also introduce the Lebesgue integral to construct nice basis of functions.

After that, having spent some time understanding function spaces, we briefly look at the study of linear operators on such spaces and in particular find some set-ups where there are similar orthonormal decompositions using eigenfunctions. We then put all this together to rigorously explain solving the inhomogeneous heat equation and other similar problems. 

But this is already enough of introduction, let us get going!

\section{The space of continuous functions}

Let us start with maybe the most intuitive of function spaces - the space of continuous functions. This is partly a recap, as you have been working with continuous functions in Analysis I-III, and we are just putting things in a wider context.

To start off the functions will be taking values on closed boxes $D \subseteq \R^n$, i.e. rectangles $[a_1, b_1] \times \dots \times [a_n, b_n]$) and taking values in $\R$. At the end of the section we will discuss to what extent we can (and may want to) generalize both of these choices. You may safely just suppose $D = [0,1]$, as no actual extra difficulty comes from going to higher dimensions.

The set of all continuous functions from $D \to \R$ will be denoted by $C(D, \R)$:
$$C(D, \R) := \{f: D \to \R, f \text{ continuous}\}.$$
In what follows we will try to understand the structure of this space.

\subsection{Vector space structure of $C(D,\R)$}

The first observation we can make about the space $C(D,\R)$ is that it has a linear structure like for example the vector space $(\R^n, +)$: if $f, g \in C(D, \R)$, then also the function $h(x) := f(x) + g(x)$ is in $C(D, \R)$, as is $\lambda f(x)$ where $\lambda \in \R$. 

Let us quickly check this for the first statement: for every $x \in D$, by continuity of $f, g$ we can choose $\delta_f, \delta_g$ such that if $y \in D, \|x-y\| < \delta_f$ then $|f(x) - f(y)| \leq \frac{\eps}{2}$ and if $y \in D, \|x-y\| < \delta_g$, then $|g(x) - g(y)| \leq \frac{\eps}{2}$. But this means that if $\|x-y\| < \min(\delta_f, \delta_g)$, we have that $|h(x)-h(y)| < \eps$ by the triangle inequality.

\begin{exo}
Show that in fact $C(D, \R)$ has also multiplicative structure: i.e. if $f, g \in C(D, \R)$, then also the product $h(x) := f(x)g(x)$ is in $C(D, \R)$. What about the function $\max(f,g)$?
\end{exo}

In fact, the space $C(D,\R)$ with addition satisfies all the axioms of a vector space! Indeed, the identity element would be just the constant zero function, the inverse element of $f$ the function $-f$ and all conditions are nicely met, as you can easily and patiently check.

\begin{exo}
Recall the axioms of a vector space and verify them in the case of $(C(D, \R), +)$.
\end{exo}

In what follows we will often also call the vector space just $C(D, \R)$.

Now we might be also interested in summing infinitely many functions, i.e. looking at sums $\sum_{n \geq 1}f_n$. But in what sense can we talk about it? More generally, given a sequence of $(g_n)_{n \geq 1}$ in which sense can we talk about its convergence and limit? 

The first idea might be to define limits pointwise: for each $x \in  D$ the sequence $(g_n(x))_{n \geq 1}$ is just a sequence of real numbers and thus we know what its convergence means. Thus we may want to define the convergence of $(g_n)_{n \geq 1}$ as functions to mean the convergence of $(g_n(x))_{n \geq 1}$ for all $x \in D$. This is called pointwise convergence and as you have already seen it suffers a small drawback:

\begin{exo}
For $D$ a closed box in $\R^n$, find a sequence of functions in $C(D, \R)$ that converges pointwise to a function that is not continuous.
\end{exo}

It is a good idea to start from the case $D = [0,1]$ (which we discussed in class), but then think how to do it in general.

We will look for other notions of convergence and to do this will introduce a norm on the set $C(D, \R)$.


\subsection{The uniform norm on $C(D, \R)$}

Recall that the vector space $\R^n$ comes also with several natural norms that give a notion of length of a vector and give us a way to measure distances between vectors. It comes out that one can also endow $C(D, \R)$ with a natural norm. 

\begin{defn}[The supremum (or uniform) norm]
For $f \in C(D, \R)$ we define its supremum (or uniform) norm $\|f\|_\infty := \sup_{x \in D}|f(x)|$.
\end{defn}

In this definition we use the fact that $D$ is closed and bounded - otherwise the supremum might not be finite. 

\begin{exo}
Find an example of $D$ that is not closed or not bounded, and $f \in C(D, \R)$ such that $\|f\|_\infty$ as defined above is infinite.
\end{exo}

We called the expression above a norm, but recall that a norm on a vector space has again a precise mathematical definition and its conditions need to be checked:

\begin{prop}
$\|f\|_\infty$ indeed defines a norm on the vector space $C(D, \R)$.
\end{prop}

\begin{proof}
We need to check the conditions for a norm.
\begin{enumerate} 
\item $\|f \|_\infty \geq 0$ with equality if and only if $f$ is equal to the constant zero function. This is clear.
\item $\|\lambda f\|_\infty = |\lambda| \|f\|_\infty$ is also clear.
\item Finally, we need to check the triangle inequality $\|f +g\|_\infty \leq \|f\|_\infty + \|g\|_\infty$.
We have 
$$\|f +g\|_\infty = \sup_{x \in D}|f(x) + g(x)| \leq \sup_{x \in D}(|f(x)|+|g(x)|)$$
by the triangle inequality.
But now 
$$\sup_{x \in D}(|f(x)|+|g(x)|) \leq \sup_{x \in D}|f(x)| + \sup_{x \in D}|g(x)|$$
and we conclude.
\end{enumerate}
\end{proof}

Thus $(C(D, \R), +, \|\cdot \|_\infty)$ is a normed vector space pretty much like $\R^n$ with any of these norms. This gives us a way to talk about convergence that is much more natural:

\begin{prop}
Let $(f_n)_{n \geq 1}$ be a sequence of $C(D, \R)$ functions converging to some function $f: D \to \R$ w.r.t. the uniform norm. Then in fact $f$ is continuous.
\end{prop}

This is a restatement of a result from Analysis I which says that pointwise limits of continuous functions are not continuous.

The proof technique is called the $3 \eps$ or $\eps/3$ argument and you have again seen it already in Analysis I. Let us give the proof just to understand what is now different from the earlier situation

\begin{proof}
It suffices to show that for every $x \in D$, we can find $\delta > 0$ such that $|f(x) - f(y)| < 3\eps$ whenever $\|x-y\| < \delta$. 

We can first choose a fixed $n \in \N$ large enough so that $\|f_n - f\|_\infty < \eps$, and in particular $|f_n(x) - f(x)| < \eps$ for every $x \in D$ by the definition (these are the first two epsilons). 

Further, by continuity of $f_n$ we can choose $\delta > 0$ such that for every $y \in D$ with $\|x-y\| < \delta$, we have that $|f_n(x) - f_n(y)| < \eps$ (this is the third epsilon). Putting things together using triangle inequality we obtain:
\begin{multline}
|f(x) - f(y)| = |f(x) - f_n(x) + f_n(x) - f_n(y) + f_n(y) - f(y)| \leq \\
\leq |f(x) - f_n(x)| + |f_n(x) - f_n(y)| + |f_n(y) - f(y)| < 3 \eps.
\end{multline}
\end{proof}

Notice that for pointwise convergence the first step fails: we can not necessarily choose an $n$ such that $\sup_{x \in D}|f_n(x) - f(x)| < 3 \eps$.

Thus using this norm the set $C(D, \R)$ is also closed under taking convergent sequences. In fact, it is even nicer than that and there are no gaps at all in the space, e.g. the space is complete - a notion you have met for $\R^n$ and that we recall here.

%Convergence under uniform norm is strictly stronger, as every sequence converging uniformly also converges pointwise. It comes out that it is sufficiently strong - the limits of sequences of continuous functions w.r.t. uniform norm are again continuous functions.

%\begin{exo}
%Is the uniform norm the only norm we can put on $(C(D, \R), +))$ such that the proposition above holds?
%\end{exo}

%This has as a consequence the completeness of the space $(C(D, \R), +, \|\cdot \|_\infty)$. 
%Recall first the meaning of completeness that you have met in $\R^n$:

\begin{defn}[Completeness of a normed space]
A normed space $(X, \| \cdot \|)$ is called complete if every Cauchy sequence $(x_n)_{n \geq 1}$ (i.e. every sequence such that for every $\eps > 0$, there is an $n_\eps$  with $\|x_n - x_m\| \leq \eps$ for all $n, m \geq n_\eps$) converges to an element $x \in X$.
\end{defn}

\begin{thm}
The space $(C(D, \R), +, \|\cdot \|_\infty)$ is a complete normed vector space.
\end{thm}

The idea is to use completeness of $\R$ to define a potential limiting function and then to verify that it really is that function. 

\begin{proof}
We only need to check the completeness. So let $(f_n)_{n \geq 1}$ be a Cauchy sequence in $C(D, \R)$. As for every $x \in D$, $(f_n(x))_{n \geq 1}$ is Cauchy and $\R$ is complete, we know a limit exists and we can denote this limit by $f(x)$. It remains to see that $f_n \to f$ in the uniform norm and that $f$ is continuous. The latter claim follows from the proposition above, so we need to just prove the convergence w.r.t. the uniform norm. This is left as an exercise on the exercise sheet.

%To do this notice that for every $n \in \N$ and every $x \in D$ we have that 
%$$|f_n(x) - f(x)| \leq \sup_{m \geq n}|f_n(x) - f_m(x)| \leq \sup_{m \geq n}\|f_n - f_m\|_\infty.$$
%But then by definition
%$$\|f_n - f\|_\infty \leq \sup_{m \geq n}\|f_n-f_m\|_\infty.$$
%However, the latter goes to zero as $n \to \infty$ because $f_n$ is Cauchy w.r.t uniform  norm by assumption.
\end{proof}

\begin{rem}
Mathematicians call any normed vector space that is complete a Banach space. Such spaces are quite important in setting up quantum field theory. 
\end{rem}

The completeness of the space has important application, one of them is finding solutions to ODEs via approximation. The tool used there is the Banach contraction mapping theorem that you have already met in Analysis II according to the course sheets and that is just recalled here:

\begin{thm}[Banach contraction mapping theorem]
Let $F: C(D, \R) \to C(D, \R)$ be contractive w.r.t. the uniform norm: $\|F(f) - F(g)\|_\infty < C\|f - g\|_\infty$ with $0 < C < 1$. Then there is a unique solution to $F(f) = f$ that can be obtained from the limit $\lim_{n \to \infty} F^{(n)}(f_0)$ starting from any $f_0 \in C(D, \R)$.
\end{thm}
%%% EXO: space-filling curve?

%\subsubsection{Convergent subsequences}

%In the subsection above we saw that if we have a Cauchy sequence in the space of continuous functions, then a limiting function exists and is also continuous. Often Cauchy criteria is not simple to check and one would like to see more of a theorem in the style of Bolzano-Weiestress, which says that a sequence of real numbers in some interval $[a,b]$ has always convergent subsequences. 

%First, it is clear that also in the case of functions boundedness is needed:

%\begin{eg}
%The sequence $f_n(x) = n$ does not have a convergent subsequence.
%\end{eg}

%But contrary to real numbers, just boundedness does not suffice as the following example illustrates:

%\begin{eg}
%Consider the set of functions $f_n(x) := x^n$ on the intervals $[0,1]$. We claim that it cannot be sequentially compact: the sequence $(f_n)_{n \geq 1}$ has no convergent subsequence in $C([0,1], \R)$. Indeed, notice that any pointwise limit of this sequence is just the function $f$ that is equal to $0$ on $[0,1)$ and equal to $1$ at $1$. As it is not continuous, it cannot be a uniform limit of continuous functions.
%\end{eg}

%Another example to have in mind is the following:

%\begin{exo}
%Show that the set of functions $f_n(x) := \sin(nx)$ defined for all $n \geq 1$ on $[0,1]$ does not admit converging subsequences.
%\end{exo}

%In both cases what goes wrong is the fact that the functions can change by some fixed amount, say $1/2$, over smaller and smaller intervals. One way to prohibit this is to ask for a certain equicontinuity of the family of functions, saying that that if two points $x, y$ in $D$ are very close, also the values of $f(x), f(y)$ have to stay close uniformly for all functions $f$. Let us state this properly:

%\begin{defn}[Equicontinuity]
%We call a subset $A \subseteq C(D, \R)$ equicontinuous if
%for every $x \in D$ and every $\eps > 0$, there exists a $\delta_x > 0$ such that whenever $f \in A$ and $\|x-y\| < \delta_x$, we have that $|f(x) - f(y)| < \eps$.
%\end{defn}

%\begin{prop}
%Suppose that a subset $A \subseteq C(D, \R)$ is sequentially precompact, i.e. has the property that every sequence $(f_n)_{n \geq 1}$ in $A$ has a convergent subsequence. Then the set of functions satisfies two properties:
%\begin{itemize}
%\item uniform boundedness: there exists a $C > 0$ such that $\|f\|_\infty \leq C$ for all $f \in A$
%\item equicontinuity: for every $x \in D$, every $\eps > 0$, there exists a $\delta_x > 0$ such that whenever $f \in A$ and $\|x-y\| < \delta_x$, we have that $|f(x) - f(y)| < \eps$.
%\end{itemize}
%\end{prop}

%In both cases we will use a proof by contrapositive and it is maybe the second case that is a bit more interesting.

%\begin{proof}
%Suppose $A$ is not uniformly bounded. Then there is a sequence of functions $f_n \in A$ with $\|f_n\| \geq n$. But such an $(f_n)_{n \geq 1}$ cannot have a convergent subsequence. Indeed, suppose for contradiction that some subsequence $(f_{n_k})_{k \geq 1}$ converges to some $\tilde f \in C(D, \R)$. Then for all $m$ large enough $\|f_m\|_\infty \leq 2 \|\tilde f\|$, giving a contradiction. We conclude that there is no convergent subsequence and thus uniform boundedness is necessary.

%Now suppose by contrapositive that $A$ is not equicontinuous. Then there is some $x \in D$, some $\eps > 0$, some functions $(f_n)_{n \geq 1} \in A$ and some points $(y_n)_{n \geq 1}$ such that $\|x-y_n\| \leq 1/n$, but $|f_n(x) - f_n(y_n)| \geq \eps$. Again, suppose for contradiction that there is a subsequential limit $f$ in the uniform norm for such a sequence. But then, by the three $\eps$ argument we obtain that $f(y_n)$ cannot converge to $f(x)$ although $y_n \to x$ as $n \to \infty$ and thus $f$ cannot be continuous, giving a contradiction. 
%\end{proof}

%The key theorem, called Arzela-Ascoli theorem. given without proof, now says that uniform boundedness and equicontinuity are in fact enough to guarantee existence of subsequences.

%\begin{thm}[Arzela-Ascoli]
%Suppose a closed set $A \subseteq C(D, \R)$ is uniformly bounded - i.e. $\exists c $ such that $\|f\|_\infty \leq c$ for all $f \in A$ - and equicontinuous. 

%Then $A$ every sequence in $A$ has a convergent subsequence in the uniform norm to some element in $C(D, \R)$. 
%\end{thm}

%\begin{rem}
%If $A$ is in addition closed, every limit will belong to $A$ and $A$ is called sequentially compact.
%\end{rem}

%The proof will be sketched in the exercises for those interested, but is not examinable. Let us look here at an important example:

%\begin{exo}
%Consider a set of functions $A \subseteq C([0,1], \R)$ such that
%each function $f \in A$ is bounded in absolute value by $1$, is differentiable and satisfies $|f'| \leq 2$. Show that the set that every sequence in $A$ admits a convergent subsequence.
%Indeed, by assumption the collection is uniformly bounded by $1$ and is equicontinuous: by the mean-value theorem $|f(x) - f(y)| \leq 2|x-y|$ for all $x, y \in [0,1]$ and all $f \in A$.
%\end{exo}

\subsection{Fourier series for continuous functions}

The expansion of a function $f$ on $[0,1]$ to a series of the form 
\begin{equation}\label{eq:fourierseries}
f(x) =  \sum_{n \geq 1} s_n \sin(2\pi n x) + \sum_{n \geq 0} c_n \cos(2\pi n x)
\end{equation}
is called the Fourier expansion or Fourier series. We saw in the introduction that it could be quite useful, but we didn't see any results on its existence / uniqueness. So let us look at this in the context of continuous functions $f$ now.

In fact we will see that these questions resolve themselves very smoothly once we find the "right functional space", but it is instructive to consider the questions already.

The first question is how should we go about finding the coefficients $s_n, c_n$? There the key is the following lemma.

\begin{lemma}\label{lem:orthtrig}
The following orthogonality relations hold for integers \(m, n \geq 0\):

1. Cosine-cosine Orthogonality:
\[
\int_0^1 \cos(2\pi n x)\cos(2\pi m x)\,dx = 
\begin{cases}
1, & \text{if } n = m = 0, \\[6pt]
\frac{1}{2}, & \text{if } n = m \neq 0, \\[6pt]
0, & \text{if } n \neq m.
\end{cases}
\]

2. Sine-Sine Orthogonality:
\[
\int_0^1 \sin(2\pi n x)\sin(2\pi m x)\,dx = 
\begin{cases}
0, & \text{if } n = 0 \text{ or } m = 0, \\[6pt]
\frac{1}{2}, & \text{if } n = m \neq 0, \\[6pt]
0, & \text{if } n \neq m.
\end{cases}
\]

3. Sine-Cosine Orthogonality:
\[
\int_0^1 \sin(2\pi n x)\cos(2\pi m x)\,dx = 0 \quad \forall n, m.
\]
\end{lemma}

\begin{proof}
    The proof is a simple consequence of trigonometric identities and their integrals and is left for the exercise sheet.
\end{proof}

The consequence of this observation is that if we expect the representation \eqref{eq:fourierseries} to hold in any nice sense, then the coefficients $s_n, c_n$ should be given by:

\begin{itemize}
\item Cosine Coefficients \(c_n\):
\[
c_n = 2 \int_0^1 f(x)\cos(2\pi n x)\,dx, \quad \text{for } n \ge 1.
\]

For the constant term \(c_0\), we have:
\[
c_0 = \int_0^1 f(x)\,dx.
\]

\item Sine Coefficients \(s_n\):
\[
s_n = 2 \int_0^1 f(x)\sin(2\pi n x)\,dx, \quad \text{for } n \ge 1.
\]
\end{itemize}

Further notice that if we want it to hold at the endpoints, then we better have $f(0) = f(1)$ as this also holds for every function in the series. 

Maybe a bit surprisingly both the existence and uniqueness are really not clear even for continuous functions!

Indeed, the understanding of counterexamples has evolved with time. The first observation is as follows
\begin{itemize}
\item There exists a continuous function $f$ satisfying $f(0) = f(1)$ whose Fourier series converges pointwise everywhere but does not  converge uniformly.  
\end{itemize}

It is not easy to come up with such a function but once given, it is easy to check (probably on the exercise sheet). 

A more stunning claim comes from the second half of 19th century from Du Bois-Reymond:
\begin{itemize}
    \item There exist continuous functions $f \in C([0,1], \R)$ with $f(0) = f(1)$ such that the Fourier series diverges at a point $x \in [0,1]$.
\end{itemize}
This was then extended by several people, including Kolmogorov to show that 
\begin{itemize}
    \item There are continuous functions $f \in C([0,1], \R)$ with $f(0) = f(1)$ where the Fourier series diverges at infinitely many or even dense set of points. 
\end{itemize}
Finally, Katznelson showed in 1970s that in fact 
\begin{itemize}
    \item For every continuous function $f$ and every $\eps > 0$, there is some continuous function $g$ with $\|g-f\| < \eps$ and the Fourier series of $g$ diverges at some point.
\end{itemize}
This means that these unpleasant functions are really everywhere!

There are two ways out of this. First, one could just try to restrict the set of functions that one is considering. Second, one could try to weaken further the notion of convergence and maybe give up having pointwise convergence. We will mainly concentrate on the second direction, as the first is too restrictive. But to finish this section let us still show how the first direction can give us some nice results:

\begin{prop}\label{prop:expansionC2}
Let $f \in C^2([0,1])$ be twice continuously differentiable and satisfying $f(0) = f(1)$ and $f'(0) = f'(1)$. Then its Fourier series
$$f(x) = \lim_{N \to \infty} \sum_{0 \leq n \leq N} (s_n \sin(2\pi nx) + c_n \cos(2\pi n x)),$$
converges w.r.t. $\| \cdot \|_\infty$. 
\end{prop}

\begin{rem}
In fact the result holds under much less stringent conditions, e.g. when the functions are so-called Holder continuous, i.e. satisfying $|f(x) - f(y)| < |x-y|^a$ for some $a > 0$. Just the proof then needs a bit more care and is out of the scope for us.
\end{rem}

The key ingredient is the following lemma, which we observed when guessing the solution to the heat equation and that really explains why Fourier series are so useful:
\begin{lemma}\label{lemma:diff}
Suppose that $f \in C([0,1], \R)$ is $k$ times continuously differentiable and satisfies $f^j(0) = f^j(1)$ for all $j = 0 \dots k-1$ \footnote{Here by $f^j(x)$ we mean the $j-th$ derivative of $f$ at $x$, the $0-$th derivative being the function itself.} Then there is some $C > 0$ such that for all $n \geq 1$
$|c_n| \leq Cn^{-k}$ and $|s_n| \leq Cn^{-k}$.
\end{lemma}

The full proof is on the exercise sheet, but let's see the case $k = 1$.
\begin{itemize}
\item We have by integration by parts that 
$$\int_{[0,1]}\sin(2\pi n x) f(x) dx =  \frac{1}{2\pi n}\int_{[0,1]} \cos(2\pi n x) f'(x) \leq \frac{1}{2\pi n}\|f'\|_\infty.$$
\end{itemize}
Let us proceed to the proof of the proposition.

\begin{proof}[Proof of Proposition \ref{prop:expansionC2}]

By the lemma we have that $|c_n|, |s_n| \leq Cn^{-2}$. 
Hence the Fourier series is Cauchy in the uniform norm. Indeed, if we denote by $S_M$ the partial series
$$S_M(f) =  \sum_{n \geq 0}^N (s_n \sin(2\pi n x) +  c_n \cos(2\pi n x)),$$
then by the triangle inequality for all $M > N$:
$$\|S_M(f) - S_N(f)\|_\infty \leq \sum_{N < n \leq M} (\|s_n \sin(2\pi n x)\|_\infty +  \|c_n \cos(2\pi n x)\|_\infty ),$$
but $\|\sin(2\pi n x)\|_\infty = \|\cos(2\pi n x)\|_\infty = 1$ and hence 
we can bound the sum by
$$2C\sum _{N < n \leq M}n^{-2} \leq 2C N^{-1},$$
which goes to $0$ as $N \to \infty$.
Hence as $C([0,1], \R)$ is complete for the uniform norm, we obtain the convergence to some continuous function $g$.

To conclude the theorem, we still need to argue that $f = g$. To do this we observe first (this is on the exercise sheet) that for all $n \geq 0$ 
$$\int_0^1 (f-g) \sin(2\pi n x) dx = \int_0^1 (f-g) \cos(2\pi n x) dx = 0.$$
It then follows from the next proposition that $g = f$.
\end{proof}

\begin{prop}\label{prop:uniqc}
Suppose $f$ is a continuous function on $[0,1]$. Then $s_n = 0, c_n = 0$ for all $n \geq 0$ if and only if $f(x) = 0$ for all $x \in [0,1]$. 

In particular, if the Fourier series of a function converges uniformly, then it is equal to the function itself and each function has at most one expansion in Fourier series.
\end{prop}

Before proceeding further, you should pause and think why this is not immediate.

In fact proving this proposition giving the means we have is not completely straightforward. We will later see how it becomes slick and swift once we have found the right functional space for the Fourier series, where each function has a unique series converging exactly w.r.t. to the norm of the space.

We will prove here the proposition modulo a key construction, that is given on the example sheet.

\begin{proof}
We want to show that $s_n = 0, c_n = 0$ for all $n \geq 0$ gives $f = 0$. We will argue by contradiction and show that if for some $x_0 \in (0,1)$ it holds that $f(x_0) \neq 0$, then there is a contradiction with the hypothesis of the proposition. 

The idea is to construct an approximations $T_{m,x_0}$ of the identity, or if you wish an approximations of the Dirac delta function $\delta_{x_0}$ using finite sums of sines and cosines and to argue that 1) on the one hand $\int_0^1 f(x) T_{m,x_0} dx \approx f(x_0)$ for $m$ large and 2) on the other hand by hypothesis $ \int_0^1 f(x) T_{m,x_0}dx = 0$ for all $m \geq 1$. The construction is recorded in the following lemma.

\begin{lemma}
For each $x_0 \in (0,1)$ one can construct a series of functions $T_{m,x_0}(x)$ as a linear sum of $\sin(2\pi x n), \cos(2 \pi x m)$ with $m,n \leq N$, i.e. by setting
$$T_{m,x_0}(x) = \sum_{n \leq N}(a_{n,x_0}\sin(2\pi n x) + b_{n,x_0}\cos(2\pi n x)$$
such that the following points hold.
\begin{enumerate}
    \item For every $m \geq 1$, $x \in [0,1]$ we have $T_{m, x_0}(x) \geq 0$
        \item For every $m \geq 1$ we have $\int_0^1 T_{m, x_0}(x) = 1$.
    \item For all $\delta > 0$: $\int_0^1 1_{|x-x_0| > \delta} T_{m, x_0}(x)dx \to 0$ as $N \to \infty$.
\end{enumerate}
\end{lemma}
Given such a sequence of $(T_{m, x_0})_{m \geq 1}$, we obtain the contradiction as follows.

\noindent On the one hand by the hypothesis for all $m \geq 1$ we have 
$$\int_0^1 f(x) T_{m, x_0}(x)dx = \sum_{n \leq m} \left( a_{n, x_0}\int_0^1 f(x) \sin(2\pi n x)dx + b_{n, x_0}\int_0^1 f(x) \cos(2\pi n x)dx\right)= 0.$$ 
On the other hand suppose $f(x_0) \neq 0$, say WLOG $f(x_0) > 0$. Then there is some $\delta > 0$ such that $f(x) > f(x_0) / 2$ in some region $[-\delta + x_0, x_0 + \delta]$.
Write
$$\int_0^1 f(x) T_{m, x_0}(x)dx = \int_{-\delta + x_0}^{\delta + x_0} f(x) T_{m, x_0} dx + \int_0^1 1_{|x-x_0| > \delta}f(x) T_{m,x_0}dx.$$
We can bound the second term in absolute value by
$$\|f\|_\infty \int_0^1 1_{|x-x_0| > \delta} T_{m, x_0}(x)dx,$$
which goes to zero by the lemma. The first term however  can be bounded from below by $f(x_0)/2 \int_{-\delta + x_0}^{\delta + x_0} T_{m, x_0}(x)dx$. Combining the conditions of Lemma, we see that for $m$ large enough this integral is larger than say $1/2$ and thus the whole term is larger than $f(x_0)/4$ for $m$ large enough. And in particular we conclude that $\int_0^1 f(x) T_{m, x_0}(x)dx \neq 0$ for $m$ large enough! This gives a contradiction. But our assumption was that $f(x_0) \neq 0$, so this cannot hold and we conclude the proposition.
\end{proof}

This was in the end not hard, but quite a fiddly proof and moreover also the existence of Fourier series for continuous functions had several delicate points. We would prefer if the existence and uniqueness would be simple consequences of a good set-up, like in the case of $\R^n$. With this in mind, we will go towards larger function spaces.


\section{Lebesgue measure and Lebesgue integral on $\R^n$}

We will continue our aim of constructing a convenient / appropriate function space for the Fourier expansions. Motivated by the finite-dimensional example, we would want to construct a space of functions with an inner product of the type $\int f(x)g(x) dx$ and then see the Fourier series as an orthogonormal basis of this space.

To do this, we will have to make a detour and renew our understanding of two intimately linked notions: 1) the integral of a function 2) the size / measure of subsets of $\R^n$. 

But let us start off by discussing why the Riemann integral does not suffice.

%We will start with some problems of the Riemann integral, then give the basic idea for the Lebesgue integral. This will lead us to deal with Lebesgue measure. We will finish the section by then returning to the rigorous definition and properties of the Lebesgue integral.

\subsection{An issue with the Riemann integral}

One way to define the Riemann integral of a function $f: [0,1] \to \R$ is as follows.
 
\begin{enumerate}
\item We subdivide $[0,1]$ into $2^n$ equal disjoint intervals $D_{n,i} = [i2^{-n}, (i+1)2^{-n}]$ each of size $2^{-n}$;
\item We call a function Riemann integrable if $U_n := 2^{-n} \sum_{i = 0}^{2^n-1} \sup_{x \in D_{n,i}} f(x)$ (which is decreasing) and $L_n := 2^{-n} \sum_{i = 0}^{2^n-1} \inf_{x \in D_{n,i}} f(x)$ (which is increasing) both converge to the same limit.
\item We define the Riemann integral of $f$, that from now on we denote for clarity by $\intR_0^1f(x)dx$ to be equal to this limit.
\end{enumerate}

It is easy to see that Riemann integral satisfies some nice properties: 

\begin{exo}
Show that the Riemann integral satisfies some desirable properties:
\begin{itemize}
\item All continuous functions on $[0,1]$ are Riemann integrable 
\item Every function $f$ that changes value finitely many times is Riemann integrable 
\item Linearity: If $f, g$ are Riemann integrable on $[0,1]$, then so is their sum and the integral is equal to the sums.
\end{itemize}
\end{exo}

%for example all  - indeed, in this case 
%$$\sup_{i=0, \dots, 2^n} |\sup_{x \in D_{n,i}} f(x)- \inf_{x \in D_{n,i}} f(x)| \to 0$$
%as $n \to \infty$.

However, the Riemann integrability does not behave well under limits or infinite sums. Indeed, consider a enumeration $q_1, q_2, \dots$ of all rational numbers in $[0,1]$ (can you give a concrete one?) and define $f_n(x) = 1$ if $x \in \{q_1, \dots, q_n\}$ and $f_n(x) = 0$ otherwise. Then each $f_n$ is Riemann-integrable (with $\intR_0^1 f_n(x) = 0$) by the exercise above, but the limit is not Riemann integrable as in every interval the sup is equal to $1$ and inf is equal to $0$ and thus $U_n = 1$ for all $n \geq 1$ and $L_n = 0$ for all $n \geq 1$. 

We will see how this is remedied with the notion of Lebesgue integral.

%\subsection{The idea of the Lebesgue integral}

%Recall the idea of the Riemann integral: if $f(x)$ is positive then the integral corresponds to the area between the graph of $f$ and the horizontal axis. We can approximate this area by slicing the area under the graph vertically: we subdivide the horizontal axis into small intervals $[k2^{-n}, (k+1)2^{-n})$ of size $2^{-n}$ and assigning to each such interval a box of height $f(k2^{-n})$ for example. Riemann integrability guarantees that we could have chosen to evaluate $f$ at any point in in the interval. The sum of the areas of such boxes then approximates our integral. (Do the drawing!)

%Another way to think of this is to say that we approximate $f$ using step functions $\tilde f_n$ whose value is constant over each interval of the form $[k2^{-n}, (k+1)2^{-n})$, i.e. is given by $\tilde f_n(x) = f(2^{-n}\lfloor x2^n \rfloor)$. For each of these functions the integral is defined as $2^{-n}\sum_{k \in \{0, 1, \dots, 2^n-1\}} f(k2^{-n})$ and the Riemann integral of $f$ is the limit of such sums.  \\

%In short the idea of Lebesgue integral is to use a similar approximation, but instead of slicing vertically, we slice horizontally; i.e. instead of cutting the horizontal axis to bits we will subdivide the vertical axis to bits where the function takes values in $[k2^{-n}, (k+1)2^{-n})$.

%This is maybe easier to see from the approximating function point of view. We approximate still $f$ by step functions, but now the steps happen when the function changes value not after every predetermined interval. More precisely, we can approximate $f(x)$ from below by $f_n(x) := 2^{-n}\lfloor f(x)2^n \rfloor$. The approximate integral would now be equal to 
%$$\sum_{k \in \Z} k 2^{-n}\text{size}(x: f(x) \in [k2^{-n}, (k+1)2^{-n})$$
%and the Lebesgue integral would be the limit of those as $n \to \infty$. 

%This is all very nice, but there is a problem! How do we measure $\text{size}(x: f(x) \in [k2^{-n}, (k+1)2^{-n})$? How do we even know that it is always well defined? For which $f$ can it be defined? To answer these questions we will define the so called Lebesgue measure on $\R^n$, which will generalize / revisit the notion of volume in $\R^n$.

\subsection{The Lebesgue measure}

We start however by revisiting the notion of size / volume / measure of subsets of $\R^n$. This is directly related to the integral as even in the case of Riemann integral, if the set $A \subseteq \R$ is nice enough then $\int_\R 1_A(x) dx = \text{size}(A)$. What should nice enough be is one of the main questions.

As said, the Lebesgue measure on $\R$ generalizes the notion of length and assigns each permissible subset of $\R$ a size. More formally, the Lebesgue measure is a function $L: \F \to [0, \infty)$, where $\F$ is some collection of subsets of $\R$ satisfying some collection of properties. What should such natural properties be?

\begin{enumerate}
\item First, in the case of $\R$, we would like the length / measure of each interval $[a,b]$, $(a,b)$, $[a,b)$ or $(a,b]$ to be just $b-a$. In particular each point $\{x\}$ should have length $0$
\item We certainly would want also $L(\emptyset) = 0$ and $L(A) \geq 0$ for all $A \in \F$.
\item Second, we would like measure to satisfy some additivity properties. For example the size of the union of two disjoint sets should clearly be just the sum of their sizes: i.e. in symbols $L(A_1 \cup A_2) = L(A_1) + L(A_2)$. By induction this should hold for any finite number of disjoint intervals: $L(A_1 \cup \dots \cup A_n) = L(A_1) + \dots L(A_n)$.
\item Further, it might make sense for this additivity to hold also if we have countably many disjoint sets? But attention! We cannot ask it for all infinite unions: indeed, for example $[0,1]$ can be seen as a disjoint union of all points $\{x\}$ in $[0,1]$, but the sum of their lengths would be $0$ whereas the measure of $[0,1]$ has to be clearly $1$!

\end{enumerate}

Observe that only the first property has something specific to do with $\R$, all the others are of very abstract nature.
A big breakthrough by Lebesgue was to understand that combining these properties gives the right mathematical framework to talk of size / measure on any set! This is encapsulated in the following general definition:

\begin{defn}[Measure space, Borel 1898, Lebesgue 1901-1903]
A measure space is a triple $(\Omega, \F, \mu)$, where
\begin{itemize}
	\item $\Omega$ is a set, called the sample space or the universe.
	\item $\F$ is a set of subsets of $\Omega$, satisfying:
	\begin{itemize}
		\item $\emptyset\in \F$;
		\item if $A \in \F$, then also $A^c \in \F$;  
		\item If $A_1, A_2, \dots \in \F$, then also $\bigcup_{n \geq 1} A_n \in \F$.
	\end{itemize}
$\F$ is called a $\sigma$-algebra and any $A \in \F$ is called a measurable set.
	\item And finally, we have a function $\mu : \F \to [0, \infty]$ satisfying $\mu(\emptyset) = 0$ and countable additivity for disjoint sets: if $A_1, A_2, \dots \in \F$ are pairwise disjoint, $$\mu(\bigcup_{n \geq 1}A_n) = \sum_{n \geq 1}\mu(A_n).$$ This function $\mu$ is called a measure. If $\mu(\Omega)  < \infty$, we call $\mu$ a finite measure.
\end{itemize} 
\end{defn}
\noindent Geometrically we interpret:
\begin{itemize}
    \item $\Omega$ as our space of points
    \item $\F$ as the collection of subsets for which our notion of volume can be defined
    \item $\mu$ our notion of volume: it gives each measurable set its volume.
\end{itemize}


We can define a measure on any set of points, finite or infinite. Some telling examples are:

\begin{eg}[Counting measure]
 On any set $\Omega$ one can define the counting measure $\mu_c$: we set $\F := \Po(\Omega)$ (the set of all subsets), and $\mu_c(\{\omega\}) := 1$ for any $\omega \in \Omega$. For any finite set $E$, $\mu_c(E)$ gives its number of elements. If $E$ is infinite, then so is $\mu_c(E)$. In particular, if $\Omega$ is an infinite set, then $\mu_c(\Omega) = \infty$, so this is a measure, but not a finite measure.
\end{eg}

Notice that on a space with finite number of points it gives the natural uniform measure - each point is treated in the same way. However, it is not the natural measure of size on say $\R$ as the size of say $[0,1]$ would be infinite. The natural uniform measure on $[0,1]$ or $\R$ will be called the Lebesgue measure, but its existence is already mathematically non-trivial - we will come to this in a bit.

\begin{eg}[Delta measure]
The (Dirac) delta function that you have seen mentioned in the courses, is actually a measure, not a function and can be defined on any space and for any $\sigma-$algebra that contains points. On any set $\Omega$ one can define the Dirac delta measure $\mu_x$ at the point $x$ as follows: suppose $\F$ contains points and we set $\mu_x(\{x\}) = 1$ and more generally $\mu_x(F) = 1$ if $x \in F$ and $\mu_x(F) = 0$ otherwise, for every $F \in \F$. In particular, rigorously the delta function is defined as a measure and not a function.

We will come back to this and its connection to the 'Dirac delta function' you have seen before later on.
\end{eg}

Finally, a both nice and important aspect of the framework of measure spaces is that it also gives the mathematical basis for probability theory - this was observed by A. Kolmogorov some 30 years after the introduction of measure spaces! A probability space is a measure space with total mass equal to $1$, i.e. $\mu(\Omega) = 1$. In that case we often use the notation of $\P$ for the measure $\mu$. The framework of probability is used for observing / measuring what's going on in the world:
\begin{itemize}
\item  $\Omega$ as the space of all microstates / all possible outcomes; e.g. the states of the atmosphere
\item $\F$ is the collection of observable events / outcomes: i.e. subsets of microstates, whose happening or not happening can be observed; for example we can maybe only measure macroscopic parameters like temperature, or the amount of rain over an hour
\item The measure $\P$ will assign a number in $[0,1]$, called probability, to each observable event. Those events that surely happen, get probability $1$.
\end{itemize}

\begin{eg}
The probability space for describing a fair coin toss would be $$(\{H, T\}, \{\emptyset, \{H\}, \{T\}, \{H,T\}\}, \P),$$ where $\P(\{H\}) = \P(\{F\}) = 1/2$. 

The probability space for describing a fair dice would be $$(\{1,2,3,4,5,6\},\Po(\{1,2,3,4,5,6\}), \P),$$ where we define $\P(F) = |F|/6$. If instead we paint all the faces 1,2,3,4,5 black so they become indistinguishable, we can modify our model by taking $\F = \{\emptyset, \{1,2,3,4,5,6\}, \{1,2,3,4,5\}, \{6\}\}$ and using the probability measure $\widetilde \P$ defined only on these subsets, still with the same formula as above.
\end{eg}

\begin{exo}
Find a measure space to describe two unrelated fair coin tosses. What assumptions are you making in giving the description? Define a sigma-algebra suitable for studying the situation where one can only ask if the two coins have the same side up, or different sides up.
\end{exo}

Finally, as mentioned not all natural measure spaces are simple to define. We already mentioned that the natural "uniform" measure on $[0,1]$ or $\R$ needs some work. But one would actually also want to define natural measures on more complicated structures like the space of all continuous functions - indeed, this gives one way to formalize path-integrals in quantum mechanics. This was achieved by Wiener in the beginning of 20th century; the similar task for string theory, i.e. defining probability measures over surfaces with different metric structures has been partially resolved only in the recent years.

\subsubsection{Basic properties of measure spaces}

Before discussing the Lebesgue measure let us play aroud a bit with the notion of a measure space. 

First, the following lemma helps to see which other sets would be measurable:

\begin{lemma}[Constructing more measurable sets]\label{lem:meassets}
	Consider a set $\Omega$ with a $\sigma$-algebra $\F$. 
	\begin{enumerate}
		\item If $A_1, A_2, \dots, \in \F$, then also $\bigcap_{n \geq 1} A_n \in \F$. 
		\item Then also $\Omega \in \F$ and if $A, B \in \F$, then also $A \setminus B \in \F$.
		\item For any $n \geq 1$, if $A_1, \dots, A_n \in \F$, then also $A_1 \cup \dots \cup A_n \in \F$ and $A_1 \cap \dots \cap A_n \in \F$.
		
	\end{enumerate}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lem:meassets}]
	By de Morgan's laws for any sets $(A_i)_{i \in I}$, we have that 
	$$\bigcap_{i \in I} A_i = (\bigcup_{i \in I} A_i^c)^c.$$
	Property (1) follows from this, as if $A_1, A_2, \dots \in \F$, then by the definition of a $\sigma$-algebra also $A_1^c, A_2^c, \dots \in \F$ and hence
	$$(\bigcup_{i \geq 1} A_i^c)^c \in \F.$$
	For (3), again by de Morgan laws, it suffices to show that $A_1 \cup \dots \cup A_n \in \F$. But this follows from the definition of a $\sigma$-algebra, as 
	$A_1 \cup \dots \cup A_n = \bigcup_{i \geq 1} A_i$ with $A_k = \emptyset$ for $k \geq n+1$.
	Finally, for (2) we can just write $\Omega = \emptyset^c$. 
 
 The fact that $A \setminus B \in \F$ is left as an exercise. %Moreover, writing $A \backslash B = A \cap B^c$, we conclude by using (3).
\end{proof}

The statements are also very intuitive at least in the context of probability: e.g. the first one says that if we can observe if some events $A_1, A_2, \dots$ happen, then we can observe if they all happen at once; the second property says that if two events can be observed, then we can always also observe if one of them happened but not the other one. 

In a similar vein, the basic conditions on the measure, give rise to several natural properties too:

\begin{prop}[Basic properties of a measure and a probability measure]\label{prop:propmeas}
	Consider a measure space $(\Omega, \F, \mu)$. Let $A_1, A_2, \dots \in \F$. Then
	\begin{enumerate}
		\item For any $n \geq 1$, and $A_1, \dots, A_n$ disjoint, we have finite additivity $$\mu(A_1) + \dots + \mu(A_n) = \mu(A_1 \cup \dots \cup A_n).$$
		In particular if $A_1 \subseteq A_2$ then $\mu(A_1) \leq \mu(A_2)$.
		\item If for all $n \geq 1$, we have $A_n \subseteq  A_{n+1}$, then as $n \to \infty$, it holds that $\mu(A_n) \to \mu(\bigcup_{k \geq 1} A_k)$. 
		\item We have countable subadditivity (also called the union bound): $\mu(\bigcup_{n \geq 1}A_n) \leq \sum_{n \geq 1} \mu(A_n)$.
	\end{enumerate}
	If in fact $\mu(\Omega)$ is finite (e.g. a probability measure), we further also have the following properties:
	\begin{enumerate}
		\setcounter{enumi}{3}
		\item For any $A \in \F$, we have that $\mu(A^c) = \mu(\Omega) - \mu(A)$.
		\item If for all $n \geq 1$, we have $A_n \supseteq  A_{n+1}$, then as $n \to \infty$, it holds that $\mu(A_n) \to \mu(\bigcap_{k \geq 1} A_k)$. 
	\end{enumerate}
	
\end{prop}

Again, please do check that all these properties also make sense intuitively!

\begin{proof}[Proof of Proposition \ref{prop:propmeas}]

Finite additivity follows from countable additivity by taking $A_k = \emptyset$ for $k \geq n+1$. 

(2), (3) are left as exercises.
%Moreover, by writing $A_2$ as a disjoint union $A_2 = A_1 \cup (A_2 \cap A_1^c)$, we have from disjoint additivity and non-negativity of measures:
%$$\mu(A_2) = \mu(A_1) + \mu(A_2 \cap A_1^c) \geq \mu(A_1).$$

%For (2), write $B_1 = A_1$ and for $n \geq 2$, $B_n = A_n/A_{n-1}$. Then $B_n$ are disjoint, $\bigcup_{n = 1}^N B_n = A_N$ and $\bigcup_{n \geq 1} B_n = \bigcup_{n \geq 1} A_n$. 

%Thus by countable additivity 
%$$\mu(\bigcup_{i \geq 1} A_i ) = \mu(\bigcup_{i \geq 1} B_i ) = \sum_{i \geq 1}\mu(B_i)$$
%But $\mu$ is non-negative, so
%$$\sum_{i \geq 1}\mu(B_i) = \lim_{n \to \infty}\sum_{i = 1}^n \mu(B_i)$$
%By countable additivity again
%$$\sum_{i = 1}^n \mu(B_i) = \mu(\bigcup_{i = 1}^n B_n) = \mu(A_n)$$ 
%and (2) follows.

%To prove countable subadditivity, write similarly $B_1 = A_1$ and for $n \geq 2$ $$B_n = A_n \backslash \bigcup_{k = 1}^{n-1} A_k.$$ 
%Then $B_n$ are disjoint with $\bigcup_{n = 1}^N B_n = \bigcup_{n = 1}^N A_n$ and moreover $B_n \subseteq A_n$.
%Thus by disjoint additivity and point (1) we have
%$$\mu(\bigcup_{n = 1}^N A_n) = \mu(\bigcup_{n = 1}^N B_n) = \sum_{n=1}^N \mu(B_n) \leq \sum_{n=1}^N \mu(A_n).$$
%Now,  taking limits as $N \to \infty$ and using (2) to get the limit on the left gives (3).

For (4), we just notice that $A$ and $A^c$ are disjoint and $A \cup A^c = \Omega$. Thus by disjoint additivity $\P(A) + \P(A^c) = 1$.
Finally, for (5), define $B_n = A_n^c$. Then $\P(A_n) = \P(B_n^c) = 1 - \P(B_n)$. Similarly $\P(\bigcap_{k \geq 1} A_k) = 1-\P(\bigcup_{k \geq 1} B_k)$.
Thus the result follows from (2).
The rest is left as an exercise
\end{proof}

\subsubsection{The Lebesgue measure}

The Lebesgue measure is the right notion uniform measure on the spaces $\R^n$ (or say a unit cube $[0,1]^n$ or a ball). This measure is called uniform because it is isotropic, i.e. it treats all the points in the set equally. More formally, it is up to a multiplicative constant the measure $\mu$ such that $\mu(A) = \mu(\lambda + A)$, where $A$ is some measurable set and $\lambda + A := \{a + \lambda: a \in A\}$. 

To define the uniform measure on $\R^n$, we should first pick the right $\sigma-$algebra. First, it certainly has to be big enough to contain at least all the boxes. Now, it is an interesting fact that in the standard axiomatization of mathematics \footnote{Meaning that we assume the axiom of choice} one cannot take the $\sigma-$-algebra to be equal to $\Po(\R^n)$ - otherwise one runs into contradictions as explained in the non-examinable part of the example sheet. However, there are some $\sigma-$algebras that are big enough to contain all sets we might be interested in and small enough at the same time to create no contradiction.

\begin{defn}[Borel $\sigma$-algebra]
The Borel $\sigma$-algebra $\F_B$ on $\R^n$ is defined as the smallest $\sigma-$algebra containing all boxes, i.e. all sets of the form $\Pi_{i=1}^n [a_i, b_i]$ with real numbers $a_i < b_i$.
\end{defn}

This definition hides a claim: the fact that such a smallest $\sigma-$algebra exists. However, it is a simple but not that illuminating exercise to show that an arbitrary intersection of $\sigma-$algebras is a $\sigma-$algebra and thus the smallest has a well-defined meaning. It is maybe more interesting to see what it contains, i.e. what we can measure \footnote{It is maybe as interesting to see that there are sets in the power-set of $\R^n$ that do not belong to the Borel $\sigma-$algebra. However, describing them explicitly is not that easy - if interested, see the for fun section on the example sheet.}:

\begin{eg}
The Borel $\sigma-$algebra contains for example all points, i.e. sets of the form $\{x\}$: indeed, we can write
$$\{x\} = \bigcap_{m \geq 1} (\{x\} + [-m^{-1},m^{-1}]^n). $$
\end{eg}

\begin{exo}
Show that the Borel $\sigma-$algebra on $\R^n$ also contains  all products of half-lines $\Pi_{i=1}^n (-\infty, a_i]$, all open balls $B(x, r)$ and in fact all open sets of $\R^n$
\end{exo}

The main theorem of this section is then the following result, that we assume without proof:

\begin{thm}[Existence and uniqueness of Lebesgue measure]
There is a unique measure $\lambda$ defined on $(\R^n, \F_B)$ such that the measure of each box $\Pi_{i=1}^n [a_i, b_i]$ is given by $\Pi_{i=1}^n (b_i - a_i)$.
\end{thm}

Some other nice properties of the Lebesgue measure follow from this theorem:
\begin{itemize}
\item It is translation invariant: for every set $A \in \F_B$, if we denote by $A+b$ the set $\{a+b: a \in A\}$, then the Lebesgue measure $\lambda$ satisfies $\lambda(A) = \lambda(A+b)$. Indeed, denote by $\widetilde \lambda(A) := \lambda (A +b)$. This defines another measure on $(\R^n, \F_B)$ such that $\widetilde \lambda (\text{box})$ equals the volume of the box. Thus by uniqueness part of the theorem we obtain $\widetilde \lambda = \lambda$ and hence $\lambda(A +b) = \lambda(A)$ for all Borel sets $A$.
\item It can be also proved that the Lebesgue measure is rotation invariant: for every set $A \in \F_B$, if we denote by $R(A)$ the set rotated by the rotation matrix $R$, then the Lebesgue measure $\lambda$ satisfies $\lambda(A) = \lambda(R(A))$.
\end{itemize}

Maybe somewhat surprisingly the proof of this natural theorem is not immediate. The problem is the following: it is simple to assign measure to each box, or each finite union of disjoint boxes etc...however, the Borel $\sigma$-algebra is much richer than that. Indeed, there are sets in the Borel $\sigma$-algebra that one cannot obtain in a finite number of steps by starting with boxes and taking iteratively unions, intersections and complements in any order.
Hence the fact that one can assign a measure to all Borel sets in a way that the axioms are satisfied and boxes have the right size is not immediate. Also the statement of uniqueness is non-evident for the same reason - why should equality for all boxes imply it for all Borel sets?

The proof goes beyond the scope of this course, but here is the sketch for one of the possible approaches for those interested (not examinable). \\

\noindent $\star $ \textit{Start of non-examinable section} $\star $\\

For any rectangle $R = \Pi_{i=1}^n [a_i, b_i]$, let's denote by $|R|$ its natural volume $\Pi_{i=1}^n (b_i - a_i)$.
\begin{enumerate}
\item First, we define for any set $A \subseteq \R^d$ a notion of size called the exterior measure: $m^*(E) := \inf \sum_{i = 1}^\infty |R_i|$, where the infimum is over all coverings of the set $E$ using rectangles - this gives a certain approximation of size from above. 

Notice that from this definition it is not immediate that even $m^*(R) = |R|$, but that can be argued for both closed and open rectangles. Also, it is important that we allow for countably many rectangles - see exercise sheet. 
\item It comes out that showing all the axioms of the measure for all subsets of $\R^d$ is impossible \footnote{as long as one assumes the Axiom of Choice}. So now comes the key idea of choosing a subclass of sets which is large enough to contain Borel sets, but small enough to be able to make everything work: we call a set measurable if for every $\eps > 0$, there is some countable collection of rectangles $(R_i)_{i \geq 1}$ such that $E \subseteq \bigcup_{i \geq 1} R_i$ and $m^*(E \triangle (\bigcup_{i \geq 1}R_i)) < \eps$. This means that our earlier approximation from above can be chosen to fit well.
\item It then remains to argue that these sets actually form a $\sigma-$algebra and that all axioms are satisfied for ($(\R^d, \F_L, m^*)$. In fact they form a $\sigma-$algebra, called the Lebesgue $\sigma-$algebra $\F_L$, that is even larger than $\F_B$!

This final step doesn't require any big theorems or inputs, but it does require quite a bit of care in setting up the order of the argument. It is then an easy conclusion that $\F_B \subseteq \F_L$, as $\F_B$ can be generated from just rectangles and we can conclude.
\end{enumerate}
\noindent $\star $ \textit{End of non-examinable section} $\star $\\

\begin{eg}
The Lebesgue measure of a point is zero: indeed for every $\eps > 0$, we have that $\lambda(\{x\}) \leq \lambda(\{x\}+[-\eps, \eps]^n) = (2\eps)^n$, which can be made arbitrarily small.

Hence also the measure of all rational numbers is zero: we have by countably additivity $\mu(\Q) = \sum_{q \in \Q} \mu(\{q\}) = 0$.
\end{eg}

\begin{exo}
Show that the Lebesgue measure of $\R^n$ is infinite and that the Lebesgue measure of the line segment $[0,1] \times \{0\} \dots \times \{0\} \subseteq R^n$ is zero. 

Now consider the Lebesgue measure on $\R$. Prove that the measure of irrational numbers contained in $[0,R]$ is equal to $R$; prove also that the Lebesgue measure of the Cantor set is zero.
\end{exo}

%\begin{exo}
%Show that there is no way to cover all rational numbers in $[0,1]$ with finitely many closed intervals so that their total length is less than $1$. Give a precise construction of how to find a covering with countably many intervals such that the total length is less than $\eps$.
%\end{exo}

\subsection{Lebesgue integral}

Recall our grand plan was to construct a function space which has a nice inner product of the form $\int f(x)g(x)dx$ and all the nice properties of a function space like linearity, closedness under limits and completeness. With Riemann integral this would never be possible, as we saw it does not behave that well under taking limits. Hence let us see another notion of integral, called the Lebesgue integral. To start off, let's see that defining a measure always gives us a natural space of functions and those will be the candidates for defining the integral for.

\subsubsection{Measurable functions}

Each measure space comes with a class of natural functions, called measurable functions. These will also form the class of functions for which we aim to define the Lebesgue integral.

We will constrain ourselves to working with functions from $\R^n \to \R$, although the notion of a "measurable" function is quite a bit larger, applying to maps between any two sets together with $\sigma-$algebras; in our case these would be the pairs $(\R^n, \F_B)$ and $(\R, \F_B)$, where in both cases we consider the Borel $\sigma-$algebra. \\

The simplest measurable functions (on $(\R^n, \F_B)$) are those given by characteristic functions $1_{E}$ for some Borel-measurable set $E \in \F_B$, i.e. functions that tell us whether $x$ is in a set - then $1_{E}(x) = 1$ - or not, in which case $1_{E}(x)= 0$. Their countable linear combinations are called simple functions:

\begin{defn}[Simple functions]
Let $E_1, E_2, \dots$ be disjoint Borel sets in $\R^n$ and $c_1, c_2, \dots$ real numbers. Then a function of the form $f(x) = \sum_{i \geq 1} c_i 1_{x \in E_i}$ is called a simple function.
\end{defn}
We can then define

\begin{defn}[Measurable function]
We call a function $f: \R^n \to \R$ measurable if it is a pointwise limit of simple functions.
\end{defn}

This definition is natural, however it is not so easy to work with. So let us start by proving an equivalence with another rather nice definition.

\begin{prop}\label{prop:eqmeas}
A function $f: \R^n \to \R$ is measurable if and only if for every $a < b$ the preimage $f^{-1}([a,b)))$ is Borel measurable. 
\end{prop}
We will sometimes call this condition the preimage condition.

The proof consists of two lemmas, one for each direction both teaching us something about measurable functions:

\begin{lemma}\label{lem:limitsmeasurable}
Suppose that the sequence of functions $(f_i)_{i \geq 1}$ from $\R^n$ to $\R$ is such that for every $a < b$ the preimage $f_i^{-1}([a,b))$ is Borel measurable. Suppose also that $f_i$ converge pointwise to $f$. 

Then $f$ also satisfies the same property, i.e. the preimages $f^{-1}([a,b))$ are Borel measurable.
\end{lemma}

\begin{lemma}\label{lem:approximations}
Suppose $f$ is such that for every $a < b$ the preimage $f^{-1}([a,b))$ is Borel measurable. Then $f$ is a pointwise limit of simple functions $f_n$. 

Further, a sequence can be chosen to be pointwise increasing and to converge uniformly.
\end{lemma}

The proof of proposition follows from these two lemmas. 

\begin{proof}[Proof of Proposition]
Lemma \ref{lem:approximations} tells us directly that if $f$ satisfies the preimage condition, then it is measurable.

Let us now show conversely that each measurable function satisfies the preimage condition. Using Lemma \ref{lem:limitsmeasurable} and the definition of measurable functions it satisfies to show that each simple function satisfies the preimage property.

So, consider a simple function $g = \sum_i c_i 1_{E_i}$ with $c_i \in \R$ and $E_i \in \F_B$. Then $f^{-1}([a,b)) = \cup_{i: c_i \in [a,b)]} E_i $ is a countable union of Borel measurable sets and thus Borel measurable as desired, finishing the proof.
\end{proof}

Let us now prove the two lemmas.

\begin{proof}[Proof of Lemma \ref{lem:limitsmeasurable}]
Our aim is to show that $f^{-1}([a,b))$ is a Borel set and this follows from:
$$f^{-1}([a,b)) = \bigcap_{j \geq 1}\bigcup_{k \geq 1}\bigcup_{n \geq 1}\bigcap_{m \geq n} f_m^{-1}((a-1/j,b - 1/k)).$$
The verification of this equality is on the exercise sheet
%Indeed, if $f(x) < a$, then there is some $k$ such that $f(x) < a - 2/k$ and as $f_n$ converge pointwise to $f$, then $f_n(x) < a - 1/k$ for all $n$ large enough; this shows LHS $\subseteq$ RHS. 

%For the converse inequality notice that if $x$ belongs to the set on the RHS, it means that $f_n(x) < a-1/k$ for some $k \geq 1$ and all $n$ large enough, and thus also by pointwise convergence $f(x) < a - 1/{2k}$ and thus RHS $\subseteq$ LHS.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:approximations}]
Consider $f_n : \R^n \to \R$ defined by
$$f_n(x) := 2^{-n}\lfloor 2^n f(x) \rfloor.$$
Each $f_n$ is a simple function as we can write $$f_n(x) = \sum_{k \in \Z} k2^{-n}1_{\{f(x) \in [k2^{-n}, (k+1)2^{-n})\}}$$
and by assumption the sets $\{f(x) \in [k2^{-n}, (k+1)2^{-n})\}$ are measurable.
Further we notice that
$$f_n(x) = 2^{-n}\lfloor 2^n f(x) \rfloor = 2^{-m}2^{m-n}\lfloor 2^{-n}f(x) \rfloor \geq  2^{-m}\lfloor 2^{-m}f(x) \rfloor = f_m(x)$$
proving monotonicity. 
As also $$f_n(x) \geq 2^{-n}2^{n}(f(x) - 2^{-n}) = f(x) - 2^{-n}$$ 
and thus $\|f(x) - f_n(x)\| \leq 2^{-n}$ and we obtain uniform convergence.
\end{proof}

Several nice properties of the space of measurable functions can be now verified. First, the space of measurable functions again has a linear structure: 

\begin{lemma}
If $f, g$ are measurable, then so are $\lambda f$ for $\lambda \in \R$ and $f + g$.
\end{lemma}

\begin{proof}
This is on the exercise sheet.
\end{proof}

Second, the space of measurable functions is closed under pointwise limits.

\begin{lemma}
Let $(f_n)_{n \geq 1}$ be a sequence of measurable functions converging pointwise to a function $f$. Then $f$ is also measurable.
\end{lemma}

Motice that this lemma follows directly from Lemma \ref{lem:limitsmeasurable} under the equivalence of definitions given by Proposition \ref{prop:eqmeas}. Finally, the space contains all continuous functions.

\begin{lemma}
Let $f: \R^n \to \R$ be continuous, then $f$ is also measurable.
\end{lemma}

\begin{proof}
This is also on the exercise sheet

%First write, as above, that $f^{-1}([a,b)) = \cap_{n \geq 1}f^{-1}((a-1/n, b))$.
%It thus suffices to show that $f^{-1}((a,b))$ is a Borel set for all $a < b$. But we know that the Borel $\sigma-$algebra contains all open sets, so it suffices to show that  $f^{-1}((a,b))$ is open for all $a < b$. This in turn means that we want to show that for every $x \in f^{-1}((a,b))$, we can find a $\delta > 0$ such that $B(x,\delta) \in f^{-1}((a,b))$. 

%But this follows from the definition of continuity. Indeed, as $f$ is continuous, then for every $x$ with $a < f(x) < b$ we can find a $\delta$ such that $|f(x) - f(y)| < 0.5\min(f(x)-a, b - f(x))$ whenever $y \in B(x, \delta)$. But then for every $y \in B(x, \delta)$, we have that also $a < f(y) < b$ and hence by definition $B(x, \delta) \subseteq f^{-1}((a,b))$ as desired.
\end{proof}

To finish this section we remark again that in everything we did above we didn't use at all that the domain of our functions was $\R^n$! We could have equally well worked on any other measure space, laying groundwork for defining integration in a very large generality!

\subsubsection{The idea behind Lebesgue integral}

Recall that if a function $f: \R \to \R$ is Riemann-integrable then we can calculate its Riemann integral on $[0,1]$ using the following approximation procedure:
\begin{itemize}
\item we subdivide $[0,1]$ into $2^n$ equal disjoint intervals $D_i$ each of size $2^{-n}$;
\item we calculate the approximated integral $2^{-n} \sum_{i = 0}^{2^{n}-1} f(i2^{-n})$;
\item we take the limit $n \to \infty$.
\end{itemize}
To calculate the Lebesgue integral (that we will shortly define) for a Lebesgue-integrable function on $[0,1]$ we can also proceed via an approximation, but rather in the image of the function:
\begin{itemize}
\item we take the dyadic approximations from the previous subsection: $f_n := 2^{-n}\lfloor 2^n f(x) \rfloor$;
\item we calculate $\sum_{i \in \Z} i2^{-n} \lambda(x \in [0,1]: f_n(x) = i2^{-n})$;
\item and take the limit $n \to \infty$.
\end{itemize} 
So in some sense the difference w.r.t. to the Riemann integral is that we group the values not according to the vicinity in the domain $[0,1]$, but rather based on the vicinity of the function values. So if you wish, you can think that the Lebesgue integral treats each function in a more personal way, the approximations are based on its behaviour. 

Let us now move to the formal definition of the Lebesgue integral, which we do using a slightly wider class of approximations.\\

\subsubsection{Definition of the Lebesgue integral via simple functions}

There are several ways to define the Riemann integral.\footnote{For instance, one can define it using upper and lower (Darboux) sums with arbitrary partitions or just dyadic ones; or even avoid these altogether and define integrability via convergence of approximating Riemann sums in a suitable sense.} Similarly, there are multiple equivalent approaches to constructing the Lebesgue integral (e.g., SteinShakarchi, KolmogorovFomin, and Boccarini all present slightly different versions). Last year we picked a definition via dyadic approximations that is maybe simplest to state and intuitive to grasp; this year we go for a definition that is simplest to work with mathematically. \\

Although the definition we will give works for measurable functions on any measure space \( (\Omega, \mathcal{F}, \mu) \), we will focus on the case \( (\Omega, \mathcal{F}, \mu) = (\mathbb{R}^n, \mathcal{F}_B, \lambda) \), i.e., \( \mathbb{R}^n \) with its Borel sigma-algebra and Lebesgue measure.


For simple functions, i.e. step functions of the form $f(x) = \sum_i c_i1_{E_i}$, with $E_i$ are disjoint Borel sets and $c_i \in \R$ the Lebesgue integral is simple to define:

\begin{lemma}[Lebesgue integral for simple functions]
Let $f(x)$ be a simple function given e.g. by $f(x) = \sum_i c_i1_{E_i}$. We call $f$ Lebesgue integrable if $\sum_i |c_i| \lambda(E_i) < \infty$ and define its Lebesgue integral by
$$\int_{\R^n} f(x)\lambda(dx) := \sum_i c_i \lambda(E_i).$$
Further, being integrable and the value of the integral are independent of the chosen representation of \( f \) as a simple function.
\end{lemma}

This is called a lemma and not a definition because of the final part. For example the function $f(x) = 1_{[0,1]}$ could be equivalently written as $f(x) = 1_{[0,1/2)} + 1_{[1/2, 1]}$ or even as an infinite sum $f(x) = \sum_i 1_{E_i}$ where $(E_i)_{i \geq 1}$ is any partition of $[0,1]$ into disjoint Borel sets (can you find one?). Thus, one does need to verify that integrability and the integral do not depend on the choice of the representation. Luckily, this is a simple check.

\begin{proof}
Denote by $S$ the set the image of $f$, i.e. the set $\{f(x): x\in \R^n\}$. Notice that for a simple function it is always countable.

Then observe that for every $s \in S$, we can define $F_s := \{x: f(x) = s\}$ that depend only on the function $f$. Further, for any representation $f(x) = \sum_{i} c_i 1_{E_i}$ we have $F_s = \cup_{i: c_i = s}E_i$ and in particular all $F_s$ are Borel and disjoint for different $s \in S$. 

As $\sum_i |c_i| \lambda(E_i) = \sum_{s \in S} |s| \lambda(F_s)$ and $\sum_i c_i \lambda(E_i) = \sum_{s \in S}s \lambda(F_s)$ we conclude that both integrability and the integral are well-defined and independent of the representation.
\end{proof}

\begin{eg}
For example, in contrast to the Riemann integral $f(x) = 1_{\Q}(x)$ is integrable with integral equal to $0$. Similarly, and $f(x) = 1_{[0,1] \setminus \Q}(x)$ is integrable with integral equal to $1$ - both are themselves simple functions!
\end{eg}

For general measurable functions we will proceed in two steps: first we define the Lebesgue integral for non-negative functions, and then generalise it to all measurable functions by separating into non-negative and positive parts.

\begin{defn}[Lebesgue integral]
Let $f:\R^n \to \R$ be non-negative and measurable. Then we define
\[ \int_{\mathbb{R}^n} f(x)\, \lambda(dx) := \sup \left\{ \int_{\mathbb{R}^n} g(x)\, \lambda(dx) \;\middle|\; 0 \le g \le f,\, g \text{ simple} \right\}.\]
We say that $f$ is integrable if this supremum is finite.

For a general measurable function \( f \), we write \( f = f_+ - f_- \), where \( f_+, f_- \) are the positive and negative parts of \( f \), given by
\[
f_+ = \max(f, 0), \quad f_- = \max(-f, 0).
\]
We say that \( f \) is integrable if both \( f_+ \) and \( f_- \) are integrable, and define
\[
\int_{\mathbb{R}^n} f(x)\, \lambda(dx) := \int_{\mathbb{R}^n} f_+(x)\, \lambda(dx) - \int_{\mathbb{R}^n} f_-(x)\, \lambda(dx).
\]
\end{defn}

We will sometimes for the sake of brevity also use the shortcut $\int f \lambda(dx) := \int_{\mathbb{R}^n} f(x)\, \lambda(dx)$ or even $\int f d\lambda$.
\begin{rem}
One could also try to alternatively define the integral as a limit of integrals of any sequence of uniformly approximating simple functions. This works well when integrating over sets of finite measure (like say $[0,1]$); however, as you see on the example sheet, it would require care when integrating over sets of infinite measure, like $\R$ or $\R^n$. 
\end{rem}

\begin{exo}
Verify from the definitions that $f(x) = x1_{[0,1]}$ is measurable and integrable. Calculate its integral also from the definition. What about $f(x) = x^{-1}1_{(0,1]}$
\end{exo}

\begin{rem}
We can further define the integral over any Borel integrable set $E$, which we denote by $\int_E f(x) \lambda(dx)$ by just considering the integral of $1_E(x) f(x)$, which as a product of measurable functions is nicely measurable.
\end{rem}

\begin{rem}
We can similarly define an integral over complex-valued functions by just separating the real and imaginary parts, i.e. if $f(x) = r(x) + iq(x)$ we call it integrable if the real functions $r, q$ are and just set $\int f d\lambda = \int r d\lambda + i \int g d\lambda$.
\end{rem}

Whereas it is natural to define the Lebesgue integral via simple functions, as countable collections go well with the measure-theoretic framework, it is technically convenient to observe that one can actually work with simple functions that are given by just finite sums.

\begin{lemma}
Let us call a simple functions $f$ simple and finite, if it can be written as $f(x) = \sum_{i=1}^n c_i 1_{E_i}(x)$ for some finite disjoint sets $E_1, \dots, E_n$ and some real numbers $c_1, \dots, c_n$.
Then we have that 
\[\sup \left\{ \int_{\mathbb{R}^n} g(x)\, \lambda(dx) \;\middle|\; 0 \le g \le f,\, g \text{ simple} \right\} =  \sup \left\{ \int_{\mathbb{R}^n} g(x)\, \lambda(dx) \;\middle|\; 0 \le g \le f,\, g \text{ simple and finite} \right\}\] 
and in particular one can equivalently define the Lebesgue integral by just considering simple functions that are given by finite sums.
\end{lemma}

\begin{proof}
It is clear that the LHS is larger than the RHS.
So it just remains to show that RHS is at least as big as the LHS. To do this notice that for any non-negative integrable simple function
$g(x) = \sum_{i\geq 1} c_i 1_{E_i}(x)$, i.e. a function for which $\sum_{i \geq 1}c_i \lambda(E_i) < \infty$, we can associate a simple finite function $g_\eps(x) = \sum_{i = 1}^{N_\eps} c_i 1_{E_i}$, where $N_\eps$ is chosen such that $\sum_{i > N_\eps}c_i \lambda(E_i) < \eps$. By definition this guarantees that
\[ |\int_{\mathbb{R}^n} g(x) \lambda(dx) - \int_{\mathbb{R}^n} g_\eps(x) \lambda(dx)| < \eps.\]
Now denote by $S_L$ the supremum on the LHS and by $S_R$ the supremum on the RHS of the equality in the lemma. 
By definition we can choose $g$ such that $ \int_{\mathbb{R}^n} g(x) \lambda(dx)  \geq S_L - \eps$. But then by construction $ \int_{\mathbb{R}^n} g_\eps(x) \lambda(dx)  \geq S_L - 2\eps$ and by taking $\eps \to 0$ we see that RHS is also at least as large as the LHS.

We conclude that two suprema agree, and the integral could equivalently have been defined using only finite simple functions.
\end{proof}

\subsubsection{Basic properties of the Lebesgue integral}

We begin by examining some basic properties of the Lebesgue integral. Notice that several of these natural properties do not hold for the Riemann integral!

\begin{prop}[Basic properties of the Lebesgue integral]\label{prop:bpl}
Let $f: \R^n \to \R$ be measurable. Then
\begin{enumerate}
\item if $f \geq 0$ and $f$ is integrable, then $\int f d\lambda \geq 0$
%\item $f$ is integrable iff $|f|$ is integrable
%\item if $f \geq 0$ is integrable, then any other measurable function $g$ with $|g| \leq f$ is also integrable. In particular 
\item if $|f(x)| \leq C$ for all $x \in \R^n$, then it is integrable over any finite box $[a_1,b_1]\times \dots \times [a_n, b_n] \subseteq \R^n$
\item if $\lambda(f \neq 0) := \lambda(\{x: f(x) \neq 0\}) = 0$, then $f$ is integrable and $\int f d\lambda = 0$
\item if $f \geq 0$ and $\int f d\lambda = 0$, then $\lambda(f \neq 0) = 0$.
\end{enumerate}
\end{prop}

Notice that even property 2 is rather interesting: it somehow says that for a measurable function only unboundedness can prevent it from being integrable! We have separated the question of "regularity" of the function (carried by the notion of measurability) from that of its size (which governs integrability).

\begin{proof}
The first property comes directly from the definition. The others are on the exercise sheet.
%These come directly from the definitions and thus are most likely in large part left as an exercise.
%The first property comes from the definition. 
%
%For the second property we notice that by definition then every simple function approximating $|f|$ from below is pointwise bounded by $C$ and thus its integral over $[a,b]$ is bounded by $C(b-a)$, thus also the supremum is finite.

%For the third property let us first suppose that $\lambda(f > 0) = 0$ and let $g$ be any simple finite function bounded above by $f$ and bounded from below by $0$. It is of the form $g = \sum_{i = 1}^n c_i 1_{E_i}$. And in particular 
%$$0 = \lambda(\{f > 0\}) \geq \lambda(\cup_{i: c_i > 0} E_i).$$ 
%Thus $\int g d\lambda = 0$ by definition and also $\int f d\lambda = 0$.

%In the other direction
\end{proof}

In particular, applying property (3) to the difference  $f-g$ of two measurable $f, g$ we should intuitively obtain:

\begin{cor}\label{cor:int}
Let $f, g$ be two measurable functions such that $\lambda( f \neq g) := \lambda(\{x: f(x) \neq g(x)\}) = 0$. Then $f$ is integrable iff $g$ is integrable and $\int f d\lambda = \int g d\lambda$.
\end{cor}

However, writing down the proof we will see that we are still lacking a tool to show this nicely \footnote{One can show this particular case also by hand, but it is a bit tiring and not worthwhile.}

\begin{proof}
Define $h = g - f$. Then $h$ is measurable and $\lambda(h \neq 0) = 0$. Thus by the proposition $h$ is integrable and $\int h d\lambda = 0$.

We would like to know say that $g = f + h$ and it is integrable because $f, h$ are and further 
$$\int g d\lambda = \int (f+h) d\lambda = \int f d\lambda + \int h d\lambda.$$
But here we are already using the linearity of the Lebesgue integral, something we still need to prove and that is stated in the next proposition. 
\end{proof}


\begin{prop}\label{prop:linear}
Let $f, g$ be Lebesgue integrable functions. We have the following linearity statement. For $a, b \in \R$, then $af + bg$ is integrable and $$\int (af+bg) d\lambda = a\int f d\lambda + b\int g d\lambda.$$
\end{prop}
It comes out this linearity is not as straightforward to prove as one hopes!

Indeed, it is straight-forward to check that for finite simple functions $f, g$ it holds that $\int (f+g)d\lambda = \int f d\lambda + \int g d\lambda$. To see this set $f = \sum_{i = 1}^n c_i 1_{E_i}$ and $g = \sum_{j = 1}^m d_j 1_{F_j}$, then 
$$g+ f = \sum_{i = 0 \dots n}\sum_{j = 0 \dots m} (c_i + d_j)1_{E_i \cap F_j},$$
where we define $c_0 = d_0 := 0$ and $E_0 := \R^n \setminus \cup_{i = 1}^n E_i$ and $F_0 = \R^n \setminus \cup_{j = 1}^m F_j$. Here we have introduced $c_0, d_0$ because the function $f$ takes value $0$ in the complement of $\cup_{i=1}^N E_i$ and $g$ takes value $0$ in the complement of $\cup_{j=1}^M F_j$.

It can be then checked from the definition that $\int (f+g) d\lambda = \int f d\lambda + \int g d\lambda$ for these finite simple functions (on the exercise sheet).

It is also easy to see that for non-negative measurable $f, g$ we have that $\int (f+g) d\lambda \geq \int f d\lambda + \int g d\lambda $. Indeed, whenever $h \geq 0, j \geq 0$ are simple finite functions bounded from above by $f, g$ respectively, the function $k := h + j$ is a simple finite function bounded from above by $f + g$.
Thus 
\[\sup \left\{ \int_{\mathbb{R}^n} k(x)\, \lambda(dx) \;\middle|\; 0 \le k \le f+g,\, k \text{ simple and finite} \right\}\]
is larger than the sum of
\[\sup \left\{ \int_{\mathbb{R}^n} h(x)\, \lambda(dx) \;\middle|\; 0 \le h \le f,\, h \text{ simple and finite}\right\}\] 
and
\[\sup \left\{ \int_{\mathbb{R}^n} j(x)\, \lambda(dx) \;\middle|\; 0 \le j \le g,\, j \text{ simple and finite} \right\}\] 
The other inequality, however, requires a few tools. The issue is the following:
\begin{itemize}
\item given a simple function $k$ below $f+g$, it is not straight-forward to construct two simple functions, $h$ below $f$, and $j$ below $g$ with $h+j = k$.
\end{itemize}
So instead of attempting a direct construction, we will take a detour through some general theorems that will allow us to prove linearity rigorously. In short we will show that under certain conditions, when $f_n \to f$ pointwise, we have $\int f_n d\lambda \to \int f d\lambda$. 

More precisely we will use one simple ingredient and one more serious one.
%two more serious theorems. 
The simple ingredient follows directly from the definition of the integral.

\begin{lemma}\label{lem:dom}
Let $0 \leq g \leq f$ be measurable. Then if $f$ is integrable, so is $g$ and moreover $\int f d\lambda \geq \int g d\lambda$.
\end{lemma}

%%%% MAYBE AS AN EXO?
%\begin{lemma}
%Let $f$ be Lebesgue integrable over disjoint sets $E$ and $F$. Then it is also integrable over $E \cup F$ and 
%$$\int_{E \cup F} f d\lambda = \int_E f d\lambda + \int_F f %d\lambda.$$
%\end{lemma}

%\begin{proof}
%This is on the exercise sheet.
%\end{proof}
%In particular by considering $E = 1_{f > 0}$ and $F = 1_{f \leq 0}$ we conclude that
% OR THIS?
%\begin{cor}
%Let $f$ be measurable. Then $f$ is integrable iff $|f|$ is integrable.
%\end{cor}

%\begin{proof}
%If $|f|$ is integrable, then $f$ also is by definition as any simple function bounded by $f_-$ or $f_+$ is also bounded by $|f|$.

%For the other direction we consider $E = 1_{f > 0}$ and $F = 1_{f \leq 0}$ and deduce from the definition of integrability of $f$ that $|f|$ is integrable over both $E$ and $F$ and hence over the whole $E \cup F$, which is the whole space.
%\end{proof}

The more substantial theorem is about approximating the integral of $f$ via integrals of approximations $f_n$.

\begin{thm}[Monotone convergence theorem]\label{thm:monotoneconv}
Let $0 \leq f_1 \leq f_2 \leq \dots$ be a sequence of integrable functions converging pointwise to some $f = \lim_{n \to \infty} f_n$. Then $f$ is integrable if $\lim_{n \to \infty} \int f_n d\lambda < \infty$ and in this case
$$\int f d\lambda =  \lim_{n \to \infty} \int f_n d\lambda.$$
\end{thm}



Before proving this, let us see how linearity follows. 

%To do this we make some more observations about the integral that follows directly from the definition:

%\begin{lemma}
%If $f \geq 0$ is integrable and $f \geq g \geq 0$, then also $g$ is integrable and $\int f d\lambda \geq \int g d\lambda$. 

%Further, when $|f|$ is integrable, then so is $f$.
%\end{lemma}

\begin{proof}[Proof of Proposition \ref{prop:linear}]

Let us just prove the more interesting case: that $\int (f+g) d\lambda = \int f d\lambda + \int g d\lambda$.
We saw that this linearity holds for simple functions on the exercise sheet. We will now first show using Monotone convergence theorem that it holds for all integrable non-negative functions, and then use the decomposition into positive and negative parts to argue for the general case.

Pick a sequence of simple functions $f_n, g_n \geq 0$ with $f_n \leq f_{n+1}$ and $g_n \leq g_{n+1}$ and $f_n \to f$, $g_n \to g$ pointwise from below. Then also $f_n+g_n \to f+g$ pointwise from below. On the one hand by the linearity of simple functions for all $n \geq 1$
$$\int (f_n + g_n) d\lambda = \int f_n d\lambda + \int g_n d\lambda.$$
On the other hand by the Monotone convergence theorem 
$$\int f_n d\lambda \to \int f d\lambda \text{  ;  } \int g_n d\lambda \to \int g d\lambda.$$
In particular this means that
$$\lim_{n \to \infty} \int (f_n + g_n)d\lambda = \lim_{n \to \infty}\left( \int f_n d\lambda + \int g_n d\lambda\right) = \int f d\lambda +\int g d\lambda$$
is finite and hence by Monotone convergence theorem $f+g$ is integrable and
$$\lim_{n \to \infty}\int (f_n + g_n)d\lambda = \int (f+g) d\lambda.$$
So we obtain that 
$$\int( f+g )d\lambda = \int f d\lambda + \int g d\lambda,$$
as desired.

For general integrable $f, g$ let us write $f = f_+ - f_-$, $g = g_+ - g_-$ and $f+g = (f+g)_+ - (f+g)_-$ and recall that all these positive and negative parts are non-negative measurable functions. 

%As it is not true that $(f+g)_+ = f_+ + g_+$, so we have to be a bit more clever then just summing blindly. 
Now, notice that $(f+g)_+ \leq f_+ + g_+$ and $(f+g)_- \leq f_- + g_-$ and thus if $f, g$ are integrable then so are $(f+g)_+, (f+g)_-$ by Lemma \ref{lem:dom} and hence also $ f+ g$.

Now we can rewrite the pointwise equality
$$(f+g)_+ - (f+g)_- = f_+ - f_- + g_+ - g_-,$$
as
$$(f+g)_+ + f_- + g_- = (f-g)_- + f_+ + g_+.$$
But to this we can apply the first part of the proof on both sides to conclude that
$$\int (f+g)_+d\lambda + \int f_- d\lambda + \int g_- d\lambda = \int (f+g)_-d\lambda + \int f_+ d\lambda + \int g_+ d\lambda.$$
It now remains to recombine the terms and to use the definition of the integral to see that
$$\int (f+g) d\lambda = \int f d\lambda + \int g d\lambda$$
as desired.

%We will now use the Monotone convergence theorem to show that $|f| + |g|$ is integrable, and then Dominated convergence theorem to show the equality of integrals.

%To see that $|f| + |g|$ is integrable pick a sequence of simple functions $f_n, g_n \geq 0$ with $f_n \leq f_{n+1}$ and $g_n \leq g_{n+1}$ and $f_n \to |f|$, $g_n \to |g|$. Then $f_n+g_n \to |f|+|g|$ pointwise from below. On the one hand by the linearity of simple functions
%$$\int (f_n + g_n) d\lambda = \int f_n d\lambda + \int g_n d\lambda.$$
%On the other hand by the Monotone convergence theorem 
%$$\int f_n d\lambda \to \int |f| d\lambda \text{  ;  } \int g_n d\lambda \to \int |g| d\lambda.$$
%In particular this means that
%$$\lim_{n \to \infty} \int (f_n + g_n)d\lambda = \lim_{n \to \infty}\left( \int f_n d\lambda + \int g_n d\lambda\right)$$
%is finite and hence by Monotonce convergence theorem
%$$\int (f_n + g_n)d\lambda \to \int |f| + |g| d\lambda.$$
%So we obtain that 
%$$\int( |f| + |g| )d\lambda = \int |f| d\lambda + \int |g| d\lambda,$$
%and we conclude the integrability of $|f| + |g|$ from those of $|f| $ and $|g|$.

%To show that $\int (f+ g) d\lambda = \int f d\lambda + \int g d\lambda$ we now use the exact same strategy.
%We pick a sequence of simple functions $f_n, g_n$ with $|f_n| \leq |f|$ and $|g_n| \leq |g|$ that converge pointwise to $f$ and $g$ respectively. Then $f_n + g_n$ converge to $f+g$ and are bounded by the integrable function $|f| + |g|$. Thus 
%on the one hand
%$$\int (f_n + g_n) d\lambda = \int f_n d\lambda + \int g_n d\lambda.$$
%As by the Dominated convergence theorem also 
%$$\int (f_n + g_n) d\lambda \to \int (f+g) d\lambda \text{  ;  } \int f_n d\lambda \to \int f d\lambda \text{  ;  } \int g_n d\lambda \to \int g d\lambda,$$
%we obtain the desired equality.
\end{proof}

\subsubsection{Convergence theorems}

\noindent Let us now look more closely at the statement in the style 
\begin{itemize}
    \item if $f_n \to f$ pointwise, then $\int f_n d\lambda \to \int f d\lambda$.
\end{itemize} 
We will first see some counterexamples, then prove the Monotone convergence theorem and a few other useful convergence results. 

The first failure could be that the limiting $f$ is not integrable. Recall that the pointwise limit of measurable functions is measurable. Thus we at least know that $f$ is regular enough to be potentially integrable. It could fail to be integrable because of "size":
\begin{eg}\label{eg:domconv}
Consider the functions $(f_n)_{n \geq 1}$ defined on $\R$ by $f_n(x) = 1_{[0,n]} - 1_{[-n,0]}$. These functions are all finite simple functions and their integral is equal to $0$. But notice that their pointwise limit $f = 1_{[0,\infty)} - 1_{(-\infty,0]}$ is measurable but not integrable. 
\end{eg}

But even if the limiting function is integrable, it's integral is not necessarily equal to the limit of integrals.

\begin{eg}\label{eg:domconv}
Consider the functions $(f_n)_{n \geq 1}$ defined on $\R$ by $f_n(x) = n1_{(0,1/n)}$. They are finite simple functions and satisfy $\int f_n d\lambda = 1$ by definition. But notice that $f_n(x)$ converge to the constant $0$ function pointwise, as for every $x \in \R$, there is some $n_x \in \N$ such that $f_n(x) = 0$ for all $n \geq n_x$. But the integral of the constant $0$ function is just $0$ and thus the integrals of $f_n$ do not converge to the integral of their pointwise limit.
\end{eg}
In this example the functions concentrate the mass on a smaller and smaller region, keeping area under the graph equal to $1$. Eventually this tiny vertical box somehow moves out of the interval $(0,1)$ and disappears. But suppose we ask all of the $f_n$ to be bounded?
\begin{eg}\label{eg:domconv}
Consider the functions $(f_n)_{n \geq 1}$ defined on $\R$ by $f_n(x) = n^{-1}1_{[0,n]}$. Again they are measurable and bounded, thus integrable with $\int f_n d\lambda = 1$. Also $f_n(x)$ converge to the constant $0$ function pointwise too, as for every $x \in \R$, there is some $n_x \in \N$ such that $f_n(x) \leq \eps$ for all $n \geq n_x$. But the integral of the constant $0$ function is just $0$ and thus the integrals of $f_n$ do not converge to the integral of their pointwise limit either.
\end{eg}
In this case the area is kept constant by keeping the box horisontally. But suppose, $\lambda(f_n \neq 0) < C$ for some constant $C$?
\begin{eg}\label{eg:domconv}
Consider the functions $(f_n)_{n \geq 1}$ defined on $\R$ by $f_n(x) = 1_{[n,n+1)}$. Again they are measurable and bounded, thus integrable with $\int f_n d\lambda = 1$. Also $f_n(x)$ converge to the constant $0$ function pointwise too, as for every $x \in \R$, there is some $n_x \in \N$ such that $f_n(x) = 0$ for all $n \geq n_x$. But the integral of the constant $0$ function is just $0$ and thus the integrals of $f_n$ do not converge to the integral of their pointwise limit either.
\end{eg}
Now the functions remain bounded but all of the mass moves away to infinity. In some sense these are the counterexamples to keep in mind and the conditions given in the Monotone convergence theorem rule those cases out. Recall the statement:
\begin{thm}[Monotone convergence theorem]\label{thm:monotoneconv}
Let $0 \leq f_1 \leq f_2 \leq \dots$ be a sequence of integrable functions converging pointwise to some $f = \lim_{n \to \infty} f_n$. Then $f$ is integrable if $\lim_{n \to \infty} \int f_n d\lambda < \infty$ and in this case
$$\int f d\lambda =  \lim_{n \to \infty} \int f_n d\lambda.$$
\end{thm}
And here is the proof.

\begin{proof}[Proof of the Monotone convergence theorem, Theorem \ref{thm:monotoneconv}]

First, as $0 \leq f_n \leq f$, it is clear that $$\lim_{n \to \infty} \int f_n d\lambda \leq \int f d\lambda.$$
The question is why does the other inequality (and thus also the claim on integrability) hold. So we can now assume $\int f d\lambda < \infty$.

Let us start with the case where the limit $f = \lim_{n \to \infty}f_n$ itself is a simple finite function. In particular it can be represented by $f = \sum_{i = 1}^m c_i 1_{E_i}$ for some disjoint Borel sets $E_i$ and distinct $c_i > 0$ and its integral equals $\int f d\lambda = \sum_{i=1}^m c_i\lambda(E_i)$. As the integral is finite by assumption, we have that $\lambda(E_i) < \infty$ for every $i =  1 \dots m$. 

For each $\epsilon > 0$ and $n \geq 1$ we can then define the sets $F_n := \{x: f_n(x) \geq (1-\eps)f(x)\}$ and further $F_{n,i} = F_n \cap E_i$. These sets are increasing and pointwise convergence of $f_n$ to $f$ guarantees that $\cup_{n \geq 1} F_{n,i} = E_i$ for all $i = 1 \dots m$. But then the properties of the measure $\lambda$ imply that $\lambda (F_{n,i}) \to \lambda(E_i)$; here we use that $\lambda(E_i) < \infty$. 
But now by definition
$f_n(x) \geq (1-\eps)\sum_{i=1}^mc_i 1_{F_{n,i}}$
and thus by Lemma \ref{lem:dom}
$$\int f_n d\lambda \geq (1-\eps)\sum_{i=1}^m c_i\lambda(F_{n,i})$$
and we conclude that 
$$\lim_{n \to \infty} \int f_n d\lambda \geq (1-\eps) \int f d\lambda.$$
As $\eps$ was arbitrary the claim follows for simple limiting functions $f$.

But now for general $f$ we can pick any simple function $g \leq f$ and repeat the argument. Even though $f_n$ do not converge to $g$, they converge to $f \geq g$ and this suffices to conclude that $\cup_{n \geq 1} F_{n,i} = E_i$. Hence we obtain similarly that 
$$\lim_{n \to \infty} \int f_n d\lambda\geq \int g d\lambda.$$
As this holds for any simple function $g$ that is bounded above by $f$, we conclude the claims of the theorem from the definition of the integral: if $\lim_{n \to \infty} \int f_n d\lambda < \infty$, then $f$ is integrable and its integral equals that limit.
\end{proof}

The theorem is usually stated for non-negative functions, but notice that if $f_1 \leq f_2 \dots$ is an increasing sequence of integrable functions, then we could consider $\hat f_n := f_n - f_1$ to get a sequence of non-negative functions converging to $f - f_1$ and by linearity also conclude a statement for such functions. Also of course one can obtain a similar  statement for increasing functions too (see the exercise sheet).

There is a very useful corollary saying that even if the functions $f_i$ are not increasing, we can still say something about the integral:

\begin{cor}[Fatou's lemma]
Let $f_1, f_2, \dots $ be non-negative integrable functions. Then 
$$\liminf_{n \to \infty} \int f_n d\lambda \geq \int \liminf_{n \to \infty}f_n d\lambda,$$
where we allow both sides also to be equal to infinity.
\end{cor}

Notice that in particular if the non-negative functions $f_n$ converge, we see the limit of their integrals is larger than the integral of their limit. To remember the direction of the inequality, just think of the example of the travelling interval!

\begin{proof}
Observe that $g_n := \inf_{m \geq n}f_m$ is non-negative, non-decreasing and by definition converges to $\liminf_{n \to \infty} f_n$. 

Thus by Monotone convergence theorem
if $\lim_{n \to \infty} \int g_n d\lambda$ is finite then
$\liminf_{n \to \infty} f_n$ is integrable and
$$\int \liminf_{n \to \infty}f_n d\lambda = \lim_{n \to \infty} \int g_n d\lambda.$$
But now $f_m \geq g_n$ for all $m \geq n$ and hence 
$$\inf_{m \geq n}\int f_m d\lambda \geq \int g_n d\lambda.$$
We conclude that 
$$\liminf_{n \to \infty} \int f_n d\lambda \geq \lim_{n \to \infty} \int g_n d\lambda = \int \liminf_{n \to \infty}f_n d\lambda$$
as desired.
\end{proof}

The other useful convergence theorem deals with an arbitrary convergent sequence $f_n$ of integrable functions. Here the counterexamples from above are avoided by asking there to be a dominating function - an integrable non-negative function $g$ such that $g \geq |f_n|$ for all $n \geq 1$. We will admit this result for the sake of time.

\begin{thm}[Dominated convergence theorem]\label{thm:domconv}
Let $f_1, f_2, \dots$ be a sequence of integrable functions converging pointwise to some $f = \lim_{n \to \infty} f_n$. Suppose also that there is some $g \geq 0$ that is integrable and such that $|f_n| \leq g$ for all $n \geq 1$. Then $f$ is integrable and
$$\int f d\lambda =  \lim_{n \to \infty} \int f_n d\lambda.$$
\end{thm}

\subsubsection{Almost sure / almost everywhere equality}

We saw that $\int f d\lambda = \int g d\lambda$ whenever $\lambda(f \neq g) = 0$. This is rather interesting and turns out to be quite important!
It says explicitly that the integral is insensitive to changing the function on sets of measure $0$ - in particular even modifying the value of the function on any countable set leaves the integral unchanged! 

This offers a new perspective on why $\int 1_{\Q} d\lambda = 0$ and motivates the following vocabulary that will be helpful to use.

\begin{defn}[Almost everywhere / almost sure equality]
We say that two measurable functions $f, g$ are equal almost surely or almost everywhere if $\lambda(f \neq g) := \lambda(\{x: f(x) \neq g(f) \}) = 0$. 

More generally we say that some property or condition $E \in \F_B$ holds almost everywhere (a.e.) or almost surely (a.s.) if $\lambda(E^c) = 0$.\footnote{It is called almost everywhere in analysis, almost surely in probability, we will probably not be able to avoid using both simultaneously.}
\end{defn}

The fact that the integral does not change when we change the function on a zero measure set means that, in the context of integration, when we ask for a property of the function to hold, we should be able to ask it to hold only almost everywhere.

For example, let us look at Lemma \ref{lem:dom}. We should be able to replace $0 \leq g \leq f$ pointwise by an almost everywhere statement: if $ \lambda(\{x:  g(x) > f(x)\}\cup\{x:  g(x) < 0\}) = 0$ and $f$ is integrable, then $g$ is integrable and $\int g d\lambda \leq \int f d\lambda$. This is now a different mathematical statement! But one can easily argue it by just considering $\tilde f, \tilde g$ defined by setting $\tilde f(x) = \tilde g(x) := 0$ in the complement of the set $\{x: 0 \leq g(x) \leq f(x)\}$. Then the inequality holds pointwise, one can apply Lemma \ref{lem:dom} to $\tilde f, \tilde g$ and conclude using Corollary \ref{cor:int}.

I will leave it on the example sheet to verify that all the other statements we have seen about the integral hold only under hypothesis given in the form of almost sure properties. In particular all the convergence theorems - Monotone convergence theorem, Fatou's lemma and the Dominated convergence theorem hold under these milder conditions using similar little tricks. \footnote{For enthusiasts. There is only one little crux that comes rather from measurability - when $f_1, f_2, \dots$ converge almost everywhere, then their pointwise limit is not necessarily measurable (might not even exist!)...yet there exists always a measurable function $\tilde f$ that is an almost everywhere limit of $f_1, f_2, \dots$. I will leave this confusion for the enthusiasts and the rest can just think that everything works well.}

\subsubsection{Comparison to the Riemann integral}

As an application of convergence theorems let us see how to compare the Lebesgue integral with the Riemann integral.

Recall first that there are many functions that are Lebesgue integrable but not Riemann integrable even on $[0,1]$.
\begin{itemize}
\item The function $1_{\Q \cap [0,1]}$ or say $f + 1_{\Q \cap [0,1]}$ for any Riemann integrable function $f$
\item All Riemann integrable functions on $[0,1]$ are bounded, but for example the function $f(x) := x^{-1/2}1_{x \in (0,1]}$ is Lebesgue integrable
\end{itemize}
However, philosophically all Riemann integrable functions are Lebesgue integrable. This statement would be completely true had we used the Lebesgue $\sigma$-algebra instead of the Borel one; in our case we have to add a little extra assumption of measurability, but as mentioned finding non-measurable functions is already not so easy!

\begin{prop}
Suppose $f: [a,b] \to \R$ is measurable and Riemann integrable. Then $f$ is also Lebesgue integrable and both integrals agree. \footnote{For enthusiasts. In fact one does not need to assume the measurability. It also holds that for each Riemann integrable function $f$ there is a measurable function $\tilde f$ such that for some Borel set $E$ of zero measure $\{f \neq \tilde f\} \subseteq E$ and such that the Lebesgue integral of $\tilde f$ agrees with that of $f$. }
\end{prop}

In particular this means that to calculate the Lebesgue integral of any reasonable function $f$ you can use the same tricks you have learned for the Riemann integral as you are calculating the same numbers! Just a few more functions are Lebesgue integrable and importantly the structure of the space of Lebesgue integrable is much nicer - a point that will be illustrated even better with the next section.

The proposition is not entirely straightforward because the piecewise approximations of $f$ used to define the approximate Riemann integrals converge to $f$ only in the sense that the limits of upper and lower Riemann integrals will agree. But to apply any of our convergence theorems we would need to have almost everywhere convergence.

%The link between the two is built by the following lemma that will be of use later on too.

%\begin{lemma}
%Suppose $f_1 \geq f_2 \geq \dots$ are non-negative integrable functions such that $\int f_n \to 0$. Then in fact $f_n \to 0$ almost everywhere in the sense that $\lambda(\{x: f_n(x) \nrightarrow 0\}) = 0$.
%\end{lemma}

%Let us first see how it implies the proposition:

\begin{proof}
For simplicity let's take $[a,b] = [0,1]$. We use the fact that for a Riemann-integrable $f$ on $[0,1]$ we know that the upper and lower approximations of the integral over dyadics both converge to the integral.

More precisely, if we set
$$L_n(f) := 2^{-n}\sum_{i=0}^{2^n-1} \inf_{x \in [i2^{-n},(i+1)2^{-n}]}f(x)$$
and
$$U_n(f) := 2^{-n}\sum_{i=0}^{2^n-1} \sup_{x \in [i2^{-n},(i+1)2^{-n}]}f(x)$$
then Riemann integrability implies that $L_n$ is increasing and $U_n$ is decreasing both to the same limit that is equal to the Riemann integral $ \int_0^1 f(x)dx$.

But now $L_n$ is also the Lebesgue integral of the simple function 
$$g_n(x) := \sum_{i=0}^{2^n-1} 1_{x \in [i2^{-n},(i+1)2^{-n})}\inf_{x \in [i2^{-n},(i+1)2^{-n}]}f(x)$$
and $U_n$ the Lebesgue integral of the simple function $$h_n(x) := \sum_{i=0}^{2^n-1} 1_{x \in [i2^{-n},(i+1)2^{-n})}\sup_{x \in [i2^{-n},(i+1)2^{-n}]}f(x).$$ 
Further $g_n$ is pointwise increasing and bounded from above by $f$, and $h_n$ is pointwise decreasing and bounded from below by $f$. Thus both converge pointwise to measurable functions denoted $f_L, f_U$ and satisfying $f_U \geq f_L$ pointwise. But now by Monotone Convergence Theorem and its cousin proved on the example sheet we see that 
$$\int_{[0,1]}f_L d\lambda = \int_{[0,1]}f_U d\lambda = \int_0^1 f(x)dx$$
and thus $\int_{[0,1]}(f_U - f_L)d\lambda = 0$. As $f_U - f_L$ is non-negative we conclude that $\lambda(f_U \neq f_L) = 0$. But also $f_U \geq f \geq f_L$ pointwise, so in particular $\lambda(f_L \neq f) = 0$, where we use the fact that $f$ is measurable by assumption to see that this event is measurable. Hence also the Lebesgue integral of $f$ agrees with $\int_0^1 f(x) dx$ as desired. 

%Moreover these functions are increasing and decreasing respectively. In particular the sequence of simple functions $(f_n = h_n - g_n)_{n \geq 1}$ is non-negative, integrable and satisfies $\int f_n d\lambda = U_n - L_n \to 0$. Thus by the Lemma we see that $f_n \to 0$ almost everywhere. But 

%But thus we can apply the version of the Monotone convergence theorem from the example sheet (after having verified it works also under almost everywhere convergence) to conclude that $$
\end{proof}

\subsubsection{Fubini's theorem}

To finish off the chapter on Lebesgue integral, we still have to see one key result which is the integration analogue to the following result on infinite double sums:

\begin{prop}
Let $(a_{i,j})_{i, j \geq 1}$ be an array of real numbers.
If $\sum_{i = 1}^n\sum_{j=1}^n |a_{i,j}|$ converges as $n \to \infty$, then for all $i \geq 1$, $\sum_{j=1}^n |a_{i,j}|$ converges and for all $j \geq 1$, $\sum_{i=1}^n |a_{i,j}|$ converges.

Moreover, then 
$$\lim_{n \to \infty} \sum_{i = 1}^n\sum_{j=1}^n a_{i,j} = \lim_{N \to \infty} \sum_{n = 1}^N\sum_{j \geq 1} a_{n, j} = \lim_{N \to \infty} \sum_{n = 1}^N \sum_{i \geq 1} a_{i,n}.$$

\end{prop}

The result for integrals that we state properly in a bit is very similar - we assume that $f: E_1 \times E_2 \to \R$ is integrable, and we want to say that for all fixed $x_1 \in E_1$, the function $f(x_1, y)$ is integrable over $E_2$; that the function $x_1 \to \int_{E_2} f(x_1, y) d\lambda(y)$ is then integrable over $E_1$ and that finally by doing these two integrations we obtain the Lebesgue integral of $f$.  

Now there are some extra questions that pose themselves in comparison to sums:
\begin{enumerate}
    \item First, why is the function $f(x_1, y): E_2 \to \R$ for every fixed $x_1$ measurable?
    \item Second, why is the integral for every $x_1$ well-defined?
    \item Third, why is the function $x_1 \to \int_{E_2} f(x_1, y) d\lambda(y)$ measurable, integrable?
\end{enumerate} 
All these aspects require some thought, although it comes out that in our setting where we work with Borel measurable functions the first one is simple to verify (on the sheet). Both the second and the third property do require thought: whereas in the case of sums the finiteness of the double sum clearly implies the those of all individual sums, in the case of integrals this is actually not the case!

\begin{eg}
Consider the function $f:(-1,1)^2 \to \R$ defined by 
$$f(x,y) = 1_{ \Q \cap (-1,1)}(x)\frac{1}{y}1_{(-1,1)}(y).$$ 
Then $\lambda(f \neq 0) = 0$ and thus $f$ is integrable with integral equal to $0$. However for $x \in \Q \cap (-1,1)$, the function $f_x(y) := f(x,y) = \frac{1}{y}1_{(-1,1)}(y)$ is measurable but not integrable.
\end{eg}

\begin{thm}[Fubini]
Let $E_1, E_2$ be Borel sets of non-zero measure and suppose that $f: E_1 \times E_2 \to \R$ is Lebesgue integrable. Then
\begin{itemize}
\item for almost every $x_1 \in E_1$, the function $f(x_1, y)$ is integrable over $E_2$;
\item the function $F(x_1) := \int_{E_2} f(x_1, y) d\lambda(y)$ if the integral is finite, and $F(x_1) = 0$ otherwise is measurable and integrable over $E_1$;
\item $\int_{E_1 \times E_2} f(x, y) d\lambda(x,y) = \int_{E_1} F(x_1) d\lambda(x_1) = \int_{E_1} (\int_{E_2} f(x,y)d\lambda(y))d\lambda(x)$;
\item and the same holds if we swap the order of integration.
\end{itemize}
Conversely, if either for almost every $x_1$ the function $|f(x_1, y)|$ is integrable over $E_2$ and further the function $F(x_1) := \int_{E_2} |f(x_1,y)|d\lambda(y)$ is integrable over $E_2$ or the same holds when $x_1$ and $x_2$ are swapped, then $f(x,y)$ is integrable over $E_1 \times E_2$.
\end{thm}

We will not prove this theorem, but we will see several applications very soon. For now let us mention that it can actually even just help to do some calculations.

A simple example of that would be 

\begin{eg}
Calculate $\int_{[0,1]\times[0,1]}xy d\lambda(x,y)$. Here we don't really have good means to directly calculate the 2d integral, but 
using Fubini we can write
$$\int_{[0,1]\times[0,1]}xy d\lambda(x,y) = \int_{[0,1]}\left(\int_{[0,1]} xy d\lambda(y)\right)d\lambda(x)$$
and now we can easily first calculate $\int_{[0,1]}xy d\lambda{y} = x/2$ and then calculate $\int_{[0,1]}x/2 d\lambda(x) = 1/4$ to obtain $\int_{[0,1]\times[0,1]}xy d\lambda(x,y) = 1/4.$
\end{eg}


\begin{exo}
The aim is to calculate $\displaystyle I = \int_{(0,\infty)}\exp(-x)\frac{\sin^2(x)}{x}d\lambda(x)$. To do this, we define $f(x,y) = \exp(-x)\sin (2xy)$ and use Fubini:
\begin{itemize}
\item Show that $f(x,y)$ is integrable over $(0, \infty) \times [0,1]$
\item Show that when first integrating $y$ over $[0,1]$ we obtain exactly $I$.
\item On the other hand, calculate explicitly the integral by first integrating over $x$. Integration by parts might be useful.
\end{itemize}
\end{exo}


\section{The spaces of integrable and square-integrable functions}

The aim of this section is to define the spaces of integrable functions and square-integrable functions on some Borel set $E$, typically either a box $[0,1]^n \subseteq \R^n$ or $\R^n$ itself. Heuristically, these spaces contain respectively all measurable functions $f$ with either $\int_E |f| d\lambda < \infty$ or $\int_E |f|^2 d\lambda < \infty$. However, to make them into nice Banach spaces we will in fact need to be a bit careful.

\subsection{The spaces of $p$-integrable functions $\Lc^p(E)$}

Let us define two sets of functions mentioned above:
$$L^1(E) := \{f:E \to \R \text{ measurable and } \int_E |f|d\lambda < \infty\}$$
$$L^2(E) := \{f:E \to \R \text{ measurable and } \int_E |f|^2d\lambda < \infty\}.$$
More generally one can define for any $p \geq 1$
$$L^p(E) := \{f:E \to \R \text{ measurable and } \int_E |f|^pd\lambda < \infty\}.$$
As one can similarly define the sets of functions taking values in $\CB$,  one sometimes makes the notation more precise by using $L^p(E, \R)$ and $L^p(E, \CB)$. 

It is not hard to see that these sets come with linear structure - they are closed under sums or multiplying by a constant. 

\begin{lemma}
Let $p \geq 1$ and $f, g$ be in the set $L^p(E)$. Then also for any $a, b \in \R$, the function $af + bg \in L^p(E)$.
\end{lemma}

\begin{proof}
For $p=1$ it follows from pointwise triangle inequality. We have $|f(x) + g(x)| \leq |f(x)| + |g(x)|$ and thus $|f+g|$ is integrable, i.e. in $L^1$

For $p = 2$ we have $|f(x)+g(x)|^2 = f(x)^2 + g(x)^2 + 2f(x)g(x)$ and we use the pointwise inequality $2f(x)g(x) \leq f(x)^2+g(x)^2$ to conclude $f(x)+g(x)$ is square-interable.

For general $p$ the result follows from the pointwise inequality, called the generalized means inequality, which says that for all positive $a, b$ and some constant $c_p > 0$ we have  $(a+b)^p \leq c_p(a^p + b^p)$.


\end{proof}
% GOOD EXAM Q: if f, g integrable, is max(f,g) integrable?

If we want to endow them with a norm, however, we encounter a problem. Indeed, for example the natural norm for the set $L^1(E)$ would be $\|f\|_1 := \int_E |f| d\lambda$. And indeed, it is non-negative and one can check that it satisfies the linear scaling property and the triangle inequality. However, recall Proposition \ref{prop:bpl} - if $\int_E |f| d\lambda = 0$ we only know that $\lambda(\{x \in E: |f(x)| \neq 0\}) = 0$. Thus a priori there are many functions of norm zero!

This might sound unpleasant, but keeping in mind the context of Fourier series and the fact that their limits don't necessarily behave well pointwise, it might also be a blessing! 


\subsubsection{The mathematical definition of $\Lc^p(E)$}

In order to turn $L^p$ into a Banach space (i.e. a complete normed vector space) we will have to deal with this issue of many $0$ norm functions. The simple and natural way to do this is to just consider functions that are almost everywhere equal as equal! Mathematically this means that we define the space of equivalence classes of functions where two functions $f, g$ are considered equivalent when $f = g$ almost everywhere.

\begin{lemma}
Let $f, g \in L^p(E)$. Then the relation $f \sim g$ if $f = g$ almost everywhere, is an equivalence relation.
\end{lemma}

\begin{proof}
One has to just recall and verify the properties of an equivalence relation:
\begin{itemize}
\item reflexivity: $f \sim f$ is clear
\item symmetry: $f \sim g$ if and only if $g \sim f$ is also clear
\item transitivity: if $f \sim g$, $g \sim h$, then $f \sim h$ follows from the fact that $\lambda(f \neq h) \leq \lambda(f \neq g) + \lambda(g \neq h)$ by the union bound.
\end{itemize}
\end{proof}

\begin{defn}[The space $\Lc^p(E)$]
For all $p \geq 1$ we define $\Lc^p(E)$ as the set of all equivalence classes of $L^p(E)/\sim.$ We denote the points in $\Lc^p(E)$, i.e. equivalence classes of functions by $[f], [g], \dots$.
\end{defn}

For example the functions $f(x) = 0$ or $f(x) = 1_\Q$ belong to the same equivalence class are both representatives of $[0]$. To be crystal clear:
\begin{itemize}
    \item We write $L^p(E)$ for the set of measurable functions with finite integral $\int_E |f|^p d\lambda < \infty$
    \item and we write $\Lc^p(E)$ for the quotient space modulo almost-everywhere equality.
\end{itemize}

We want to see that $\Lc^p(E)$ has a nice vector space structure, and in fact can be turned into a normed vector space. But we first need to redefine addition and multiplication by constant.
\begin{itemize}
\item We define $[f] + [g] := [f+g]$, i.e. by taking two representatives $f,g$ of the equivalence classes $[f], [g]$, adding them up and then our sum is defined as the equivalence class of this sum. Huh! This may sound a bit complicated but really is the natural thing to do and importantly the result does not depend on the representative s that we choose: if $f = \hat f$ a.e. and $g = \hat g$ a.e. then also $f+g = \hat f + \hat g$ a.e.
\item Similarly we set $c [f] := [cf]$.
\end{itemize}
Second, we need to define the norm. We first define
\begin{itemize}
\item For any function $f \in L^p(E)$ we set $$\| f \|_p := \left(\int_E |f|^p d\lambda \right)^{1/p}$$
\item And by a slight abuse of notation we set \[ \|[f]\|_p := \| f \|_p,\]
where we have picked some representative $f$ of the equivalence class. It is again clear that the choice of the representative of the equivalence class plays no role in the definition of the norm as if $f =g$ almost everywhere, then also $|f|^p = |g|^p$ almost everywhere.
\end{itemize}

These two definitions reflect a general principle: although the elements of the spaces $\Lc^p(E)$ are equivalence classes of functions, we can and will usually work directly with representatives. In fact we will go as far as sometimes to also use $f$ to denote the equivalence class $[f]$. This comes out to be completely harmnless as long as one is careful that our questions are well-defined in the context of equivalence classes.

Yet some care is needed! For instance, it does not make sense to ask for the value of an element of $\Lc^p(E)$ at a specific point $x$, because changing the value of a function on a set of measure zero---including just one point---does not change its equivalence class. At first this may seem unnatural, but in fact $\Lc^p$ spaces are the natural mathematical setting for quantum mechanics, where the state of a particle is described by a wavefunction in $\Lc^2(\mathbb{R}^n)$. And crucially, no experiment contradicts this modeling choice: we do not---and in practice cannot---observe the value of a wavefunction at a single point.

We are now ready to state the first proposition

\begin{prop}
For every $p \geq 1$, for every Borel set $E \subseteq \R^n$, we have that $\Lc^p(E)$ with addition defined above and the norm given by 
$\|[f]\|_p$ is a normed vector space.
\end{prop}

Here, the $1/p$-th power is somehow necessary to make the norm scale linearly. The triangle inequality is clear for $p=1$ from the usual triangle inequality for the absolute value, but needs some work for the general $p$. As we are only interested in $p = 1, 2$, let us argue it in the case $p = 2$. Again, as above it suffices to argue it for concrete representatives of the equivalence classes.

\begin{lemma}[Triangle inequality for $p = 2$]
For any $f, g \in L^2(E)$, we have that 
$$\|f+g\|_2 \leq \|f\|_2 +\|g\|_2.$$
\end{lemma}

\begin{proof}
By writing out the definitions and squaring both sides we are left to prove
$$\int_E |f+g|^2 d\lambda \leq \int_E (|f|^2 + |g|^2) d\lambda + 2\left(\int_E |f|^2 d\lambda \int_E |g|^2d\lambda\right)^{1/2}.$$
By opening the square on the left hand side this reduces to
$$\int_E fg d\lambda \leq \left(\int_E |f|^2 d\lambda \int_E |g|^2d\lambda\right)^{1/2}.$$
This is the Cauchy-Schwarz inequality for square-integrable functions.
\end{proof}

\begin{thm}[Cauchy-Schwarz inequality]
Let $f, g \in L^2(E)$. Then
$$\int_E |fg| d\lambda \leq \left(\int_E |f|^2 d\lambda \int_E |g|^2d\lambda\right)^{1/2}.$$
\end{thm}

There are tens of proofs of this inequality, which is the cousin of the same inequality in $\R^n$ and has similar interpretations too - the product of the lengths of two vectors is always larger than the length of their inner product. We will also give a proof in this style.
\begin{proof}
By multiplying $f, g$ by a constant we can suppose $\int_E |f|^2 d\lambda = 1$ and $\int_E |g|^2 d\lambda $.

Now for each $x \in E$, we have that $|f(x)g(x)| \leq \frac{1}{2}(f(x)^2+g(x)^2)$ by the arithmetic-geometric mean inequality. Hence
$$\int_E |fg| d\lambda \leq \frac{1}{2}\int_E (|f|^2 + |g|^2) d\lambda = 1 = \left(\int_E |f|^2 d\lambda \int_E |g|^2d\lambda\right)^{1/2},$$
where we used the normalization choice in the last equality.
\end{proof}


\subsection{The Banach space structure of $\Lc^p(E)$}

As we saw with the space of continuous functions, as soon as we have a norm we can start talking about the limits of the elements in the space and about properties like completeness. The key theorem of this section is the following.

\begin{thm}\label{thm:BanachLp}
For every $p \geq 1$, for every Borel set $E \subseteq \R^n$, we have that $(\Lc^p(E), \| \cdot \|_p)$ is a Banach space.
\end{thm}
Recall that the additional point here was completeness - the fact that every Cauchy sequence converges. 

We will not prove this theorem, but let us discuss some ingredients that are interesting and important also on their own. The first obvious question that required already a bit of thought in the case of $C([0,1], \R)$ was: given a Cauchy sequence how do we identify the limiting function?

As a first naive thought, one could imagine that maybe being Cauchy w.rt. $\| \cdot \|_1$ will tell us something about convergence pointwise or at least convergence almost everywhere. This is unfortunately not the case.
\begin{itemize}
\item First, recall that pointwise convergence does not imply convergence of integrals and in particular does not imply convergence in $L^p$. For example, the sequence $f_n = n1_{(0,1/n)}$ converges pointwise to $f = 0$ but $\|f_n - f\|_1 =1$ for all $n$ and thus $f_n$ does not converge to $f$ w.r.t the norm $\|\cdot\|_1$.
\item Conversely, it is clear that if $f_n$ converges to $f$ w.r.t. $\| \cdot \|_1$, then we can at most hope for almost everywhere convergence of $f_n(x)$ to $f(x)$. Indeed, if $f_n$ to $f$ w.r.t. $\|\cdot \|_1$, then also $f_n$ to $g$ w.r.t. $\| \cdot \|_1$ for any $g$ that is only almost everywhere equal to $f$. 
\item However, what is maybe a bit surprising is that we can find a sequence of functions converging w.r.t. $\|\cdot\|_1$ but converging nowhere pointwise - i.e. a sequence of functions $f_n$ such that $\|f_n - f\|_1 \to 0$ and yet $f_n(x)$ does not converge for any $x$!
\end{itemize}

Let us look at an example illustrating this second point.

%\begin{eg}
%First, it is relatively easy to construct examples such that $\|f_n(x) - f_n\|_1 \to 0$ but $f_n(x)$ does not converge at some finite set of $x$. For example let's define 
%$$f_n(x) = \sum_{i = 1 \dots m}1_{((i-1/n)/m, (i+1/n)/m))}(x).$$
%Then for $n \geq 2m$ we have that $\|f_n(x)\|_1 = 2m/n$ which converges to $0$. In other words $f_n \to 0$ w.r.t. $\|\cdot \|_p$. However, $f_n(x) = 1$ for all $x = 1/m, \dots, 1$ and for all $n \geq 1$ and thus $f_n(x)$ does not converge to zero at these points. 
%\end{eg}  
\begin{eg}
The idea is to construct a 'travelling diminishing interval' that firstly visits each point of the domain infinitely many times, such that the values of $f_n(x)$ keep on fluctuating as $n$ changes, and that secondly gets smaller and smaller to obtain convergence w.r.t. $\|\cdot\|_1$.

Let us formalise this idea. For $n \in \N$ let $k_n$ be such that $2^{k_n} \leq n < 2^{k_n+1}$. We define on $[0,1]$ a sequence of functions by $f_n(x) := 1_{[2^{-k_n}(n-2^{k_n}),2^{-k_n}(n+1-2^{k_n})]}$. Then $f_n$ are all integrable and observe that $\|f_n\| = 2^{-k_n}$, which goes to zero as $n \to \infty$. On the other hand for every $x \in [0,1]$ there are infinitely many $n$ such that $f_n(x) = 0$ and infinitely many $n$ such that $f_n(x) = 1$ and hence $f_n(x)$ cannot converge.
\end{eg}

Next time we will see why $\Lc^p(E)$ is nevertheless complete.

To understand why $\Lc^p(E)$ still has changes to be complete despite this failure, let us first look at a very useful lemma, which says that functions that are close in the $\| \cdot \|_1$ norm are still also close pointwise in the sense of measure.

\begin{lemma}[Markov inequality]\label{lem:Markov}
Suppose $f$ is integrable and non-negative. Then $\lambda(\{x: f(x) > c\}) \leq \frac{\int f d\lambda}{c}$.

In particular, if $f, g \in L^p(E)$ satisfy $\|f-g\|_p \leq \eps$, then $\lambda(\{x: |f(x) - g(x)| > c\}) \leq (\eps / c)^p$. 
\end{lemma}

\begin{proof}
%We have 
%$$\eps \geq \|f -g \|_1 \geq \|(f-g)1_{|f-g| > \lambda} > \lambda\lambda(\{x: |f(x) - g(x)| > \lambda\}),$$
%where at the last step we used that $\int_E d\lambda(x) = \lambda(E)$.
On the exercise sheet
\end{proof}

In particular, if two functions are close in the sense of $\|\cdot\|_1$ they can differ pointwise even macroscopically, but only on a small set.

This helps us prove the following key observation, which says that if a sequence of functions converges sufficiently fast to its limit w.r.t. $\|\cdot \|_p$, then it does converge almost everywhere! The theorem is thereafter a simple exercise that I leave for the very motivated.

\begin{lemma}
Let $p \geq 1$ and $f_n$ be a sequence in $L^p(E)$ converging to $f \in L^p(E)$ w.r.t $\|\cdot \|_p$ such that $\|f_n - f\|_p \leq \eps_n$ with $\sum_{n \geq 1} \eps_n^p < \infty$. Then $f_n$ converges to $f$ almost everywhere.
\end{lemma}

\begin{rem}
This is a very close cousin of a result called the Borel-Cantelli lemma in probability theory.
\end{rem}

\begin{proof}
We write
\[\{x: f_n(x) \nrightarrow f(x)\} \subseteq \bigcup_{k \geq 1}\bigcap_{m \geq 1}\bigcup_{n \geq m}\{x: |f_n(x) - f(x)| \geq 1/k\}\]
From the union bound
\[\lambda(\bigcup_{k \geq 1}\bigcap_{m \geq 1}\bigcup_{m \geq n}\{x: |f_m(x) - f(x)| \geq 1/k\}) \leq \sum_{k \geq 1} \lambda(\bigcap_{m \geq 1}\bigcup_{n \geq m}\{x: |f_n(x) - f(x)| \geq 1/k\}).\]
Further  for each $k$
\[\lambda(\bigcap_{m \geq 1}\bigcup_{n \geq m}\{x: |f_m(x) - f(x)| \geq 1/k\} \leq \lambda(\bigcup_{n \geq m_k}\{x: |f_n(x) - f(x)| \geq 1/k\}),\]
which we can again by union bound $\sum_{n \geq m_k}\lambda(\{x: |f_n(x) - f(x)| \geq 1/k\}).$ By the Lemma \ref{lem:Markov} and the hypothesis we have that for any fixed $n, k$:
\[\lambda( \{x: |f_n(x) - f(x)| \geq 1/k\}) \leq k^p\eps_n^p.\]
Fix $\delta > 0$. As $\sum_{n \geq 1}\eps_n^p < \infty$, we can for each $k$ choose $m_k$ such that the tail is as small as we wish:
\[\sum_{n \geq m_k} k^p\eps_n^p < \delta k^{-2}.\]
Putting all together this gives us 
\[\lambda(\{x: f_n(x) \nrightarrow f(x)\}) \leq \delta \sum_{k\geq 1}k^{-2},\]
which can be made arbitrarily small by taking $\delta > 0$ arbitrarily small. This means that in fact $\lambda(\{x: f_n(x) \nrightarrow f(x)\})  = 0$ and we have proved the lemma.
\end{proof}

%which says that if a sequence of functions is Cauchy in $L^p$, then some subsequence of it converges almost everywhere and in $L^p$.
%In particular, as every convergent sequence is Cauchy (why?), it means that every sequence of functions converging w.r.t. $\|\cdot\|_1$ to some $f$, has an almost everywhere convergent subsequence to a function that is equal to $f$ almost everywhere. 

%\begin{lemma}
%For $p \geq 1$, let $(f_n)_{n \geq 1}$ be Cauchy sequence of functions in $L^p(E)$, i.e. we know that for every $\eps > 0$ there is some $n_\eps \in \N$ with $\|f_m - f_n\|_p < \eps$ for all $m, n \geq n_\eps$. 

%Then we can pick a subsequence $(f_{n_k})_{k \geq 1}$ and find a function $f \in L^p(E)$ such that $f_{n_k}$ converges to $f$ almost everywhere and w.r.t. $\| \cdot \|_p$ as $k \to \infty$. 
%\end{lemma}

%This lemma gives us the desired limiting function and implies Theorem \ref{thm:BanachLp}. Indeed, for the limiting $f$ given by the lemma, we have $$\|f - f_n\|_p \leq \|f_{n_k} - f_n \|_p - \|f - f_{n_k}\|_p$$ by the triangle inequality. This can be made smaller than any $\eps$ by first taking $n_k$ large enough so that the second term is less than $\eps/2$ (Lemma) and in the Cauchy condition $\|f_n - f_m\|_p < \eps/2$ for all $n \geq n_k$. Then the inequality holds for all $n \geq n_k$. Thus we conclude that $(f_n)_{n \geq 1}$ also converges to $f$ w.r.t. $\|\cdot \|_p$.


\subsection{The space of square-integrable functions}

So far everything we did was true for $\Lc^p(E)$ with any $p \geq 1$. But in fact the spaces do not behave equivalently as comparing the following two results, stated informally, shows:
\begin{itemize}
\item Kolmogorov, when he was 19 years old, showed that there exists a function $f \in L^1([0,1])$ such that its Fourier series diverges everywhere and in particular does not converge w.r.t. $\|\cdot\|_1$.
\item However, for every $p > 1$ and every $f \in L^p([0,1])$, the Fourier series converges almost everywhere and w.r.t. $L^p([0,1])$.
\end{itemize}
Both are highly non-trivial results, hinting at potential subtle differences between $L^p$ spaces. We will only prove a part of the second result, in particular that for every $f \in L^2([0,1])$ its Fourier series converges w.r.t. $L^2$.

\subsubsection{The scalar product on $\Lc^2(E)$}

Recall the following facts that are common to all $\Lc^p(E)$ spaces:
\begin{itemize}
\item $L^2(E):=\{f:E \to \R: f^2 \text{ integrable}\}$
\item $\Lc^2(E)$ the set of equivalence classes $L^2(E) / \sim$ under $f \sim g$ iff $f = g$ almost everywhere
\item With the norm: $$\|f\|_2 := \sqrt{\int_E |f|^2 d\lambda},$$
$\Lc^2(E)$ is a Banach space.
\end{itemize}

Now, compared to $\Lc^1(E)$ (or indeed any $\Lc^p(E)$ with $p \neq 2$), the space $\Lc^2(E)$ has even more structure. In addition to a norm, one can also define a notion of an inner product / angle, very much similar to the inner product on $\R^n$. This works as follows.

For any $f, g \in L^2(E)$ we set
$$\langle f, g \rangle_2 := \int_E f g d\lambda,$$
\footnote{For $L^2(E, \CB)$ the inner product is $\int_E f \bar g d\lambda$, where $\bar g$ denotes the complex conjugate}. This assumes that $f g$ is integrable, which is clear from $|f(x)g(x)| \leq |f(x)|^2 + |g(x)|^2$ or from Cauchy-Schwartz.

One can verify that on $L^2(E)$ (and also $\Lc^2(E)$, the definition of $\langle f, g \rangle_2$ above satisfies the axioms of an inner product:

\begin{defn}[Inner product]
Let $V$ be a vector space. We call $\langle v,w \rangle$ a real inner product if the following conditions hold:
\begin{itemize}
\item Real-valued: $\langle v,w \rangle \in \R$
\item Symmetry: $\langle v, w \rangle = \langle w, v \rangle$ 
\item Linearity: for all $a, b \in \R$, we have $\langle av + bu, w \rangle = a \langle v, w \rangle + b \langle u, w \rangle$
\item Non-negativity: $\langle v, v \rangle \geq 0$ with equality if and only if $v = 0$.
\end{itemize}
We call it a complex inner product if it is real-valued, conjugate symmetric and linearity holds only for $a, b \in \CB$. A vector space $V$ endowed with an inner product is called an inner product space.
\end{defn}

In fact each inner product gives rise to a norm. 

\begin{exo}
Consider an inner product $\langle \cdot, \cdot \rangle$ defined on a vector space $V$. Then $\| v \| := \sqrt{\langle v, v \rangle}$ defines a norm.
\end{exo}
The key is to notice that the Cauchy-Schwartz inequality holds for every norm that stems from an inner product: $\langle v, w \rangle^2 \leq \langle v, v\rangle \langle w, w\rangle$!

By our definitions for $f \in L^2(E)$ we have that $\|f\|_2^2 = \langle f, f \rangle$, i.e. the inner product gives rise to our previously defined norm. 

An important thing to notice is that not every norm can stem from an inner product. Indeed, coming from an inner product forces some algebraic conditions on the norm. For example, the so called parallelogram law has to be always satisfied for a norm compatible with a real inner product:
\begin{lemma}[Parallelogram law]
Suppose on a vector space the norm $\|\cdot\|$ is compatible with a real inner product $\|f\|^2 = \langle f, f \rangle_2$. Then
$$\|v+w\|^2 + \|v-w\|^2 = 2(\|v\|^2+\|w\|^2).$$
\end{lemma}

\begin{proof}
This comes from a direct computation using the linearity of $\langle f, f \rangle_2$.
\end{proof}
Now we can test this for $\|\cdot \|_p$ on say $L^2([0,1])$. Consider $v = 1_{[0,1/2]}$ and $w = 1_{[1/2,1]}$. Then we get for the LHS $2$ and for RHS $2^{-2/p+2}$, These are equal if and only if $p = 2$!
This shows that no other $\| \cdot \|_p$ on $[0,1]$ can stem from an inner product.

\subsubsection{Orthonormal bases of $\Lc^2([0,1])$ and Fourier series}
The key result that makes Fourier series work so nicely for functions in $L^2([0,1])$ is the following statement. Informally we could just say:

\begin{thm}
The set of functions $(\sqrt{2}\sin(2\pi n x))_{n \geq 1}, (\sqrt{2}\cos(2\pi nx ))_{n \geq 1}$ together with the constant function $1$ form an orthonormal basis of $L^2([0,1])$.
\end{thm}

This would be philosophically correct, just one would need to just interpret everything under the flag of almost surely. A formally precise statement is:

\begin{thm}\label{thm:orthtrig}
The set of equivalence classes $([\sqrt{2}\sin(2\pi n x)])_{n \geq 1}, ([\sqrt{2}\cos(2\pi nx )])_{n \geq 1}$ together with the equivalence class of the constant function $[1]$ form an orthonormal basis of the vector space $(\Lc^2([0,1]), \langle\cdot, \cdot \rangle_2)$.
\end{thm}

Here, the orthonormal basis for a (potentially infinite-dimensional) inner product space is a direct generalisation from the finite-dimensional case.

\begin{defn}[Orthonormal basis]
Let $(V, \langle \cdot, \cdot, \rangle)$ be an inner product space. We call $(v_i)_{i \geq 1}$ an orthonormal basis of $V$ if the following two conditions hold
\begin{itemize}
\item Orthogonality: for all $i, j \geq 1$ we have that$\langle v_i, v_j \rangle = 1_{i = j}$
\item Basis condition: Each $w \in V$ can be written as $\sum_{i \geq 1} c_i v_i$, where $c_i \in \R$ and the convergence is w.r.t. the norm $\|v\| := \langle v, v \rangle^{1/2}$.
\end{itemize}
\end{defn}
Orthogonal basis have several useful properties mirroring those of the finite-dimensional setting:
\begin{lemma}\label{lem:orthcon}
Let $(V, \langle \cdot, \cdot \rangle)$ be an inner product space and $(v_i)_{ i \geq 1}$ an orthonormal basis. Then
\begin{itemize}
\item If $w \in V$ has a writing $w = \sum_{i \geq 1} a_i v_i$, then $\|w\|^2 = \sum_{i \geq 1}a_i ^2$.
\item The writing $w = \sum_{i \geq 1} a_i v_i$ is unique and each coefficient is given by $a_i = \langle w, v_i \rangle$.
\end{itemize}
\end{lemma}

\begin{proof}
For the first property, let $w_N := \sum_{i = 1}^N a_i v_i$. Then 
$$\|w_n\|^2 = \langle w_N, w_N\rangle = \langle \sum_{i = 1}^N a_i v_i, \sum_{i = 1}^N a_i v_i \rangle = \sum_{i \geq 1}^N a_i^2,$$
where in the last equality we used linearity and orthogonality. But now $\|w_n\|^2 \to \|w\|^2$ - indeed, 
$$|\|w_n\|^2 -\|w\|^2| = |\|w_n\| - \|w\||(\|w_n\|+\|w\|),$$
which goes to zero as $\|w_n-w\| \to 0$ by assumption.

In particular, the first part directly implies that if $\sum_{i \geq }a_i v_i = 0$ then $a_i = 0$ for all $i \geq 1$. The uniqueness is on the exercise sheet. %To show the uniqueness of expansion for general $w$
%it now suffices to show that if $w = \sum_{i \geq 1} a_i v_i$ and $w = \sum_{i \geq 1} b_i v_i$, then $0 = \sum_{i \geq 1}(a_i-b_i)v_i$. 
%write again $w_N = \sum_{i = 1}^Na_i v_i$ and notice here by direct calculation (like in the finite-dimensional case) $a_i = \langle w_N, v_i \rangle$. But as $\|w_N - w\| \to 0$, we conclude that $a_i = \langle w_N, v_i \rangle \to \langle w, v_i\rangle$, which gives both the uniqueness of the expansion and the value of each $a_i$. 
%We leave this and the final part to the exercise sheet.
\end{proof}

Notice that in particular this means that Theorem \ref{thm:orthtrig} rather directly implies the following!

\begin{thm}[Fourier series in $L^2$]
Let $f \in L^2([0,1]$. If we define the partial Fourier series by
$$S_N(f) := \langle f, 1\rangle_2 + \sum_{n=1}^N2\langle f, \sin (2\pi n x)\rangle_2 \sin(2\pi n x) + \sum_{n=1}^N 2\langle f, \cos(2\pi n x)\rangle_2\cos(2\pi n x),$$
then $\|f - S_N(f)\|_2 \to 0$ as $N \to \infty$, i.e. the partial Fourier series of $f$ converge to $f$ w.r.t. to the $L^2$-norm.
\end{thm}
Let us now discuss the proof of Theorem \ref{thm:orthtrig}. First, notice that all of $\sin(2\pi nx), \cos(2\pi m x)$ are continuous and bounded on $[0,1]$ and thus in $L^p([0,1])$ for all $p \geq 1$. 

%Thus it really remains  to prove the basis condition. This is not entirely pleasant to do by hand but follows from two lemmas of general interest.

Second, orthogonality follows from Lemma \ref{lem:orthtrig} as firstly it suffices to show it for chosen representatives of the equivalence classes and second, everything is Riemann integrable and we can thus use the computation we did also for the Lebesgue integral. 

To make use of this orthogonality one further needs the following, that mirrors the lemma above about complete orthogonal systems. The proof of this is on the exercise sheet
\begin{lemma}
Let $v_1, v_2, \dots$ be orthonormal vectors in a complete inner product space $V$. Then for any $w \in V$, we have that $\widehat w := \sum_{i \geq 1} \langle v_i, w\rangle $ is well-defined and satisfies 1) $\|\widehat w \| \leq \|w\|$ and 2) $\langle w-\widehat w, v_i\rangle = 0$ for all $i \geq 1$.
\end{lemma}
What we obtain from this lemma is the following. Let $w \in L^2([0,1])$ and set 
$$\widehat w := \langle w, 1\rangle + \sum_{n \geq 1}2\langle w, \sin(2\pi nx)\rangle \sin(2\pi n x) + \sum_{n \geq 1}2\langle w, \cos(2\pi nx)\rangle \cos(2\pi n x).$$
Then by the lemma above this is well defined and $w - \widehat w$ is orthogonal to all the $\sin(2\pi n x), \cos(2 \pi n x)$ and the constant function. If we are able to show that in fact $w - \widehat w$ is zero almost everywhere, then have proved the completeness of the basis, i.e. the spanning property of the basis. Thus it remains to show the following lemma.

\begin{lemma}
Suppose $w \in L^2([0,1])$ is orthogonal to all the $\sin(2\pi n x), \cos(2 \pi n x)$ and the constant function w.r.t. $\langle \cdot, \cdot \rangle_2$, then $w$ is the zero function. 
\end{lemma}

Recall that we already proved such a result in case $f$ was a continuous function! So this is a generalisation, but one can again use the Fjer kernel to conclude (we leave it in the non-examinable part of the example sheet).   

In fact there are many useful orthonormal bases on $[0,1]$. For example:

\begin{lemma}\label{lem:sinorcosbasis}
The set of functions $(\sqrt{2}\sin(\pi n x))_{n \geq 1}$ forms an orthonormal basis of $L^2([0,1])$ as does the set of functions  $(\sqrt{2}\cos(\pi n x))_{n \geq 1}$ together with the constant function $1$.
\end{lemma}

This might look a bit odd - we had a basis using both $\sin$ and $\cos$ functions, and now we say that we can use just one of the two. But observe that we are now using more of them - instead of $2\pi nx$ we now have in the argument $\pi n x$.

To see the link between the two clearly let us first observe that by scaling and translation we have that:

\begin{lemma}
Let $L \in \N$, then an orthonormal basis of $L^2([-L/2, L/2))$ is given by the set of functions: $(\sqrt{\frac{2}{L}}\sin(\frac{2}{L}\pi n x))_{n \geq 1}, (\sqrt{\frac{2}{L}}\cos(\frac{2}{L}\pi nx ))_{n \geq 1}$ together with the constant function $\frac{1}{\sqrt{L}}$. 

For the complex $L^2([-L/2, L/2], \CB)$ they are given in a nice compact form by $(\frac{1}{\sqrt{L}}\exp(2\pi inx/L)_{n \in \Z}$.
\end{lemma}

Then looking at the functions over $[-1,1]$ gives us the link between the different basis and proves the lemma.
\begin{proof}[Proof of Lemma \ref{lem:sinorcosbasis}]
We have that $(\sin(\pi n x))_{n \geq 1}, (\cos(\pi nx ))_{n \geq 1}$ together with the function $1/2$ form an orthonormal basis of $L^2([-1,1])$.

But each $f \in L^2([-1,1])$ can be written as a sum $f = f_o + f_e$ of an odd function $f_o(x) = -f_o(-x)$ and even function $f_e(x) = f_e(-x)$. Concretely, $f_o(x) = \frac{f(x) - f(-x)}{2}$ and $f_e(x) = \frac{f(x) + f(-x)}{2}$.

Now, notice that each even function is orthogonal to an odd function over $L^2([-1,1])$! Moreover, for each $n$, $\sin(\pi n x)$ is odd and $\cos(\pi n x)$ is even. Thus any even function in $L^2([-1,1])$ can be expanded only using $\cos$ and the constant function, and every odd function just using $\sin$.

On the other hand, every $g \in L^2([0,1])$ can be extended to either an even function on $[-1,1]$ by defining $g_e(x)$ defined as $g(x)$ on $[0,1]$ and as $g(-x)$ on $[-1,0]$, or an odd function if we define $g_o(x)$ as $g(x)$ on $[0,1]$ 
 and as $-g(-x)$ on $[-1,0]$. Their expansions in $L^2([-1,1])$ then descend to the $\sin$ or $\cos$ expansions on $[0,1]$.
 \end{proof}

A nice consequence is that we can now fully solve the heat equation on $[0,1]$.

\begin{thm}
The heat equation 
$$\frac{\partial u(t,x)}{\partial t} = D\Delta u(t,x)$$
on $[0,1]$ with the initial condition $u(0,x) = u_0(x) \in L^2([0,1])$ has a unique solution when we either fix:
\begin{itemize}
\item Periodic boundary conditions: $u_t(0) = u_t(1)$ and $u_t'(0) = u_t'(1)$
\item Dirchlet boundary conditions: $u_t(0) = u_t(1) = 0$,
\item Neumann boundary conditions: $u_t'(0)=u_t'(1) = 0$.
\end{itemize}
In each cases the solution is explicit and is given by 
$$u(t,x) = \langle u_0, 1\rangle + \sum_{n \geq 1}\exp(-D4\pi^2 n^2 t)\left(\sin(2\pi n x) \langle u_0, 2\sin(2\pi nx)\rangle +  \cos(2\pi n x) \langle u_0, 2\cos(2\pi nx)\rangle\right)$$
in the case of periodic conditions. By
$$u(t,x) = \sum_{n \geq 1}\exp(-D\pi^2 n^2 t)\sin(\pi n x) \langle u_0, 2\sin(\pi nx)\rangle $$
in the case of Dirichlet conditions. And by 
$$u(t,x) = \langle u_0, 1\rangle + \sum_{n \geq 1}\exp(-D\pi^2 n^2 t) \cos(\pi n x) \langle u_0, 2\cos(\pi nx)\rangle$$
in the case of Neumann boundary conditions.
\end{thm}

\begin{proof}
As $u_0 \in L^2([0,1])$ we know that any of the given expansions is true at $t = 0$ by the fact that that all three series form ON basis.

Also as $\exp(-D4\pi^2 n^2 t) \leq 1$ or $\exp(-\pi^2n^2 t) \leq 1$ we also see that in all cases $u_t(x)$ in the statement is in $L^2([0,1])$ for every $t > 0$. It is then easy to check that every proposed solution is twice-differentiable in $x$ and satisfies the equation.

Thus it just remains to argue that the solution is unique. To do this suppose there are two solutions 
$u(t,x)$ and $v(t,x)$ from the same initial condition and same boundary conditions.
Then also $w(t,x) = u(t,x) - v(t,x)$ is a solution starting from the zero function at $t = 0$.

Now observe that 
$\|u_t(x)\|^2$ is decreasing in time. Indeed, we write
$$\frac{1}{2}\frac{\partial \|u_t(x)\|^2}{\partial t} = \int_{[0,1]}u_t(x) \frac{\partial u(t,x)}{\partial t} d\lambda(x)$$ 
which using heat equation can be written as 
$$D\int_{[0,1]}u_t(x) \Delta u_t(x) = -D\int_{[0,1]}|\nabla u_t(x)|^2 + u_t(1)u'_t(1)-u_t(0)u'_t(0).$$
Using one of the three boundary conditions we conclude that
$\frac{1}{2}\frac{\partial \|u_t(x)\|^2}{\partial t} \leq 0$ and thus indeed $\|u_t(x)\|^2$ is decreasing in time. 

This is called an energy estimate and here the justification of all the steps will be left for the exercises sheet, but in the first step we just changed the order of integration and derivation and wrote out the time-derivative of $u_t(x)^2$ and thereafter we used integration by parts. This in particular means that if $u_0(x) = 0$ then $u_t(x) = 0$ for all $t > 0$ giving uniqueness.
\end{proof}

\begin{rem}
In fact one can solve also the heat equation using so called mixed or Robin boundary conditions, we will try to find a more general conceptual framework that also fits those soon enough.
\end{rem}

\subsection{Fourier transform}

We have now seen how to solve the heat equation on any interval using the Fourier series, that gave us a convenient orthonormal basis. This generalises nicely to boxes of the form $[-L/2, L/2]^n \subseteq \R^n$. But what about $\R$ and $\R^n$?

First notice that when we considered Fourier basis on $[-L/2, L/2]$ then in the complex 
form they were given by $(\frac{1}{\sqrt{L}}\exp(2\pi i nx/L)_{n \in \Z}$. So in some sense as $L \to \infty$ the set of frequencies we consider becomes more and more dense. And indeed, over $\R$ the Fourier series becomes an integral, called the Fourier transform:

\begin{defn}[Fourier transform]
Let $f$ be a real or complex-valued function $f \in L^1(\R)$.
We can define the Fourier transform $\F (f) (k): \R \to \CB$ by 
$$\hat f(k) := \F(f)(k) := \int_\R f(x) \exp(-i 2\pi k x)d\lambda(x).$$
\end{defn}
This is well defined for each $k$ again as $f(x)$ is integrable and $|f(x) \exp(-i 2\pi k x)| = |f(x)|$.

We would hope that the Fourier transform would work as nicely on $L^2(\R)$ as the Fourier series on $L^2([0,1])$, but notice some differences:
\begin{itemize}
    \item First, there is a question question of integrability. Whereas on $[0,1$ each square-integrable function is also integrable, its Fourier series is well-defined because $f(x)\exp(-2\pi i k x)$ is integrable for any $k$. But $L^2(\R)$ is not a subset of $L^1(\R)$: there are functions in $L^2(\R)$ that are not integrable (for example the function $f(x) = (|x|+1)^{-1}$) and hence there is no reason that $f(x)\exp(-2\pi i kx)$ should be integrable for $f \in L^2(\R)$.
    \item Second, contrary to the case of intervals, we are using a non-countable number of waves over $\R$. Moreover notice that none of the wave functions $\exp(i2\pi x k)$ itself is square-integrable over $\R$ (or integrable for that sake!)
\end{itemize}

Still, things work again nicely for $\Lc^2$ spaces, explaining why they are so important:

\begin{thm}[Fourier transform on square-integrable functions]\label{thm:FTl2}
The Fourier transform 
$$\F(f)(k) := \hat f(k) := \int_{\R} f(x)\exp(-2\pi i kx)d\lambda(x)$$ 
defined on $\Lc^2(\R) \cap \Lc^1(\R)$ has a unique extensions to an operator $\tilde \F$ on the whole of $\Lc^2(\R)$ such that $\tilde \F: \Lc^2(\R) \to \Lc^2(\R)$ is an isomorphism of complete vector spaces with an inner product.

Moreover, the inverse operator is given similarly by an extension of 
$$\F^{-1}(f)(x) := \int_{\R} \hat f(k) \exp(2\pi i kx) d\lambda(k)$$
from the subset $\hat f \in \Lc^2(\R) \cap \Lc^1(\R)$.

We will often abuse the notation and denote the-above mentioned extensions also by $\F$ and $\F^{-1}$. 
\end{thm}

Already the statement of this result looks a bit unpleasant with this extension and so! But this is necessary as for a general function $f \in \Lc^2(\R)$ the integral just doesn't make directly sense.

There are several ways to prove this theorem, We will discuss a way that goes via approximations from larger and larger intervals and thus also helps to connect the Fourier transform back to the Fourier series. 

In fact we will start checking that everything works well for a subset of functions.

\begin{prop}
Let $f \in L^2(\R) \cap L^1(\R)$ be such that $\hat f$ has a decay $|\hat f(k)| \leq Ck^{-2}$ for some constant $C$ and $f(x) = 0$ outside some interval $[-M, M]$. Then we have the following Fourier inverse transform: for almost every $x \in \R$ we can write
\begin{equation}\label{eq:ft}
f(x) := \int_{\R} \hat f(k) \exp(2\pi i xk) d\lambda(k).
\end{equation}

Moreover, we have the Plancherel formula:
$$\|f\|_2^2 := \int_{\R} |f(x)|^2 d\lambda(x) = \int_{\R} |\hat f(k)|^2 d\lambda(k) =: \|\hat f\|_2^2.$$
\end{prop}

First, using the orthonormal basis $(\sqrt{\frac{1}{L}}\exp(2 \pi i L^{-1} nx))_{n \in \Z}$ we can write
\begin{equation}\label{eq:ft1}
f(x) = L^{-1}\sum_{n \in \Z}\hat f_L(n/L)\exp(2\pi i L^{-1}nx),
\end{equation}
where the summing is absolute for any $x \in [-L/2, L/2]$ and in the sense $L^2$, and 
\begin{equation}\label{eq:ft2}
\hat f_L(n/L) := \int_{[-L/2, L/2]}f_L(x)\exp(-2\pi i L^{-1}nx) d\lambda(x).
\end{equation}
%%% Exercise on scaling
Further, this Fourier series can be extended to a function on $\R$ by setting $\hat f_L(x) := f_L(L^{-1}\lfloor x L \rfloor)$. 

\begin{proof}
Consider $f$ as in the statement and let $L \geq 2M$.
Notice that $\hat f_L(n/L) = \hat f(n/L)$ for all $n \in \Z$. Further, as $|f(x) \exp(-2\pi L^{-1}nx)| \leq |f(x)|$ we can first use the Dominated convergence theorem to show that in fact for every $k \in \R$ we have that
$\hat f_L(k) \to \hat f(k)$ as $L \to \infty$, and thus the Fourier coefficient on $\R$ corresponds to the approximated Fourier coefficients. 

We will now check that under our conditions \eqref{eq:ft1} becomes in the limit $L \to \infty$ the inverse Fourier transform \eqref{eq:ft}.  First, given the decay of $\hat f$ we can write the RHS of \eqref{eq:ft1}  as the Lebesgue integral of the $\hat f_L$ extended to $\R$, i.e. 
$$L^{-1}\sum_{n \in \Z}\hat f_L(n/L)\exp(2\pi i L^{-1}nx) = \int_{\R} \hat f_L(k) \exp(2\pi iL^{-1}\lfloor k L \rfloor x)d\lambda(k).$$

Now $\hat f_L(k) \exp(2\pi iL^{-1}\lfloor k L \rfloor x) \to \hat f(k) \exp( 2\pi i k x)$ pointwise, and we can use the Dominated convergence theorem and the assumed decay of $\hat f$ to conclude that 
$$\int_{\R} \hat f_L(k) \exp(2\pi iL^{-1}\lfloor k L \rfloor x) d\lambda(k) \to \int_{\R} \hat f(k) \exp(2 \pi i k x) d\lambda(k).$$
Finally, the Plancherel identity for $f$ on the interval $[-L/2, L/2]$ implies that
$$\|f\|_2^2 = L^{-1}\sum_{n \in \Z}|\hat f_L(n/L)|^2.$$
It remains to notice that the RHS converges to $\|\hat f\|_2^2$ - to do this we can argue like above using pointwise convergence and then dominated convergence that we obtain from the assumed decay of $\hat f(k)$.
\end{proof}

To finish the proof of Theorem \ref{thm:FTl2} it remains to do two things:
\begin{enumerate}
\item Check that this special set of functions we consider in the proposition is dense in $\Lc^2(\R)$ w.r.t. the $\|\cdot \|_2$ norm meaning that for every $f \in L^2(\R)$ we can find some sequence $f_n$ such that $f_n \to f$ w.r.t. the $\|\cdot\|_2$ norm and $f_n$ satisfies the conditions of the proposition. 

This can be argued as follows. First, observe that we can always approximate $f$ by $f1_{-n,n}$ arbitrarily well.

Thus it suffices to show that we can approximate each $f1_{[-n,n]}$ arbitrarily well by some functions whose Fourier transforms behave well, i.e. decay like $Ck^{-2}$. But actually we have seen such functions! We just consider the heat equation on the interval $[-n,n]$ starting from $u_0 := f1_{[-n,n]}$ - we saw both that $u_t \to u_0$ in $L^2$ norm but also that $u_t$ has a very rapidly decreasing Fourier transform! The details of this second part of the argument are non-examinable and are on non-examinable part of the exercise sheet.
\item Find an argument to extend the Fourier transform using this density. We will see this argument in a more general set-up in the following section.
\end{enumerate}

\section{Hilbert spaces and linear operators}

We will now try to put the theory of $L^2$ spaces, their nice interplay with Fourier series and its helpfulness to solve PDEs into a more general and abstract setting. This will allow in particular to generalise our method of solving the heat equation to a wider range of equations.

\subsection{Hilbert spaces}

The spaces $\Lc^2$ are a prime example of what one calls Hilbert spaces. A Hilbert space is an inner product space that is complete, or in other words a Banach space  with an inner product. 
\begin{defn}[Hilbert space]
Let $(H, \| \cdot \|)$ be a Banach space. If $H$ can be endowed with a compatible inner product $\langle f, g \rangle$, i.e. such that $\| f \|^2 = \langle f, f \rangle$, then we call $H$ a Hilbert space.
\end{defn}

Mostly one encounters only certain class of nice Hilbert spaces that have countably dense subset in the following sense.

\begin{defn}[Separable Hilbert space]
Let $H$ be a Hilbert space. We call $H$ separable if there is some countable set $S$ such that for every $h \in H$ and every $\eps > 0$ there is some $s \in S$ with $\|h-s\| < \eps$.
\end{defn}

What makes these Hilbert spaces extremely nice is the fact that they admit a countable basis.

\begin{thm}
A Hilbert space $H$ is separable if and only if there exists an orthonormal basis $(h_n)_{n \geq 1}$ of $H$.
\end{thm}

For the space $\Lc^2([0,1])$ we showed that the trigonometric functions form an orthonormal basis and thus we know that it is also separable. We could concretely find this countable dense set by looking at all $L^2$ functions with rational Fourier coefficients. 

It then follows that $\Lc^2([a, b])$ is also separable for any $a < b \in \R$. From there one can conclude that $\Lc^2(\R)$ is separable and also that $\Lc^2([a_1, b_1] \times \dots [a_n, b_n])$ and $\Lc^2(\R^n)$ are separable. Some of these considerations are on the example sheet.

Let us argue how to prove the theorem. One part is relatively easy. Namely, given a countable orthonormal basis $(h_n)_{n \geq 1}$, we can consider expansions $\sum_{n \geq 1}a_n h_n$ with $a_n \in \Q$ and $\sum a_n^2 < \infty$. There are counatbly many such functions and they are dense in $H$. 

The other part follows from Gram-Schmidt orthogonalization in two steps as we will explain now.\\

\paragraph{Step 1:}
As a first step, one extracts from our dense countable set $S$, a subset $S_B$ that is linearly independent.

So consider the countable set $S = \{h_1, h_2, \dots \}$ whose closure is equal to $H$. We can now make it a bit smaller as follows:
\begin{enumerate}
\item we set $S_B^1 := \{h_1\}$;
\item at step $m$ we include $h_m$ in $S_B^m$ if and only if there is no linear combination of elements in $S_B^{m-1}\cup \{h_m\}$ that gives $0$;
\item we define $S_B = \cup_{i \geq 1} S_B^i$.
\end{enumerate}
This is called a linearly independent spanning set: linearly independent because we cannot find any $h_1, \dots, h_m \in S_B$ and numbers $a_1, \dots, a_m$ such that $\sum_{i = 1}^m a_i h_i = 0$; spanning because for any $h \in H$ and any $\eps > 0$, we can find $h_1, \dots, h_m$ and $a_1, \dots, a_m$ such that $\| \sum_{i=1}^m a_i h_i - h\| < \eps$. \\

\paragraph{Step 2:}
The next step is to filter out an orthonormal set. This is done by revisiting an old friend of yours - the Gram-Schmidt orthogonalization procedure.
%To do this, we first notice that in a Hilbert space one can give sense to projections, exactly like in $\R^n$. Indeed, given a closed subspace $H_1$, i.e. the span of some vectors $\{h_1, \dots, h_n\}$, we can define the projection $P_{H_1}(h)$ as the vector in $H_1$ that minimizes $\|h - P_{H_1}(h)\|$ over all vectors in $H_1$ (Why is this well-defined?).

\begin{prop}[Gram-Schmidt orthogonalization]
Suppose we have a linearly independent spanningset $S_B = \{h_1, h_2, \dots \}$. Consider the following recursive procedure:
\begin{enumerate}
\item We set $H_B^1 := \{h_1/||h_1||\}$
\item Given $H_B^{n-1}$, we set $H_B^n := H_B^{n-1} \cup \{\tilde h_n\}$ where $\tilde h_n$ is defined by normalizing the vector $h_n - \sum_{i = 1}^{n-1} h_i \langle h_n, h_i \rangle $ to have unit norm. \footnote{Here $\sum_{i = 1}^{n-1} h_i \langle h_n, h_i \rangle$ is the projection of $h_n$ onto the subspace spanned by $h_1, \dots, h_{n-1}$.}
\item We define $H_B = \cup_{ i \geq 1} H_B^i$.
\end{enumerate}
Then $H_B$ is an orthonormal basis of $H$.
\end{prop}
\begin{proof}
By construction $H_B$ is orthonormal. So it remains to check that it indeed satisfies the basis condition.

Consider any $h \in H$. Then as $S_B$ is a spanning set, and each $s \in S_B$ is a finite combination of some $h_1, \dots, h_n$ in $H_B$, we can for any $m > 0$ find $h_1, \dots h_{n_\eps}$ and $c_1, \dots, c_{n_m} \in \R$ such that $\|h - \sum_{i=1}^{n_m}c_i h_i \| < 1/m$. It is important here to make consistent choices of $c_i$ and this can be done by considering explicitly the orthogonal projection $g_m := \sum_{i=1}^{n_m}h_i \langle h_i, h \rangle$, which satisfies $\|h-g_m\| < 1/m$ as the projection minimizes the distance. We conclude that 
$$h = \sum_{i =1}^\infty h_i \langle h_i, h \rangle$$
as desired.
\end{proof}


\subsection{Bounded and unbounded linear operators on Hilbert spaces}

Linear operators on $\R^n$ are both extremely useful and very well behaved - they can be always nicely represented using a matrix, and they are continuous.

When we go to infinite-dimensional vector spaces, things get more tricky. Linear operators are still easy to define.

\begin{defn}[Linear operator on a Hilbert space]
Let $(\Hh, \| \cdot \|)$ be a separable Hilbert space. A linear map $T : \Hh \to \Hh$, i.e. a map that satisfies $T(af + bg) = aT(f) + bT(g)$ for all $a, b \in \CB$ (or in $\R$) and $f, g \in \Hh$ is called a linear operator on $\Hh$. 
\end{defn}

However, not all of them behave very nicely. We are used to the fact that all linear maps from $\R^n$ to $\R^m$ are continuous, even Lipschitz continuous. This is no longer the case - only so called bounded operators are continuous.

\begin{defn}[Bounded linear operator]
Let $(\Hh, \| \cdot \|)$ be a separable Hilbert space and $T$ a linear operator. We call $T$ bounded if $\sup_{\|f\| \leq 1} \| Tf\| < \infty$. Further, we define this supremum itself as the operator norm of $T$: 
$\|T\|_{op} := \sup_{\|f\| \leq 1}\|Tf\|.$
\end{defn}

A key result says that continuity and boundedness are the same thing. We are more interested in the part bounded implies continuous, which is on the example sheet.
\begin{thm}
A linear operator $T: \Hh \to \Hh$ is continuous if and only if it is bounded.
\end{thm}

%\begin{proof}[Sketch of proof:]
%If $T$ is bounded, then by definition of the operator norm $\|Tf - Tg\| \leq \|T\|_{op}\|f-g\|$ and continuity follows.

%On the other hand, if $T$ is not bounded, one can find $h_n$ of unit norm with $\|T h_n\| \to \infty$ by definition. It is then not hard to use an orthogonalization procedure to find orthogonal $f_n$ with $\|T f_n\| \geq 2^{2n}$ and thereby construct $f_N := \sum_{n=1}^N 2^{-n}f_n$ that converge in $H$ but for which $\|T f_n\|$ diverge!
%\end{proof}

Here are some examples of bounded linear operators.

\begin{eg}[Bounded linear operators]~
\begin{itemize}
\item The identity operator $Id: \H \to \H$ defined by $Id(f) := f$. It has norm equal to $1$
\item Finite rank operators that are the equivalent of linear operators on finite spaces: pick $f_1, \dots, f_n \in \Hh$, $g_1, \dots, g_n \in \Hh$ and define $T(f) := \sum_{i = 1}^n  \langle f, f_i \rangle g_i.$
\item Fourier transform on $\Lc^2(\R)$ is bounded and has unit norm as we have that $\|\F (f)\|_2 = \|f \|_2$.
\item Position operator from quantum mechanics on $\Lc^2([a,b])$ defined by $T(f) := x f$ is bounded: indeed 
$$\|T(f)\|_2^2 = \int_{[a,b]}|x|^2|f(x)|^2d\lambda(x) \leq \max(|a|, |b|)^2 \|f\|_2^2.$$
\end{itemize}
\end{eg}

But there are natural unbounded ones too!
\begin{exo}
Show that the following operators are not bounded:
\begin{itemize}
\item The position operator on $\Lc^2(\R)$ defined by $T(f) := x f$ is not defined on the whole of $\Lc^2(\R)$ and is not bounded;
\item The momentum operator on $\Lc^2(\R)$
defined by $T(f) := -i \frac{df}{dx}$ is neither defined on the whole of $\Lc^2(\R)$ and is not bounded.
\end{itemize}
\end{exo}
The mathematical theory of unbounded operators is more subtle and not touched upon here - as we saw natural properties like continuity get lost and in fact one cannot even define the operator on the whole space. This is contrast with bounded operators that can always be nicely extended to the whole space, even if sometimes we manage to directly define them only on a subspace. The following theorem in particular fills in the remaining part of the proof for the Fourier transform - it shows that the Fourier transform that we defined using the Lebesgue integral for a subclass of nice functions can be extended to all of $\Lc^2(\R)$.

\begin{thm}[Extension of bounded operators]
Let $T$ be a bounded operator on a subspace $D \subseteq \Hh$ of a separable Hilbert space $\Hh$. Then there is a extension of the operator $T$ to an operator $\tilde T: \Hh \to \Hh$ such that $\tilde T = T$ when restricted to $D$ and $\| \tilde T \|_{op} = \|T\|_{op}$.

Moreover, if $D$ is dense, this extension is unique. 
\end{thm}

\begin{proof}
Let us look at the proof in the case where $D$ is dense. Then the extension works using completeness: let $h \in H$ and suppose $T(H)$ not defined. As $D$ is dense, we have $h_1, h_2, \dots \in D$ such that $\|h-h_i\| \to 0$. But then $\|T(h_n-h_m)\| \leq \|T\|_{op}\|h_n-h_m\|$ and in particular as $(h_n)_{n \geq 1}$ is Cauchy because it converges, so is $(Th_n)_{n \geq 1}$. But $H$ is complete, so $(Th_n)_{n \geq 1}$ converges to some $\tilde h$. We then define $\tilde Th := \tilde h$. This gives a way to extend $T$ to the whole of $H$ and one can verify that the norm of the extension doesn't change.

To see uniqueness, consider two such extensions $\tilde T$ and $\widehat T$ both agreeing on $D$. Then $S := \tilde T - \widehat T$ is a bounded linear operator defined on $H$ and equal to $0$ on $D$. But then for every $h \in H$ we have a sequence $h_n$ converging to $h$ with $Sh_n = 0$, it follows that $Sh = 0$ and thus $\tilde T = \widehat T$.
\end{proof}

\subsection{Compact operators and the spectral theorem}

Among all bounded operators, the compact operators are the very well-behaved ones. They behave very much like the friendly linear operators on $\R^n$.

\begin{defn}[Compact operator]
Let $(\Hh, \| \cdot \|)$ be a Hilbert space and $T$ a bounded linear operator. We call $T$ compact if for every sequence $(f_n)_{n \geq 1}$ with $\|f_n\| \leq 1$ there is a subsequence $(f_{n_k})_{k \geq 1}$ such that $(T(f_{n_k}))_{k \geq 1}$ converges.
\end{defn}

\begin{rem}
Another equivalent way to define compactness is just to say that the image of the unit ball under $T$ has to be compact. But often the criteria via sequences is what one checks.
\end{rem}

In particular, notice that compact operators are bounded by definition. Yet not all the bounded operators that one would hope or expect to be compact are compact:\\

\begin{eg}[Non-compact and compact operators]~
\begin{itemize}
    \item The identity operator is compact if and only if the Hilbert space $(\Hh, \|\cdot \|)$ is finite dimensional (and hence isomorphic to $(\R^n, \|\cdot \|_2)$. 
    
    Indeed, it is an application of Bolzano Weierstrass to see it is compact in case it the dimension is finite. 
    
    In the infinite-dimensional case consider an orthonormal basis $(\phi_n)_{n \geq 1}$. But $Id(\phi_n) = \phi_n$ and $\|\phi_n - \phi_m\|_2 = \sqrt{2}$ for all $m \neq n$. Hence $(Id(\phi_n))_{n \geq 1}$ has no subsequence that is Cauchy and hence no subsequence that is convergent.
    \item The Fourier operator $\F$ on $\Lc^2(\R)$ is bounded but not compact - indeed a orthonormal sequence $(\phi_n)_{n \geq 1}$ is mapped to an orthonormal sequence $(\hat \phi_n)_{n \geq 1}$ which has no convergent subsequence.
    \item However, finite rank operators are compact, again it is a matter of Bolzano Weierstrass as you can verify on the example sheet.
\end{itemize}
\end{eg}

In fact compact operators are close as one can get to finite-rank operators: they are given by their limits.

\begin{prop}
Let $(\Hh, \| \cdot \|)$ be a Hilbert space. Then a bounded operator $T: \Hh \to \Hh$ is compact if and only if it is a limit of finite-rank operators in the operator norm, i.e. there exist $(T_n)_{n \geq 1}$ of finite rank with $\|T_n - T\|_{op} \to 0$. 
\end{prop}
The key lemma is the following:

\begin{lemma}
Let $(T_n)_{n \geq 1}, T$ be bounded operators and suppose that $\|T_n - T\|_{op} \to 0$. If $(T_n)_{n \geq 1}$ are compact, then so is $T$.
\end{lemma}

\begin{proof}[Proof idea]
We need to prove that for any sequence $(f_n)_{n \geq 1}$ in $\Hh$ with $\|f_n \| \leq 1$ we can find a subsequence $(f_{n_k})_{k \geq 1}$ such that $T(f_{n_k})$ converges as $k \to \infty$.

To do this we use an iterative extraction procedure: 
\begin{enumerate}
\item As $T_1$ is compact, we can find a subsequence $(f_{n_{1,k}})_{k \geq 1}$ such that $(T_1(f_{n_{1,k}}))_{k \geq 1}$ converges.
\item As further $T_2$ is compact, we can extract a further subsequence $(f_{n_{2,k}})_{k \geq 1}$ from the previous one such that $(T_2(f_{n_{2,k}}))_{k \geq 1}$ also converges (notice that by definition the convergence holds for $T_1$.
\item we continue iteratively.
\end{enumerate}
The claim is then that if we consider the diagonal subsequence $(f_{n_{k,k}})_{k \geq 1}$ then $(T(f_{n_{k,k}}))_{k \geq 1}$ converges as $k \to \infty$. To do this, it suffices to verify that the sequence is Cauchy, which one can do by a $3\epsilon$ argument: we first take $n$ large enough such that $\|T_n - T\|_{op}$ is as small as we wish and then go far enough in the sequence such that for $\|T_n(f_{k,k}) - T_n(f_{l,l})\|$ is as small as we wish for all $k, l$ large enough.
\end{proof}

\subsubsection{The spectral theorem for compact operators}

Maybe the most important result about compact operators is the fact that when they are symmetric, then they can be diagonalised in the very similar way as in the finite-dimensional case, i.e. similary to say real symmetric matrices. The definition of Hermitian / symmetric operator in this setting is as follows.

\begin{defn}[Hermitian / symmetric operator]
Let $(\Hh, \| \cdot \|)$ be a Hilbert space and $T$ a bounded operator. We call $T$ Hermitian or symmetric if for all $f, g \in \Hh$ it holds that $\langle Tf, g \rangle = \langle f, Tg \rangle$.
\end{defn}

The spectral theorem then says that any Hermitian compact operator can be diagonalized. As mentioned, it is the generalisation of diagonalisation of symmetric / Hermitian matrices.

\begin{thm}[Spectral theorem for compact operators]\label{thm:spectral}
Let $\Hh$ be a separable Hilbert space and $T: \Hh \to \Hh$ a compact Hermitian operator.

Then we can find an orthonormal basis $(\phi_n)_{n \geq 1}$ and real numbers $(\lambda_n)_{n \geq 1}$, called eigenfunctions and eigenvalues such that
\begin{itemize}
\item $T(f) = \sum_{n \geq 1}\langle f, \phi_n\rangle \lambda_n \phi_n$ and in particular $T(\phi_n) = \lambda_n \phi_n$ for all $n \geq 1$
\item For each $\eps > 0$, there are only finitely many eigenvalues $\lambda_i$ with $|\lambda_i| > \eps$.
\end{itemize}
\end{thm}

We call this the diagonalization of the operator, because the basis of eigenfunctions $(\phi_n)_{n \geq 1}$ allows us to represent the operator $T$ by a multiplication in each of the coordinates exactly like in the finite-dimensional case: if $f = \sum_{n \geq 1}c_n\phi_n$ then $T(f) = \sum_{n \geq 1} c_n \lambda_n \phi_n$.

The proof of the spectral theorem is quite beautiful itself, but we have to skip it. I have still given the argument in an appendix of the notes. 

\subsubsection{Heat equation / Fourier series from a compact operator}

We will now connect this abstract theorem to the heat equation and its solution via the Fourier series. Our aim is to find a way of solving the heat equation in a way that generalises to the inhomogeneous case and to further PDEs.\\

So consider from the beginning the inhomogeneous heat equation $$\frac{\partial u(t,x)}{\partial t} = k(x) \Delta_x u(t,x)$$
with some boundary conditions to be determined.
Here the inhomogeneity in space is given by a positive continuous function $k(x)$ - when it is constant we are back to the usual heat equation. 

When we solved the heat equation earlier for $k = const$ we observed that trigonometric functions behave well with respect to $\Delta_x$. You can verify that this doesn't help us much in the case $k(x)$ non-constant, as multiplying a trigonometric function by $k(x)$ mixes up all the frequencies.

So instead, let us make another good guess to start off: we will look for solutions of the form $u(x,t) = v(x)w(t)$. Plugging this form into the heat equation we obtain $$v(x) \frac{d w(t)}{dt} = k(x)w(t)\frac{d^2 v(x)}{dx^2}.$$ Thus we obtain a solution $u(x,t) = v(x)w(t)$ as soon as we find solutions to the two ODEs:
$$(1) \; \;k(x)\frac{d^2 v(x)}{dx^2}+ \lambda v(x) = 0$$
and $$(2) \; \; \frac{d w(t)}{dt} = -\lambda w(t).$$
The second of these equations has a straightforward solution given by $w(t) = c \exp(-\lambda t)$. The first equation is what is called a Sturm-Liouville type of equation and can be interpreted as finding the eigenvalues and eigenfunctions of the operator $(Lf)(x) := k(x)\frac{d^2 f(x)}{dx^2}$.\\

Let us see how this Sturm-Liouville type of equation can be solved in the earlier homogeneous case $k = const$ and how it links to Fourier series. In this case the equation becomes just 
$$(1) \; \;k\frac{d^2 v(x)}{dx^2} = - \lambda v(x),$$
i.e. the equation for eigenfunctions of the Laplacian operator $\Delta$ times a constant. 

Now the idea is that whereas $\Delta$ is a unbounded operator, its inverse - the Green's operator $T_G$ - is actually a compact operator and one can use the spectral theorem for that! 

You have already seen the Green's operator, but let us recall it with our vocabulary and mindset. Indeed, consider the Poisson equation $-\Delta f = u$ on say the unit ball $B(0,1) \subset \R^d$ with zero boundary conditions. This can be solved by $f(x) := \int_{B(0,1)} G(x,y)u(y)dy$
 where $G(x,y)$ is the Green's kernel of the Laplacian with zero boundary conditions i.e. the symmetric kernel given informally by $(-\Delta)^{-1}$ with zero boundary conditions. In $d = 1,2$ there is an explicit formula:
\begin{itemize}
    \item given on $[0,1]$ by $G(x,y) := x(1-y)$ for $0 \leq x \leq y \leq 1$ 
    \item and by $G(x,y) := - \log |x-y| + \log |1-x\bar y|$ on $B(0,1) \subseteq \R^2$.
\end{itemize}

\begin{defn}[Green's operator of the Laplacian]
We call the operator on $\Lc^2(B)$ defined by $(T_Gf)(x) := \int_B G(x,y)f(y)d\lambda(y)$ the Green's operator (of the Laplacian) with zero boundary conditions.
\end{defn}

Now consider the setting of the unit interval and let us admit for the moment that the (zero boundary). We claim the following.

\begin{claim} 
Green's operator from $\Lc^2([0,1])$ to $\Lc^2([0,1])$ is compact and symmetric.
\end{claim}

So in particular the spectral theorem applies and we can diagonalise the Green's kernel. What are the eigenfunctions / eigenvaues? IT comes out that we obtain exactly the $\sin$-Fourier series!

\begin{claim}
The spectral theorem applied to the Green's operator $T_G$ on $\Lc^2([0,1])$ gives the collection of eigenfunctions with zero boundary conditions: $\phi_n = \sin(\pi n x)$ and $\lambda_n = -(n\pi)^{-2}$. Further, these are exactly all the zero boundary eigenfunctions of $\Delta$ with eigenvalues $\lambda_n^{-1}$.
\end{claim}

\begin{proof}
To see this observe that by definition every eigenfunction $f_i$ with eigenvalue $\lambda_i$ of $T_G$ solves $T_G(f_i) = \lambda_i f_i$. But as $T_G$ is the inverse operator of $-\Delta$, we know that $T_G(f_i)$ satisfies $-\Delta (T_G(f_i)) = \lambda_i f_i$. Thus we see that every eigenfunction of $T_G$ is also an eigenfunction of $-\Delta$ and vice-versa. Further the eigenvalues are related just by taking $\lambda_n \to \lambda_n^{-1}$. Moreover, we observe that $T_G(f)(0) = T_G(f)(1) = 0$ for any $f \in L^2([0,1])$.

But now we have already seen a collection of eigenfunctions of the Laplacian with zero boundary conditions on $(0,1)$: the sine series $\sin(\pi nx)$ for $n \geq 1$. We also saw that they form an orthonormal basis, so they must be in fact exactly the complete set of eigenfunctions of the Green's operator, with eigenvalues $\lambda_n := (n\pi)^{-2}$!
\end{proof}

This means that all solutions to the equation (1) are given by $(\sin (\pi n x))_{n \geq 1}$, each with $\lambda_n = n^2\pi^2/k$ because of the extra $k$ factor. You can think of it as follows: the spectral theorem gives us the right orthonormal basis of $\Lc^2$ that matches with the spatial equation (1) and hence the initial PDE! 

The nice thing is that this new strategy generalises! The only thing that remains to do is to understand how and in which set-up we can solve the equation (1) that we obtained from separation of time and space variables. This in turn reduces to being able to write down the inverse operator and verifying this is compact. This in fact can be done in a very large generality with no serious difficulty! This is achieved by the so-called Sturm-Liouville theory discussed this year in the non-examinable section. 

This way we obtain hand-crafted series expansions suitable to our initial PDE in a much wider generality. So when in the beginning of the course we found the matching series - Fourier series in the case of the heat equation - just by chance, then now we have a general method for finding such a series, which I think is very nice!

\subsection{Sturm-Liouville equations * non-examinable *}

In this non-examinable section we discuss how to solve the equation (1) just above in a general set-up. It fits in the framework of so-called Sturm-Liouville equations. They are helpful to solve a large class of PDEs. 

%We will consider here what are called regular Sturm-Liouville problems on an interval $[a,b]$. The aim is to find pairs $u: [a,b] \to \R$ and $\lambda \in \CB$ that solve on $[a,b]$ the ODE
%\begin{equation*}\frac{d}{dx}\left(p(x)\frac{du}{dx}\right) + q(x) u(x) = -\lambda w(x) u(x),
%\end{equation*}
%(or written shorter $(pu')' + qu = \lambda w u$), together with certain boundary conditions at both endpoints. Notice that in the usual ODE theory we would work with a fixed $\lambda$ and we we would impose boundary conditions only at one single endpoint.
%\begin{align*}
%&\alpha_a u(a) + \beta_au'(a) = 0 \\ 
%&\alpha_b u(b) + \beta_bu'(b) = 0,
%\end{align*}
%for some $\alpha_a, \beta_a, \alpha_b, \beta_b$ so that neiter both $\alpha_a, \beta_a$ nor both $\alpha_b, \beta_b$ are zero. Here $p(x), p'(x), q(x), w(x)$ are all assumed to be continuous on $[a,b]$ and $w, p$ further are positive throughout the interval.

The form of the equation encompasses quite a large class of ODEs - in fact, any 2nd order ODE of the form $a(x)u''(x) + b(x)u'(x) + c(x)u(x) + \lambda d(x)u(x) = 0$ with say positive $a, d$ can be brought into this form. Let us state a general (but not the most general) theorem and then discuss how to connect it  to the abstract theory developed above.

\begin{thm}[Sturm-Liouville problem]
Consider a regular Sturm-Liouville problem on $[a,b]$, i.e. finding pairs $u \in C^2([a,b], \CB)$ and $\lambda \in \CB$ such that 
\begin{equation}\label{eq:SL}\frac{d}{dx}\left(p(x)\frac{du}{dx}\right) + q(x) u(x) = -\lambda u(x),
\end{equation}
where the functions $p, p', q: [a,b] \to \R$ are continuous and further $p$ is positive throughout $[a,b]$, and that the boundary conditions
\begin{align}\label{eq:SLbnd1}
&\alpha_a u(a) + \beta_au'(a) = 0 \\ 
\label{eq:SLbnd2}
&\alpha_b u(b) + \beta_bu'(b) = 0,
\end{align}
hold for some $\alpha_a, \beta_a, \alpha_b, \beta_b$ so that neiter both $\alpha_a, \beta_a$ nor both $\alpha_b, \beta_b$ are zero. 

Then there is countable sequence of real eigenvalues $\lambda_1 < \lambda_2 < \dots $ and corresponding solutions $u_{\lambda_i}$ such that $(\lambda_i, u_{\lambda_i})_{i \geq 1}$ are the only possible pairs of solutions and moreover $(u_{\lambda_i})_{i \geq 1}$ form an orthonormal basis of $L^2([a,b])$.
\end{thm}\footnote{You may notice that the inhomogenous heat equation doesn't directly fit in this framework. However, the general Sturm-Liouville theoryalso works when on the RHS one has $-\lambda k(x)u(x)$ for a continuous positive function $k$, it just changes a bit the orthogonality statements as one would need to look at weighted $L^2$ spaces - the reason why we kept this coefficient equal to $1$. One can also reduce the inhomogenous heat equation to the form in the theorem via a change of variable and by a redefinition of the function $u$}

Let us see how to prove this theorem. First there are some consequences from the general theory of ODEs:
\begin{itemize}
    \item One can always find a unique solution for Equation \eqref{eq:SL} with boundary conditions only at one endpoint of the interval - i.e. either with $u(a) = u_0(a)$, $u'(a) = u_{00}(a)$ or with $u(b) = u_0(b)$, $u'(b) = u_{00}(b)$. In particular we can for any $\lambda \in \CB$ find solutions $u_1(x)$, $u_2(x)$ that satisfy respectively either boundary conditions \eqref{eq:SLbnd1} or \eqref{eq:SLbnd2}. 
    \item Moreover for any $\lambda \in \CB$ there is at most one solution up to multiplication: indeed, if there were two solutions, one would be able to contradict the uniqueness of solutions for ODEs with boundary conditions $u(a) = u_0(a)$, $u'(a) = u_{00}(a)$.
\end{itemize}

Second, notice that if we denote $Tf := \frac{d}{dx}\left(p(x)\frac{du}{dx}\right) + q(x) u(x)$ defined on the subset $C^2([a,b], \CB)$ of $L^2([a,b], \lambda, \CB)$ then by an integration by parts identity for any $f, g \in C^2([a,b], \CB)$ we see that 
$(Tf, g) = (Tg, f)$. Thus $T$ is Hermitian, and in particular if $\lambda_1, \lambda_2$ are two eigenvalues, then as we saw above the respective eigenfunctions are orthogonal w.r.t. $\| \cdot \|_2$. In particular there can be only countably many eigenvalues because our space is separable. We can also infer already from here that all eigenvalues have to be real. Yet $T$ is not compact, so we cannot conclude just as yet.

But the trick is - exactly as above in the case of the Laplace operator - to construct an inverse operator $T_K$ of $T$ that is compact and to which we can apply the spectral theorem to get an orthonormal basis of eigenfunctions. Showing that every eigenfunction of $T_K$ is also an eigenfunction of $T$ will then conclude the completeness of the eigenfunction basis and thus the theorem exactly as in the case of Fourier.

To do this assume WLOG that $0$ is not an eigenvalue of $T$ (otherwise we can take some $\lambda_0$ that is not an eigenvalue and relabel $\lambda \to \lambda - \lambda_0$ in the set-up).

Using the remarks above, we can then choose the solutions $u_1(x)$, $u_2(x)$ satisfying boundary conditions either only at $a$ or $b$ respectively to be linearly independent - otherwise they would satisfy both boundary conditions and $0$ would be an eigenvalue. Then we have that $W(x) := u_1(x)u_2'(x) - u_2(x)u_1'(x)$ is non-zero but by a direct computation $(pW)'(x) = 0$ and thus $p(x)W(x)$ is a constant we denote by just $pW$.

Now we define 
\begin{equation*}
K(x,y) = \begin{cases} -u_1(x)u_2(y)/pW & \text{ for } a\leq x \leq y \leq b\\
-u_1(y)u_2(x)/pW & \text{ for } a\leq y \leq x \leq b
\end{cases}
\end{equation*}

This gives a special type of operator:

\begin{defn}[Hilbert-Schmidt integral operators]
Let $B(0,R) \subseteq \R^d$ be the ball\footnote{It could also be anything else reasonable like a box of $\R^d$ or similar} of radius $R > 0$. 

If $K(x,y): B(0,R) \times B(0,R) \to \R$ is square integrable, i.e. $\int_{B(0,R) \times B(0,R)}|K(x,y)|^2 d\lambda(x,y) < \infty$, the integral operator defined on $\Lc^2(B(0,R))$ and given by 
$$T_K(f) := \int_{B(0,R)} K(x,y)f(y)d\lambda(y)$$
is called the Hilbert-Schmidt integral operator.
\end{defn}
For example it is easy to see that the Green's operator in 1D, 2D, 3D is Hilbert-Schmidt. The key claim now, stated a bit informally, is the following:

\begin{lemma}
Hilbert-Schmidt integral operators as defined just above and seen as operators from $\Lc^2([a,b]) \to \Lc^2([a,b])$ are compact.
\end{lemma}

In particular they can be diagonalised! As one can further verify that all eigenfunctions of $T$ will necessarily be eigenfunctions of $T_K$, and vice-versa, the solution to the Sturm-Liouville problem follows!\\

I think it is quite elegant how this slightly abstract looking spectral theorem could help us with such concrete problems. In fact, historically Sturm-Liouville problems were the motivation and inspirations for a part of spectral theory, so the interplay of physics and mathematics is most certainly a fruitful one. ;) \\

Thank you!

\section{*non-examinable appendix: the proof of spectral theorem for hermitian compact operators*ee}

The proof of the spectral theorem relies on the following lemmas:

\begin{lemma}\label{lem:st1}
Let $T$ be compact and Hermitian on a Hilbert space $\Hh$. Then either $\|T\|_{op}$ or $-\|T\|_{op}$ is an eigenvalue, i.e. there is some $f \in \Hh$ such that $Tf = \|T\|_{op} f$ or $Tf = -\|T\|_{op} f$.
\end{lemma}

\begin{lemma}\label{lem:st2}
Let $T$ be compact and Hermitian on a Hilbert space $\Hh$. Then all the eigenvalues $\lambda$ are real, and for each non-zero eigenvalue $\lambda$ the subspace of $f \in \Hh$ with $T f = \lambda f$ is finite-dimensional.
\end{lemma}

\begin{lemma}\label{lem:st3}
Let $T$ be compact and Hermitian on a Hilbert space $(\Hh, \| \cdot \|)$. Then any two eigenfunctions $f, g$ corresponding to different eigenvalues are orthogonal. Moreover, for every $\eps > 0$ there are only finitely many eigenvalues with $|\lambda_i| > \eps$.
\end{lemma}

Before proving them, let us see how to conclude the theorem. 

\begin{proof}[Proof sketch of Theorem \ref{thm:spectral}]
Most of the statements in the theorem are given by the lemmas above. 
Indeed, by the first lemma the set of eigenvalues is non-empty, by the second lemma we can for each eigenvalue $\lambda$ find orthonormal vectors $\phi_{\lambda,1}, \dots \phi_{\lambda, d_\lambda}$ with $\Hh \phi_{\lambda,i} = \lambda \phi_{\lambda, i}$. Further by the third lemma we can choose all the eigenfunctions to be orthonormal.

It just suffices to prove that this set of orthonormal eigenfunctions $(\phi_n)_{n \geq 1}$ is complete: i.e. that every $f$ can be written in $\Hh$ as $f = \sum_{n \geq 1} \langle f, \phi_n \rangle \phi_n$.

Now one can verify that the linear span of all these eigenfunctions is a closed subspace of $\Hh$. Consider it's orthogonal complement $\Hh_R \subseteq \Hh$ that is itself a Hilbert space. Notice that for any $h \in \Hh_R$ we have that $Th \in \Hh_R$: indeed $(Th, \phi_n) = (h, T\phi_n) = (h, \lambda_n \phi_n) = 0$ for any eigenfunction $\phi_n$. If $\Hh_R$ is non-empty, then the restriction of $T$ to $\Hh_R$ is again a compact Hermitian operator. But by Lemma 1, it always has an eigenvalue and an eigenfunction, giving a contradiction with the fact that we already listed all of them. Thus $\Hh_R$ is empty and thus the set of orthonormal eigenfunctions indeed forms a basis.
\end{proof}

None of the lemmas is too tricky to prove and it is interesting to see how being Hermitian really forces things to be work out. 

\begin{proof}[Proof of Lemma \ref{lem:st1}]
We can assume by scaling that $\|T\|_{op} = 1$ and thus we need to show that either $1$ or $-1$ is an eigenvalue.

First we claim that there is some unit norm function $g$ with $\|Tg\| = 1$: indeed, by definition of $\|T\|_{op}$ there is some sequence of unit norm functions $g_n$ with $\|T g_n \| \to 1$. As $T$ is compact, $\|T g_n \|$ converges to some $g$. We claim that $\|T g\| = 1$: indeed, $T^2 g_n \to T g$ but 
$$\|T^2 g_n\| \|g_n\| \geq (T^2 g_n, g_n) = (T g_n, T g_n) = \| T g_n \|^2 \to 1$$
and hence as $\|g_n\| = 1$, we see that $\|T^2 g_n \| \to 1$, concluding that $\|Tg\| = 1$.

We will now show that either $Tg = g$ and thus $-1$ is an eigenvalue or $Tg + g$ is an eigenfunction with eigenvalue $1$. To do this write $Tg = cg + df$ with $f$ orthogonal to $g$ and $c, d \in \CB$ with $|c|^2 + |d|^2 = 1$.  
Then as 
$$1 = (Tg, Tg) = (T^2g, g) \leq \|T^2 g\| \|g\| = \|T^2 g\|,$$
we conclude that $T^2g = g$. But now we can conclude: either $Tg + g = 0$ or $T(Tg + g) = g + Tg$ as promised.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:st2}]
Let $(f, \lambda)$ be an eigenfunction, eiganvalue pair. Then 
$$ \lambda(f, f) = (Tf, f) = (f, Tf) = \bar \lambda (f,f),$$
and thus $\lambda$ is real.

Suppose for contradiction that $\lambda \neq 0$ but the subspace of functions $f$ with $Tf = \lambda f$ is not finite-dimensional. Then we can find unit norm orthogonal functions $f_1, f_2, \dots$ such that $Tf_n = \lambda f_n$ and in particular $\|Tf_n - Tf_m\| \geq \sqrt{2}|\lambda|$. But this is a contradiction as $T$ is compact and thus $T f_n$ should admit a convergent subsequence, in particular a subsequence that is Cauchy.  
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:st3}]
First, let $(f, \lambda_1), (g, \lambda_2)$ be two eigenfunction, eigenvalue pairs with $\lambda_1 \neq \lambda_2$. Like in the lemma above:
$\lambda_1 (f, g) = (Tf ,g) = (f, Tg) = \lambda_2 (f,g),$
which can only hold when $(f,g) = 0$, i.e. $f, g$ are orthogonal.

Now suppose there are infinitely many eigenvalues with $|\lambda| > \eps$. Then we can find orthonormal eigenfunctions $(\phi_n)_{n \geq 1}$ with eigenvalues $(\lambda_n)_{n \geq 1}$ with $|\lambda_n| > \eps$. But we can now replicate the proof of the lemma above to obtain a contradiction: as $T$ is compact the sequence $(T \phi_n)_{n \geq 1}$ should admit a convergent subsequence, yet $\|T \phi_n - T\phi_m\| \geq \sqrt{2}\eps$ thus there is no subsequence that is Cauchy. 
\end{proof}

%\end{proof}
\end{document}

%The proof of the Dominated convergence theorem is not examinable, but let us give it for completeness.

%\begin{proof}
%Let us start from the case $f \geq 0$ and $f_n \geq 0$.
%Consider the functions $g_m(x) := \inf_{n \geq m} f_n(x)$. Then $g_m$ is a sequence of increasing non-negative functions converging also to $f:=\lim_{n \geq 1} f_n$. Thus we can apply the Monotone convergence theorem to conclude that 
%$$\lim_{m \to \infty} \int g_md\lambda = \int f d\lambda.$$
%On the other hand $g_m \leq f_m$ for every $m$ and hence 
%$$\lim_{m \to \infty} \int f_m d\lambda \geq \int f d\lambda.$$

\end{document}

This is rather interesting! The integral is insensitive to changes of the function sets of measure $0$ - in particular even changes of the function on any countable set leave the integral unchanged! This offers a new perspective on why $\int 1_{\Q} d\lambda = 0$ and motivates the following vocabulary that will be helpful to use.

\begin{defn}[Almost everywhere / almost sure equality]
We say that two measurable functions $f, g$ are equal almost surely or almost everywhere if $\lambda(f \neq g) = 0$. More generally we say that some property or condition $E \in \F_B$ holds almost everywhere (a.e.) or almost surely (a.s.) if $\lambda(E^c) = 0$.\footnote{It is called almost everywhere in analysis, almost surely in probability, we will probably not be able to avoid using both simultaneously. }
\end{defn}

Indeed, whenever we now ask for a property, we may as well ask it to hold only almost everywhere, as we are always free to change the function values on a set of measure zero! So when we, say, for example, $f \geq g$ almost surely, we mean $\lambda(\{f < g\}) = 0$. \\

The Lebesgue integral also shares some of the natural properties of the Riemann integral - for example linearity and additivity. Of those additivity in terms of integrating over disjoint sets is relatively simple

\begin{lemma}
Let $f, g$ be Lebesgue integrable functions. We have the following additivity statement. If $E, F$ are Borel measurable and disjoint, then 
$$\int_{E \cup F} f d\lambda = \int_E f d\lambda + \int_F f d\lambda.$$
\end{lemma}

\begin{proof}
Recall that by definition $\int_A f d\lambda = \int_{\R^n} 1_A f d\lambda$. 

Also observe that it suffices to prove the claim for non-negative functions as one can always separate the positive and negative parts.


But now notice that every non-negative finite simple function $g$ satisfying $g \leq 1_{E \cup F} f$ can be written as $g_E + g_F$ where $g_E = g 1_E$ and $g_F = g 1_F$ and $\int g d\lambda = \int g_E d\lambda + \int g_F d\lambda$. Thus from the definition of the Lebesgue integral we see that 
$$\int_{E \cup F} f d\lambda \leq \int_{E} f d\lambda + \int_{F} f d\lambda.$$
Yet conversely also for every two simple functions $g_E, g_F$ satisfying $g_E \leq f 1_E$ and $g_F \leq f 1_F$ we have a simple function $g:=g_E + g_F$ satisfying $g \leq 1_{E \cup F} f$. Hence again from the definition of the Lebesgue integral
$$\int_{E \cup F} f d\lambda \geq \int_{E} f d\lambda + \int_{F} f d\lambda$$
and we conclude.
\end{proof}

In fact notice that $E$ and $F$ could even depend on the chosen function $f$ and as a consequence we have that

\begin{cor}
Let $f$ be measurable. Then $f$ is integrable iff $|f|$ is integrable
\end{cor}

We can also now conclude a certain domination type of inequality that formalizes a natural intuition: if only the size effects integrability, then if one integrable function dominates (i.e. is larger) than the other, then this other should also be integrable.

\begin{lemma}[Domination]
If $f \geq 0$ is integrable, then any other measurable function $g$ with $|g| \leq f$ a.s. is also integrable. 

Further, if $f \geq g$ almost everywhere and both are integrable, then $\int f d\lambda \geq \int g d\lambda$.
\end{lemma}

However, a bit surprisingly, linearity is not quite straightforward to prove from the definitions!

\begin{prop}
Let $f, g$ be Lebesgue integrable functions. We have the following linearity statement. For $a, b \in \R$, then $af + bg$ is integrable and $$\int (af+bg) d\lambda = a\int f d\lambda + b\int g d\lambda.$$
\end{prop}

Indeed, it is straight-forward to check that for finite simple functions the equality holds: if $f = \sum_{i = 1}^n c_i 1_{E_i}$ and $g = \sum_{i = 1}^m d_j 1_{F_j}$, then 
$$g+ f = \sum_{i = 0 \dots n}\sum_{j = 0 \dots m} (c_i + d_j)1_{E_i \cap F_j},$$
where we define $c_0 = d_0 := 0$ and $E_0 := \R^n \setminus \cup_{i = 1}^n E_i$ and $F_0 = \R^n \setminus \cup_{j = 1}^m F_j$. And hence by definition $\int (f+g) d\lambda = \int f d\lambda + \int g d\lambda$.

It is also easy to see that for non-negative measurable $f, g$ we have that $\int (f+g) d\lambda \geq \int f d\lambda + \int g d\lambda $ as whenever $h \geq 0, j \geq 0$ are simple finite functions bounded from above by $f, g$ respectively, $h + j$ is a simple finite function bounded from above by $f + g$. The other inequality, however, requires a few tools. The crux is the following:
\begin{itemize}
\item given a simple function $k$ below $f+g$, it is not straight-forward to construct two simple functions, $h$ below $f$, and $j$ below $g$ with $h+j = k$.
\end{itemize}
So instead of attempting a direct construction, we will take a detour through some general theorems that will allow us to prove linearity rigorously.
%\begin{rem}
%This definition of the integral might not feel quite satisfactory straight up, already because the somewhat arbitrary choice of the dyadic approximation. However, it makes things very explicit and in the end one can verify that this choice makes no difference at all.
%\end{rem}

\end{document}
A similar construction is in fact needed to prove the following fundamental existence and uniqueness theorem of probability measures on $\R$, when prescribing probabilities of sets of type $(-\infty, a]$, i.e. the so called cumulative distribution function: i.e. one can show that for every right-continuous and non-decreasing $F: \R \to [0,1]$ with $F(-\infty) = 0$ and $F(\infty) = 1$, there exists a unique probability measure $\P$ on $(\R, \F_B)$ such that $\P((-\infty,a]) = F(a)$.

Many (probability) measure are given by a density function, i.e. by integrable functions $f$ in the sense that $\P((-\infty,a]) = \int_{-\infty}^a f(x)dx$, an example would be the normal distribution with density $(2\pi)^{-1/2}\exp(-x^2/2)$. Some are given by atoms: e.g. the probability measure that puts all mass on outcomes $\{\pm 1\}$ with equal probability would correspond to $\P((-\infty, x])$ that is zero for $x < -1$, is equal to $1/2$ for $x \in [-1, 1)$ and equal to $1$ for $x \geq 1$. 

But in fact there is a third, more curious type of measures too, for which $\P((-\infty,x])$ changenous continuously with $x$ so that in particular $\P(\{x\}) = 0$ for all $x \in \R$...and yet there is no density function. This happens when all the mass of of the measure is distributed on a set that is large in some sense - uncountable - but is small in the other sense, its Lebesgue measure is zero. Here is a prime example:

\begin{eg}[Probability measure on the Cantor set]
Recall the construction of the $1/3$ Cantor set: we let $C_0 = [0,1]$, construct $C_n$ from $C_{n-1}$ by cutting out the middle third of each remaining interval, and define the Cantor set by $C = \cap_{n \geq 1}C_n$. We have seen that $C$ is a closed set with empty interior and that moreover the Lebesgue measure of $C$ is zero.

For $n \geq 1$, we now define $f_n$ to be equal to $0$ on $\R \subseteq C_n$ and equal to $(3/2)^n$ on $C_n$. Let $F_n(x) = \int_{-\infty}^x f_n(x)$. It is then a simple check that the sequence of continuous functions $F_n$ is Cauchy w.r.t. the uniform norm and thus converges to a continuous function $F$ satisfying $F(-\infty) = 0$ and $F(\infty) = 1$. Thus by the theorem above it defines a probability measure $\P$ on $\R$. 

But now notice that both this function $F$ and the related probability measure $\P$ are quite curious: for each $x \in \R$, $\P(\{x\}) = 0$ and on the other hand $\P(\{C\}) = 1$! Also the set $\{x: F(x) \text{ not constant in a neighbourhood of x}\}$ has zero Lebesgue measure and yet the function $F$ grows from $0$ to $1$.
\end{eg}

Such measures living on fractal sets are not a mathematical curiosity, they come up for example when studying the conformally invariant scaling limits of 2D statistical physics models like percolation or the Ising model.

\begin{exo}
Prove that the functions $F_n$ form a Cauchy sequence w.r.t. the uniform norm.
\end{exo}


\end{document}

he following: we can and should ask for countable additivity, however, we should also restrict our attention  $\R$. 


\end{document}

\newpage
\begin{proof}
Assume that $f$ is a continuous function such that
\[
s_n = 2\int_0^1 f(x)\sin(2\pi n x)\,dx = 0, \quad c_n = 2\int_0^1 f(x)\cos(2\pi n x)\,dx = 0, \quad \forall n \geq 1, \quad \text{and} \quad c_0 = \int_0^1 f(x)\,dx = 0.
\]

**Step 1: Constructing Approximate Bumps**  
For any $x_0 \in [0,1]$, define the function
\[
g_N(x) := \frac{f(x)}{\sin(\pi(x - x_0))} \quad \text{for } x \neq x_0 \quad \text{and} \quad g_N(x_0) := 0.
\]
Then consider the trigonometric polynomial:
\[
\phi_N(x, x_0) = \sin\bigl((2N + 1)\pi(x - x_0)\bigr).
\]

**Why is the integral outside the interval negligible?**  
Since $f$ is uniformly continuous, for any $\varepsilon > 0$, there exists $\delta_N \to 0$ as $N \to \infty$ such that if $x, y$ are in the same interval of length $\frac{1}{2N+1}$, then $|f(x) - f(y)| \leq \delta_N$. Partition $[0,1]$ into intervals of length $\frac{1}{2N+1}$, and denote the midpoint of each interval by $x_j$. Then:
\[
\int_0^1 f(x) \phi_N(x, x_0) dx = \sum_j \int_{I_j} f(x) \phi_N(x, x_0) dx.
\]
Using midpoint approximation:
\[
\int_{I_j} f(x) \phi_N(x, x_0) dx = f(x_j) \int_{I_j} \phi_N(x, x_0) dx + \int_{I_j} (f(x) - f(x_j)) \phi_N(x, x_0) dx.
\]
The first integral vanishes due to periodicity, and for the second integral:
\[
\left|\int_{I_j} (f(x) - f(x_j)) \phi_N(x, x_0) dx\right| \leq \delta_N \cdot \frac{1}{2N+1}.
\]
Summing over all subintervals yields:
\[
\left|\int_0^1 f(x) \phi_N(x, x_0) dx\right| \leq \delta_N \to 0.
\]

**Step 2: Using Orthogonality**  
Since $f$ is orthogonal to all sine and cosine functions:
\[
\int_0^1 f(x)\cos(2\pi n x)dx = 0, \quad \int_0^1 f(x)\sin(2\pi n x)dx = 0, \quad \forall n \ge 1.
\]

Substitute $\phi_N(x, x_0)$ into the integral:
\[
\int_0^1 f(x) \phi_N(x, x_0) dx = \int_0^1 f(x)dx = 0.
\]

**Step 3: Limit Argument**  
For any $\varepsilon > 0$, choose $\delta > 0$ so that $|f(x) - f(x_0)| < \varepsilon$ when $|x - x_0| < \delta$. Then:
\[
\int_0^1 f(x) \phi_N(x, x_0) dx = \int_{x_0 - \delta}^{x_0 + \delta} f(x) \phi_N(x, x_0) dx + \int_{[0,1] \setminus [x_0 - \delta, x_0 + \delta]} f(x) \phi_N(x, x_0) dx.
\]
As $N \to \infty$, the integral outside the interval becomes negligible. Inside the interval:
\[
\biggl|\int_{x_0 - \delta}^{x_0 + \delta} (f(x) - f(x_0)) \phi_N(x, x_0) dx\biggr| \leq \varepsilon \int_0^1 \phi_N(x, x_0) dx = \varepsilon.
\]
Since $\int_0^1 \phi_N(x, x_0) dx = 1$, it follows that:
\[
\lim_{N \to \infty} \int_0^1 f(x) \phi_N(x, x_0) dx = f(x_0).
\]

**Step 4: Conclusion**  
Since the integral equals zero for each $N$, we conclude:
\[
\forall x_0 \in [0,1], \quad f(x_0) = 0.
\]

**Step 5: Uniqueness of Fourier Series**  
If a Fourier series converges uniformly to a function $f$, its coefficients must match the integrals defining $c_n$ and $s_n$. If there were another function with the same Fourier series, their difference would yield a zero series, and hence, by the above argument, must be zero.

This completes the proof.
\end{proof}


\end{document}
\subsubsection{Functionals and operators on $C(D, \R)$}

By convention, real or complex-valued functions defined on a space of functions are often called functionals. One can think of them as giving one piece of information about the underlying function. Here are some examples

\begin{enumerate}
\item For any fixed $x \in D$, $f(x)$ is a functional and it is clearly continuous as $|f(x) - g(x)| \leq \|f-g\|_\infty$. If $C([0, T], \R)$ corresponds to a trajectory, this would correspond to asking the position of a point at some fixed time.
\item $\sup_{x \in D} f(x)$ and $\inf_{x \in D} f(x)$ are functionals by the extreme value theorem. They ask for example for the maximal, minimal temperature. Are they continuous w.r.t. the uniform norm?
\item But also for example assuming that $D = [0,1]$ is a box, $\int_{[0,1]} f(x) dx$ with the Riemann integral would be a functional. By integrating, we could for example obtain the mass of a body via its density. Is this a functional continuous w.r.t. the uniform norm? I.e. if we change the density of a body a little bit, does it mass also change a tiny bit?
\end{enumerate}

%If we are given a function $v: C([0,1], \R)$ giving the speed of a particle over the time interval $[0,1]$, then these functionals would correspond respectively to finding its speed at a given point, finding the maximum and minimal speeds, or finding the distance travelled.

Often, we want to consider maps that take continuous functions to other functions, such maps are sometimes also called operators. For example:

\begin{enumerate}
\item If $f \in C([0,1], \R)$ is in addition differentiable, we can consider the mapping sending $f$ to its derivative $f'$. Notice that this mapping is linear in the sense that $(f+g)' = f'+g'$ and $(\lambda f)' = \lambda f'$, however the resulting function may not be continuous.
\item Inversely, we can consider the mapping that sends $f \in C([0,1], \R)$ to a function $F(x) := \int_0^x f(x)dx$, i.e. anti-differentiation. Notice that this time the mapping is defined for all $f \in C([0,1], \R)$, the resulting function is also continuous and the mapping is still linear. 
\item There are also non-linear mappings, for example one could just send every function $f$ to $f^2$. We already saw this really maps $C(D, \R)$ into $C(D, \R)$, but clearly the map is neither injective nor surjective.
\end{enumerate}

We will look quite closely to such linear mappings in the second half of the course and try to be precise both on the domain and image of these maps.

\subsubsection{Extensions}

Finally, let us come back to some choices we made and discuss some extensions. There will be some exercises to discover these notions, they will also help you better understand the proofs of this section.

\begin{itemize}
    \item First, nothing changes if instead of real-valued functions we look at complex valued functions - we could just treat real and imaginary part separately and put uniform norms on each of them.
   \item Sometimes, it is useful to restrict oneself further and talk about continuously differentiable functions, or twice continuous differentiable functions or indeed smooth functions. All these function spaces can also be endowed with a norm that makes them complete: e.g. the space of once continuous differentiable functions $C^1([0,1], \R)$ would be endowed with a norm $\|f\|_{C^1} := \|f\|_\infty + \|f'\|_\infty$. 
    \item If we drop the closed or boundedness assumption of the domain $D$, then as mentioned our norm is not necessarily well-defined. However, one can still give sense to a distance between functions by defining $d(f,g) := \min (\|f-g\|_\infty, 1)$. What changes, if anything?
    \item More generally, one would sometimes also like to talk about continuous functions $C(X, Y)$ defined on more general spaces $X$ and taking values in a more general spaces $Y$, for example we would like to map trajectories of particles in space to their maximum distance. All of the above pretty much works as long as $X, Y$ are equipped with a metric structure (i.e. a distance function) that makes $X$ sequentially compact (every sequence has a convergent subsequence) and $Y$ complete. 
 
\end{itemize}

%Some exo idea: polynomials dense, continuous diff. functions with bound on derivative compact in the space of cts functions,


\subsection{Measure spaces and measurable functions}

The space of continuous functions is very nice. So why don't we just stop there? It is just too small - for example, we would also like to study the heat flow between two small rooms with different temperatures, after we open up an isolating door between the two rooms. Then the heat profile just before opening the door would be described by a step function. Similarly, we would like to analyse situations where a voltage is suddenly turned on, or when we study particles with the idealised point masses. 

We will now look for the largest space that we could potentially be interested in: it will contain all continuous functions, all pointwise limits of those (e.g. like step functions), all pointwise limits of their pointwise limits and so on. In other words we will look for the smallest space of functions that contains continuous functions and is closed under pointwise limits; the smallest so that it would still not contain all possible monsters. To give a succinct description of this, we first revisit the notion of area or measure which is important for the physical description of our world on its own and creates links between different fields of mathematics and physics.

\subsubsection{Measure spaces}

The notion of measure generalizes the notion of length, area, volume, but maybe a bit surprisingly at the same time also encompasses the notion of probability. To formalize this notion, we define a measure space that is a set (e.g. of points in a space or all possible states of an experimental setup) together with a some structure:
\begin{itemize}
	\item first, a set of subsets closed under some operations, called a $\sigma$-algebra corresponding to all sets that can be measured; 
	\item and second, a function defined on these subsets, called a measure, that can be thought of as a volume or area or probability, as explained below. 
\end{itemize}

We state the definition first in quite a large generality to show how it brings together different fields, but will then quickly specialize on the case where $\Omega = \R^n$.

\begin{defn}[Measure space, Borel 1898, Lebesgue 1901-1903]
A measure space is a triple $(\Omega, \F, \mu)$, where
\begin{itemize}
	\item $\Omega$ is a set, called the sample space or the universe.
	\item $\F$ is a set of subsets of $\Omega$, satisfying:
	\begin{itemize}
		\item $\emptyset\in \F$;
		\item if $A \in \F$, then also $A^c \in \F$;  
		\item If $A_1, A_2, \dots \in \F$, then also $\bigcup_{n \geq 1} A_n \in \F$.
	\end{itemize}
$\F$ is called a $\sigma$-algebra and any $A \in \F$ is called a measurable set.
	\item And finally, we have a function $\mu : \F \to [0, \infty]$ satisfying $\mu(\emptyset) = 0$ and countable additivity for disjoint sets: if $A_1, A_2, \dots \in \F$ are pairwise disjoint, $$\mu(\bigcup_{n \geq 1}A_n) = \sum_{n \geq 1}\mu(A_n).$$ This function $\mu$ is called a measure. If $\mu(\Omega)  < \infty$, we call $\mu$ a finite measure.
\end{itemize} 
\end{defn}
\noindent Geometrically we interpret:
\begin{itemize}
    \item $\Omega$ as our space of points
    \item $\F$ as the collection of subsets for which our notion of volume can be defined
    \item $\mu$ our notion of volume: it gives each measurable set its volume.
\end{itemize}
We can define a measure on any set of points, finite or infinite, and certainly not only $\R^n$. E.g. here is an example:

\begin{eg}[Counting measure]
 On any set $\Omega$ one can define the counting measure $\mu_c$: we set $\F := \Po(\Omega)$ (the set of all subsets), and $\mu_c(\{\omega\}) := 1$ for any $\omega \in \Omega$. For any finite set $E$, $\mu_c(E)$ gives its number of elements. If $E$ is infinite, then so is $\mu_c(E)$. In particular, if $\Omega$ is an infinite set, then $\mu_c(\Omega) = \infty$, so this is a measure, but not a finite measure.
\end{eg}

\begin{eg}[Dirac delta measure]
The Dirac delta function that you have seen mentioned in the courses, is actually a measure, not a function and can be defined on any space and for any $\sigma-$algebra that contains points as follows. On any set $\Omega$ one can define the Dirac delta measure $\mu_x$ at the point $x$ as follows: suppose $\F$ contains points and we set $\mu_x(\{x\}) = 1$ and more generally $\mu_x(F) = 1$ if $x \in F$ and $\mu_x(F) = 0$ otherwise, for every $F \in \F$.

We will come back to this and its connection to the 'delta function' you have seen before later on.
\end{eg}

As mentioned, the same framework works for probability theory. A probability space is a measure space with total mass equal to $1$, i.e. $\mu(\Omega) = 1$. In that case we often use the notation of $\P$ for the measure $\mu$. The framework of probability is used for observing / measuring what's going on in the world:
\begin{itemize}
\item  $\Omega$ as the space of all microstates / all possible outcomes; e.g. the states of the atmosphere
\item $\F$ is the collection of observable events / outcomes: i.e. subsets of microstates, whose happening or not happening can be observed; for example we can maybe only measure macroscopic parameters like temperature, or the amount of rain over an hour
\item The measure $\P$ will assign a number in $[0,1]$, called probability, to each observable event. Those events that surely happen, get probability $1$.
\end{itemize}

\begin{eg}
The probability space for describing a fair coin toss would be $$(\{H, T\}, \{\emptyset, \{H\}, \{T\}, \{H,T\}\}, \P),$$ where $\P(\{H\}) = \P(\{F\}) = 1/2$. 

The probability space for describing a fair dice would be $$(\{1,2,3,4,5,6\},\Po(\{1,2,3,4,5,6\}), \P),$$ where we define $\P(F) = |F|/6$. If instead we paint all the faces 1,2,3,4,5 black so they become indistinguishable, we can modify our model by taking $\F = \{\emptyset, \{1,2,3,4,5,6\}, \{1,2,3,4,5\}, \{6\}\}$ and using the probability measure $\widetilde \P$ defined only on these subsets, still with the same formula as above.
\end{eg}

\begin{exo}
Find a measure space to describe two unrelated fair coin tosses. What assumptions are you making in giving the description? Define a sigma-algebra suitable for studying the situation where one can only ask if the two coins have the same side up, or different sides up.
\end{exo}

As we will see in the next section defining useful measures is not always very simple, e.g. defining probability measures over all possible continuous functions (one way to formalize path-integrals in quantum mechanics), was achieved by Wiener in the beginning of 20th century; the similar task for string theory, i.e. defining probability measures over surfaces with differnt metric structures has been partially resolved only in the recent years.

But let us first look at some very basic properties of the collection of measurable sets $\F$ and the measure $\mu$ itself. First, by definition complements and countable unions of measurable sets are measurable, however in fact the set of measurable sets is stable under even more operations:

\begin{lemma}[Constructing more measurable sets]\label{lem:meassets}
	Consider a set $\Omega$ with a $\sigma$-algebra $\F$. 
	\begin{enumerate}
		\item If $A_1, A_2, \dots, \in \F$, then also $\bigcap_{n \geq 1} A_n \in \F$. 
		\item Then also $\Omega \in \F$ and if $A, B \in \F$, then also $A \setminus B \in \F$.
		\item For any $n \geq 1$, if $A_1, \dots, A_n \in \F$, then also $A_1 \cup \dots \cup A_n \in \F$ and $A_1 \cap \dots \cap A_n \in \F$.
		
	\end{enumerate}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lem:meassets}]
	By de Morgan's laws for any sets $(A_i)_{i \in I}$, we have that 
	$$\bigcap_{i \in I} A_i = (\bigcup_{i \in I} A_i^c)^c.$$
	Property (1) follows from this, as if $A_1, A_2, \dots \in \F$, then by the definition of a $\sigma$-algebra also $A_1^c, A_2^c, \dots \in \F$ and hence
	$$(\bigcup_{i \geq 1} A_i^c)^c \in \F.$$
	For (3), again by de Morgan laws, it suffices to show that $A_1 \cup \dots \cup A_n \in \F$. But this follows from the definition of a $\sigma$-algebra, as 
	$A_1 \cup \dots \cup A_n = \bigcup_{i \geq 1} A_i$ with $A_k = \emptyset$ for $k \geq n+1$.
	Finally, for (2) we can just write $\Omega = \emptyset^c$. 
 
 The fact that $A \setminus B \in \F$ is left as an exercise. %Moreover, writing $A \backslash B = A \cap B^c$, we conclude by using (3).
\end{proof}

The statements are also very intuitive at least in the context of probability: e.g. the first one says that if we can observe if some events $A_1, A_2, \dots$ happen, then we can observe if they all happen at once; the second property says that if two events can be observed, then we can always also observe if one of them happened but not the other one. 

In a similar vein, the basic conditions on the measure, give rise to several natural properties too:

\begin{prop}[Basic properties of a measure and a probability measure]\label{prop:propmeas}
	Consider a measure space $(\Omega, \F, \mu)$. Let $A_1, A_2, \dots \in \F$. Then
	\begin{enumerate}
		\item For any $n \geq 1$, and $A_1, \dots, A_n$ disjoint, we have finite additivity $$\mu(A_1) + \dots + \mu(A_n) = \mu(A_1 \cup \dots \cup A_n).$$
		In particular if $A_1 \subseteq A_2$ then $\mu(A_1) \leq \mu(A_2)$.
		\item If for all $n \geq 1$, we have $A_n \subseteq  A_{n+1}$, then as $n \to \infty$, it holds that $\mu(A_n) \to \mu(\bigcup_{k \geq 1} A_k)$. 
		\item We have countable subadditivity (also called the union bound): $\mu(\bigcup_{n \geq 1}A_n) \leq \sum_{n \geq 1} \mu(A_n)$.
	\end{enumerate}
	If in fact $\mu(\Omega)$ is finite (e.g. a probability measure), we further also have the following properties:
	\begin{enumerate}
		\setcounter{enumi}{3}
		\item For any $A \in \F$, we have that $\mu(A^c) = \mu(\Omega) - \mu(A)$.
		\item If for all $n \geq 1$, we have $A_n \supseteq  A_{n+1}$, then as $n \to \infty$, it holds that $\mu(A_n) \to \mu(\bigcap_{k \geq 1} A_k)$. 
	\end{enumerate}
	
\end{prop}

Again, please do check that all these properties also make sense intuitively!

\begin{proof}[Proof of Proposition \ref{prop:propmeas}]

Finite additivity follows from countable additivity by taking $A_k = \emptyset$ for $k \geq n+1$. 

(2), (3) are left as exercises.
%Moreover, by writing $A_2$ as a disjoint union $A_2 = A_1 \cup (A_2 \cap A_1^c)$, we have from disjoint additivity and non-negativity of measures:
%$$\mu(A_2) = \mu(A_1) + \mu(A_2 \cap A_1^c) \geq \mu(A_1).$$

%For (2), write $B_1 = A_1$ and for $n \geq 2$, $B_n = A_n/A_{n-1}$. Then $B_n$ are disjoint, $\bigcup_{n = 1}^N B_n = A_N$ and $\bigcup_{n \geq 1} B_n = \bigcup_{n \geq 1} A_n$. 

%Thus by countable additivity 
%$$\mu(\bigcup_{i \geq 1} A_i ) = \mu(\bigcup_{i \geq 1} B_i ) = \sum_{i \geq 1}\mu(B_i)$$
%But $\mu$ is non-negative, so
%$$\sum_{i \geq 1}\mu(B_i) = \lim_{n \to \infty}\sum_{i = 1}^n \mu(B_i)$$
%By countable additivity again
%$$\sum_{i = 1}^n \mu(B_i) = \mu(\bigcup_{i = 1}^n B_n) = \mu(A_n)$$ 
%and (2) follows.

%To prove countable subadditivity, write similarly $B_1 = A_1$ and for $n \geq 2$ $$B_n = A_n \backslash \bigcup_{k = 1}^{n-1} A_k.$$ 
%Then $B_n$ are disjoint with $\bigcup_{n = 1}^N B_n = \bigcup_{n = 1}^N A_n$ and moreover $B_n \subseteq A_n$.
%Thus by disjoint additivity and point (1) we have
%$$\mu(\bigcup_{n = 1}^N A_n) = \mu(\bigcup_{n = 1}^N B_n) = \sum_{n=1}^N \mu(B_n) \leq \sum_{n=1}^N \mu(A_n).$$
%Now,  taking limits as $N \to \infty$ and using (2) to get the limit on the left gives (3).

For (4), we just notice that $A$ and $A^c$ are disjoint and $A \cup A^c = \Omega$. Thus by disjoint additivity $\P(A) + \P(A^c) = 1$.
Finally, for (5), define $B_n = A_n^c$. Then $\P(A_n) = \P(B_n^c) = 1 - \P(B_n)$. Similarly $\P(\bigcap_{k \geq 1} A_k) = 1-\P(\bigcup_{k \geq 1} B_k)$.
Thus the result follows from (2).
The rest is left as an exercise
\end{proof}

\subsubsection{The Lebesgue measure}

We will now describe the notion of uniform measure on the spaces $\R^n$ (or say a unit cube $[0,1]^n$ or a ball). This measure is called uniform because it is isotropic, i.e. it treats all the points in the set equally; in fact  on $\R^n$ it is up to normalization the only translation and rotation invariant measure. 

Our framework of measure theory tells us that we should at least be aware of the following questions: to which sets can we attribute a volume? Or, in other words what would be the natural $\sigma$-algebra? 

First, it has to be big enough to contain at least all the boxes. Second, it comes out that making it too big can become problematic. So we choose the smallest $\sigma-$algebra containing all boxes:

\begin{defn}[Borel $\sigma$-algebra]
The Borel $\sigma$-algebra $\F_B$ on $\R^n$ is defined as the smallest $\sigma-$algebra containing all boxes, i.e. all sets of the form $\Pi_{i=1}^n [a_i, b_i]$ with real numbers $a_i < b_i$.
\end{defn}

This definition hides a claim: the fact that such a smallest $\sigma-$algebra exists. However, it is a simple but not that illuminating exercise to show that an arbitrary intersection of $\sigma-$algebras is a $\sigma-$algebra and thus the smallest has a well-defined meaning. It is maybe more interesting to see what it contains, i.e. what we can measure \footnote{It is maybe as interesting to see that there are sets in the power-set of $\R^n$ that do not belong to the Borel $\sigma-$algebra. However, describing them explicitly is not that easy - if interested, see the for fun section on the example sheet.}:

\begin{eg}
The Borel $\sigma-$algebra contains for example all points, i.e. sets of the form $\{x\}$: indeed, we can write
$$\{x\} = \bigcap_{m \geq 1} (\{x\} + [-m^{-1},m^{-1}]^n). $$
\end{eg}

\begin{exo}
Show that the Borel $\sigma-$algebra on $\R^n$ also contains  all products of half-lines $\Pi_{i=1}^n (-\infty, a_i]$, all open balls $B(x, r)$ and in fact all open sets of $\R^n$
\end{exo}

The main theorem of this section is then the following result, that we assume without proof:

\begin{thm}[Existence and uniqueness of Lebesgue measure]
There is a unique measure $\lambda$ defined on $(\R^n, \F_B)$ such that the measure of each box $\Pi_{i=1}^n [a_i, b_i]$ is given by $\Pi_{i=1}^n (b_i - a_i)$
\end{thm}

Some other nice properties of the Lebesgue measure follow from this theorem:
\begin{itemize}
\item It is translation invariant: for every set $A \in \F_B$, if we denote by $A+b$ the set $\{a+b: a \in A\}$, then the Lebesgue measure $\lambda$ satisfies $\lambda(A) = \lambda(A+b)$. Indeed, denote by $\widetilde \lambda(A) := \lambda (A +b)$. This defines another measure on $(\R^n, \F_B)$ such that $\widetilde \lambda (\text{box})$ equals the volume of the box. Thus by uniqueness part of the theorem we obtain $\widetilde \lambda = \lambda$ and hence $\lambda(A +b) = \lambda(A)$ for all Borel sets $A$.
\item It can be also proved that the Lebesgue measure is rotation invariant: for every set $A \in \F_B$, if we denote by $R(A)$ the set rotated by the rotation matrix $R$, then the Lebesgue measure $\lambda$ satisfies $\lambda(A) = \lambda(R(A))$.
\end{itemize}

Maybe somewhat surprisingly the proof of this natural theorem is not immediate. The problem is the following: it is simple to assign measure to each box, or each finite union of disjoint boxes etc...however, the Borel $\sigma$-algebra is much richer than that. Indeed, there are sets in the Borel $\sigma$-algebra that one cannot obtain in a finite number of steps by starting with boxes and taking iteratively unions, intersections and complements in any order.
Hence the fact that one can assign a measure to all Borel sets in a way that the axioms are satisfied and boxes have the right size is not immediate. Also the statement of uniqueness is non-evident for the same reason - why should equality for all boxes imply it for all Borel sets?

The proof goes beyond the scope of this course, but here is the sketch for one of the possible approaches for those interested (not examinable). \\

\noindent $\star $ \textit{Start of non-examinable section} $\star $\\

For any rectangle $R = \Pi_{i=1}^n [a_i, b_i]$, let's denote by $|R|$ its natural volume $\Pi_{i=1}^n (b_i - a_i)$.
\begin{enumerate}
\item First, we define for any set $A \subseteq \R^d$ a notion of size called the exterior measure: $m^*(E) := \inf \sum_{i = 1}^\infty |R_i|$, where the infimum is over all coverings of the set $E$ using rectangles - this gives a certain approximation of size from above. 

Notice that from this definition it is not immediate that even $m^*(R) = |R|$, but that can be easily shown for both closed and open rectangles. Also, it is important that we allow for countably many rectangles - see exercise sheet. 
\item It comes out that showing all the axioms of the measure for all subsets of $\R^d$ is impossible \footnote{as long as one assumes the Axiom of Choice}. So now comes the key idea of choosing a subclass of sets which is large enough to contain Borel sets, but small enough to be able to make everything work: we call a set measurable if for every $\eps > 0$, there is some countable collection of rectangles $(R_i)_{i \geq 1}$ such that $E \subseteq \bigcup_{i \geq 1} R_i$ and $m^*(E \triangle (\bigcup_{i \geq 1}R_i)) < \eps$. This means that our earlier approximation from above can be chosen to fit well.
\item It then remains to argue that these sets actually form a $\sigma-$algebra and that all axioms are satisfied for ($(\R^d, \F_L, m^*)$. In fact they form a $\sigma-$algebra, called the Lebesgue $\sigma-$algebra $\F_L$, that is even larger than $\F_B$!

This final step doesn't require any big theorems or inputs, but does require some care in setting up the order of the argument. It is then an easy conclusion that $\F_B \subseteq \F_L$, as $\F_B$ can be generated from just rectangles and we can conclude.
\end{enumerate}
\noindent $\star $ \textit{End of non-examinable section} $\star $\\

\begin{eg}
The Lebesgue measure of a point is zero: indeed for every $\eps > 0$, we have that $\lambda(\{x\}) \leq \lambda(\{x\}+[-\eps, \eps]^n) = (2\eps)^n$, which can be made arbitrarily small.

Hence also the measure of all rational numbers is zero: we have by countably additivity $\mu(\Q) = \sum_{q \in \Q} \mu(\{q\}) = 0$.
\end{eg}

\begin{exo}
Show that the Lebesgue measure of $\R^n$ is infinite and that the Lebesgue measure of a line segment $[0,1] \subseteq R^n$ is zero. 

Now consider the Lebesgue measure on $\R$. Prove that the measure of irrational numbers contained in $[0,R]$ is equal to $R$; prove also that the Lebesgue measure of the Cantor set is zero.
\end{exo}

%\begin{exo}
%Show that there is no way to cover all rational numbers in $[0,1]$ with finitely many closed intervals so that their total length is less than $1$. Give a precise construction of how to find a covering with countably many intervals such that the total length is less than $\eps$.
%\end{exo}

A similar construction is in fact needed to prove the following fundamental existence and uniqueness theorem of probability measures on $\R$, when prescribing probabilities of sets of type $(-\infty, a]$, i.e. the so called cumulative distribution function: i.e. one can show that for every right-continuous and non-decreasing $F: \R \to [0,1]$ with $F(-\infty) = 0$ and $F(\infty) = 1$, there exists a unique probability measure $\P$ on $(\R, \F_B)$ such that $\P((-\infty,a]) = F(a)$.

Many (probability) measure are given by a density function, i.e. by integrable functions $f$ in the sense that $\P((-\infty,a]) = \int_{-\infty}^a f(x)dx$, an example would be the normal distribution with density $(2\pi)^{-1/2}\exp(-x^2/2)$. Some are given by atoms: e.g. the probability measure that puts all mass on outcomes $\{\pm 1\}$ with equal probability would correspond to $\P((-\infty, x])$ that is zero for $x < -1$, is equal to $1/2$ for $x \in [-1, 1)$ and equal to $1$ for $x \geq 1$. 

But in fact there is a third, more curious type of measures too, for which $\P((-\infty,x])$ changenous continuously with $x$ so that in particular $\P(\{x\}) = 0$ for all $x \in \R$...and yet there is no density function. This happens when all the mass of of the measure is distributed on a set that is large in some sense - uncountable - but is small in the other sense, its Lebesgue measure is zero. Here is a prime example:

\begin{eg}[Probability measure on the Cantor set]
Recall the construction of the $1/3$ Cantor set: we let $C_0 = [0,1]$, construct $C_n$ from $C_{n-1}$ by cutting out the middle third of each remaining interval, and define the Cantor set by $C = \cap_{n \geq 1}C_n$. We have seen that $C$ is a closed set with empty interior and that moreover the Lebesgue measure of $C$ is zero.

For $n \geq 1$, we now define $f_n$ to be equal to $0$ on $\R \subseteq C_n$ and equal to $(3/2)^n$ on $C_n$. Let $F_n(x) = \int_{-\infty}^x f_n(x)$. It is then a simple check that the sequence of continuous functions $F_n$ is Cauchy w.r.t. the uniform norm and thus converges to a continuous function $F$ satisfying $F(-\infty) = 0$ and $F(\infty) = 1$. Thus by the theorem above it defines a probability measure $\P$ on $\R$. 

But now notice that both this function $F$ and the related probability measure $\P$ are quite curious: for each $x \in \R$, $\P(\{x\}) = 0$ and on the other hand $\P(\{C\}) = 1$! Also the set $\{x: F(x) \text{ not constant in a neighbourhood of x}\}$ has zero Lebesgue measure and yet the function $F$ grows from $0$ to $1$.
\end{eg}

Such measures living on fractal sets are not a mathematical curiosity, they come up for example when studying the conformally invariant scaling limits of 2D statistical physics models like percolation or the Ising model.

\begin{exo}
Prove that the functions $F_n$ form a Cauchy sequence w.r.t. the uniform norm.
\end{exo}


\subsubsection{Measurable functions}

We now turn to the notion of measurable functions: this will be the smallest space of functions that contain all continuous functions and are closed under pointwise limits. 

We will work with functions from $\R^n \to \R$, though
in fact the notion of a "measurable" function is quite a bit larger, applying to maps between any two measure spaces. \\

The simplest measurable the functions are those given by characteristic functions $1_{x \in E}$ for some measurable set $E \in \F_B$, i.e. functions that tell us whether $x$ is in a set (then $1_{x \in E} = 1$) or not (in which case $1_{x \in E} = 0$). Their countable linear combinations are called simple functions:

\begin{defn}[Simple functions]
Let $E_1, E_2, \dots$ be disjoint Borel sets in $\R^n$. Then a function of the form $f(x) = \sum_{i \geq 1} c_i 1_{x \in E_i}$ is called a simple function.
\end{defn}
We can then define

\begin{defn}[Measurable function I]
We call a function $f: \R^n \to \R$ measurable if it is a pointwise limit of simple functions.
\end{defn}

This definition is natural, however it is not so easy to work with. The most commonly used definition is rather the following:

\begin{defn}[Measurable function II]
We call a function $f: \R^n \to \R$ (Borel-)measurable if for every $a \in \R$, the preimage $f^{-1}((-\infty,a))$ is Borel measurable, i.e. $f^{-1}((-\infty,a)) = \{x \in \R^n: f(x) \in (-\infty, a)\} \in \F_B$.
\end{defn}
 One should naturally prove the equivalence of the definitions, but we will admit this simple result for now and rather look a bit into the structure of the set of measurable functions. All of the following lemmas would be much messier to prove with the definition I, but are rather straightforward from definition II.

First, the space of measurable functions again has a linear structure: 

\begin{lemma}
If $f, g$ are measurable, then so are $\lambda f$ for $\lambda \in \R$ and $f + g$.
\end{lemma}

\begin{proof}
This is on the exercise sheet.
\end{proof}

Second, continuous functions are measurable.

\begin{lemma}
Let $f: \R^n \to \R$ be continuous, then $f$ is also measurable.
\end{lemma}

\begin{proof}
We know that the Borel $\sigma-$algebra contains all open sets, so it suffices to show that for every $a$, the set $f^{-1}((-\infty,a))$ is open. This in turn means that we want to show that for every $x \in f^{-1}((-\infty,a))$, we can find a $\delta > 0$ such that $B(x,\delta) \in f^{-1}((-\infty,a))$. 

But this follows from the definition of continuity. Indeed, as $f$ is continuous, then for every $x$ with $f(x) < a$ we can find a $\delta$ such that $|f(x) - f(y)| < 0.5(a - f(x))$ whenever $y \in B(x, \delta)$. But then for every $y \in B(x, \delta)$, we have that $f(y) < a$ and hence by definition $B(x, \delta) \subseteq f^{-1}((-\infty,a))$.
\end{proof}

And third, the space of measurable functions is closed under pointwise limits.

\begin{lemma}
Let $(f_n)_{n \geq 1}$ be a sequence of measurable functions converging pointwise to a function $f$. Then $f$ is also measurable.
\end{lemma}
%\begin{proof}
%The second claim comes from the following observation:
%$$\{x \in \R^n: (f + g)(x) < a\} = \cup_{q \in \Q} \{x\in \R^n: f(x) < q %\} \cap \{x\in \R^n: g(x) < a- q\}.$$
%\end{proof}

%\begin{exo}
%Prove that the product of two measurable functions is again measurable.
%\end{exo}

\begin{proof}
Our aim is to show that $f^{-1}((-\infty,a))$ is a Borel set and with this aim we observe the following equality:
$$f^{-1}((-\infty,a)) = \bigcup_{k \geq 1}\bigcup_{n \geq 1}\bigcap_{m \geq n} f_m^{-1}((-\infty,a - 1/k)).$$
Indeed, if $f(x) < a$, then there is some $k$ such that $f(x) < a - 2/k$ and as $f_n$ converge pointwise to $f$, then $f_n(x) < a - 1/k$ for all $n$ large enough; this shows LHS $\subseteq$ RHS. 

For the converse inequality notice that if $x$ belongs to the set on the RHS, it means that $f_n(x) < a-1/k$ for some $k \geq 1$ and all $n$ large enough, and thus also by pointwise convergence $f(x) < a - 1/{2k}$ and thus RHS $\subseteq$ LHS.
\end{proof}

\begin{exo}
Let $(f_n)_{n \geq 1}$ be a sequence of measurable functions. Prove that $\sup_n f_n(x)$ and $\inf_n f_n(x)$ are also measurable.
\end{exo}

Finally, let us give a concrete way to approximate any measurable function by simple functions. This approximation will turn out to be rather useful when defining the integral and actually allows us all to conclude the equivalence of the definitions above.

\begin{lemma}[Dyadic approximation of measurable functions]
Let $f: \R^n \to \R$ be measurable. Define the dyadic approximation from below $f_n : \R^n \to \R$ by
$$f_n(x) := 2^{-n}\lfloor 2^n f(x) \rfloor.$$
Then every $f_n$ is a simple function, and the sequence $(f_n)_{n \geq 1}$ increases uniformly to $f$.
\end{lemma}

\begin{proof}
For any $n \in \N$ and any $f$ we have that 
$$f(x) = 2^{-n}2^nf(x) \geq  2^{-n}\lfloor 2^n f(x) \rfloor = f_n(x).$$
Further 
$$f_n(x) \geq 2^{-n}2^{n}(f(x) - 2^{-n}) = f(x) - 2^{-n}$$ 
and thus $\|f(x) - f_n(x)\| \leq 2^{-n}$ and we obtain uniform convergence.
%Moreover, for $m \geq n$ we have that $f_n$ is the $n-$th dyadic approximation of $f_m$, so $f_m(x) \geq f_n(x)$ by the inequality above and the sequence is increasing. 

To see that each $f_n$ is a simple function, observe that it can be written as $$f_n(x) = \sum_{k \in \Z} k2^{-n}1_{f(x) \in [k2^{-n}, (k+1)2^{-n})}.$$
Thus it suffices to prove that $\{f(x) \in [k2^{-n}, (k+1)2^{-n})\}$ are Borel sets, which follows from the fact that they can be equivalently  written as 
$$f^{-1}((-\infty, (k+1)2^{-n})) \setminus f^{-1}((-\infty, k2^{-n})),$$
which are Borel sets by the definition of measurability and Lemma \ref{lem:meassets}. 

Monotonicity is left as an exercise.
\end{proof}

We can now conclude:

\begin{prop}[Equivalence of definitions]
The two definitions given for measurable functions are equivalent.
\end{prop}

As promised, this simple argument is not examinable:\\

\noindent $\star $ \textit{Start of non-examinable section} $\star $\\
\begin{proof}
Let us call the functions 1-measurable if the satisfy definition 1 and 2-measurable if the satisfy definition 2.

First, let us show that if a function $f$ is 1-measurable it is also 2-measurable: simple functions are 2-measurable as by definition the preimage of any such $f$ is given by a countable union of Borel sets. But the third lemma above says that then also any pointwise limit of them is 2-measurable and in particular every 1-measurable function is 2-measurable.

Second, suppose now that a function $f$ is 2-measurable. We can then use the lemma above to write it as a uniform limit of simple functions and hence it is also 1-measurable.
\end{proof}
\noindent $\star $ \textit{End of non-examinable section} $\star $\\

Finally, notice that in everything we did above we didn't use at all that the domain of our functions was $\R^n$! We could have equally well worked on some subset, some geometric surface or a very abstract space!

\subsection{Lebesgue integral}

Recall that if a function $f: \R \to \R$ is Riemann-integrable then we can calculate its Riemann integral on $[0,1]$ using the following approximation procedure:
\begin{itemize}
\item we subdivide $[0,1]$ into $2^n$ equal disjoint intervals $D_i$ each of size $2^{-n}$;
\item we calculate the approximated integral $2^{-n} \sum_{i = 0}^{2^{n-1}} f(i2^{-n})$;
\item we take the limit $n \to \infty$.
\end{itemize}
To calculate the Lebesgue integral (that we will shortly define) for a Lebesgue-integrable function on $[0,1]$ we can also proceed via an approximation, but rather in the image of the function:
\begin{itemize}
\item we take the dyadic approximations from the previous subsection: $f_n := 2^{-n}\lfloor 2^n f(x) \rfloor$;
\item we calculate $\sum_{i \in \Z} i2^{-n} \lambda(x \in [0,1]: f_n(x) = i2^{-n})$;
\item and take the limit $n \to \infty$.
\end{itemize} 
In fact, one could define the Riemann integral of $f: [0,1] \to \R$ via such dyadic approximations as follows. 
\begin{enumerate}
\item We subdivide $[0,1]$ into $2^n$ equal disjoint intervals $D_i = [i2^{-n}, (i+1)2^{-n}]$ each of size $2^{-n}$;
\item We call a function Riemann integrable if $U_n := 2^{-n} \sum_{i = 0}^{2^n} \sup_{x \in D_i} f(x)$ (which is decreasing) and $L_n := 2^{-n} \sum_{i = 0}^{2^n} \inf_{x \in D_i} f(x)$ (which is increasing) both converge to the same limit.
\item We define the Riemann integral of $f$, that from now on we denote for clarity by $\tiny{\circledR}\int_0^1f(x)dx$ to be equal to this limit.
\end{enumerate}
We will follow exactly this recipe to define the Lebesgue integral, with the only difference being that we will do the approximation in the image, not the domain. Notice straight away a big advantage - by using the image for approximations we can a priori try to define integrals for functions $f: \Omega \to \R$ on arbitrary spaces $\Omega$!

\begin{rem}There are many several equivalent ways of constructing the Lebesgue integral (e.g. Stein-Shakarchi, Kolmogorov-Fomin and Boccaroni all use slightly different ways); we will use a way that is very analoguous to the construction of the Riemann integral. Its advantage is that it is simple to state and intuitive to grasp, its downside is that it assumes a good mastering of infinite sums.
\end{rem}

\subsubsection{Definition of the Lebesgue integral}

Although the definition we make here works for measurable functions from any measure space $(\Omega, \F, \mu)$ to $\R$, we will concentrate on the case of $(\Omega, \F, \mu) = (\R^n, \F_B, \lambda)$ i.e. on $\R^n$ with its Lebesgue measure. 

First, some notations. If we have any function $f$ taking only countably many different values $c_1, c_2, \dots$ we set 
$$S(f) := \sum_{i \geq 1}c_i \lambda(\{x: f(x) = c_i\}).$$
This sum is well-defined as long as it is absolutely convergent, i.e. as long as 
$$S(|f|) := \sum_{i \geq 1}|c_i| \lambda(\{x: |f(x)| = |c_i|\})$$
is finite. We will often use the shorthand notation $$\lambda(f_n(x) = c) := \lambda(\{x: f_n(x) = c\}).$$

Further, if $f$ is a measurable function we denote $(f_n)_{n \geq 1}$ its dyadic approximations given by $f_n := 2^{-n}\lfloor 2^n f(x) \rfloor$ and we set $D_{n,i} = [i2^{-n}, (i+1)2^{-n})$. We set $S_n(f) := S(f_n)$.

%$$S_n(f) := \sum_{i \in \Z}i2^{-n}\lambda(\{x:f_n(x) = i2^{-n}\}) = \sum_{i \in \Z}i2^{-n}\lambda(\{x: f(x) \in D_{n,i}\}).$$
%This sum is well-defined as long as it is absolutely convergent, i.e. as long as 
%$$S_n(|f|) = \sum_{i \in \Z}i2^{-n}\lambda(\{x: |f_n(x)| = i2^{-n}\}) = \sum_{i \geq 0}i2^{-n}\lambda(\{x: |f_n(x)| = i2^{-n}\})$$
%is finite. We will often just use the notation 
%$\lambda(\{f_n(x) = i2^{-n}\} := \lambda(\{x \in \R^n:f_n(x) = i2^{-n}\}$ and similar for brevity. 

%Let us also make two small observations, both under the assumption that the sums are absolutely convergent:
%\begin{itemize}
 %   \item the sequence $S_n(f)$ is increasing in $n$: indeed, we can write \begin{align*}S_{n+1}(f) &= \sum_{i \in \Z}i2^{-n-1}\lambda(\{f(x) \in D_{n+1,i})\}) \\
  %  &\geq \sum_{i \in 2\Z}\frac{i}{2} 2^{-n} (\lambda(\{f(x) \in D_{n+1,i}\}\cup\{f(x) \in D_{n+1,i+1}\}) \\
 %   &= \sum_{i \in 2\Z}\frac{i}{2} 2^{-n} \lambda(\{f(x) \in D_{n, i/2}\}) = S_n(f).
 %   \end{align*}
 %   \item If further $\lambda(\{f \neq 0\}) := \lambda(\{x: f(x) \neq 0\} < \infty$, we can also control the growth: for all $m \geq n$, 
 %   $$S_m(f) \leq S_n(f) + 2^{-n}\lambda(\{f \neq 0\}).$$
%\end{itemize}
%Observe that if we have two measurable functions $f(x) \geq g(x)$ for all $x \in %\R^n$, with $S_n(|f|), S_n(|g|)$ both finite, then
%$$S_n(f) $$
We can now define the Lebesgue integral for functions with $\lambda(\{f \neq 0 \}) < \infty$, e.g. including functions that are non-zero only on an interval. We will come back to the extension later on. 

\begin{defn}[Lebesgue integral]
Let $\lambda$ be the Lebesgue measure on $(\R^n, \F_B)$ and $f:\R^n \to \R$ be a measurable function with $\lambda(\{f \neq 0\}) := \lambda(\{x:f(x) \neq 0\}) < \infty$. Consider $(f_n)_{n \geq 1}$ the dyadic approximations given by $f_n := 2^{-n}\lfloor 2^n f(x) \rfloor$. 

We call $f$ Lebesgue integrable if for some $n \geq 1$ the approximate sum $S_n(f)$ is absolutely summable, i.e. $S(|f_n|) < \infty$. 

We then define the Lebesgue integral as the following limit
$$\int_{\R^n} f(x) d\lambda(x) := \lim_{n \to \infty} S_n(f).$$
We will often use the shorthand notation $\int f d\lambda$ when the integration is over $\R^n$.
\end{defn}

The definition above actually entails a claim: a priori it is not clear that the limit of $S_n(f)$ even exists.

\begin{lemma}\label{lem:def}
Suppose $f$ is measurable, $\lambda(f \neq 0) < \infty$ and $f$ integrable, i.e. $S(|f_n|)$ is finite for some $n \geq 1$. 

Then in fact $S(|f_n|) < \infty$ for all $n \geq 1$ and the limit of the sequence $S_n(f)$ exists. 
\end{lemma}
\begin{proof}
We start by arguing that if $S(|f_n|)$ is finite for some $n$, it is finite for all $n \geq 1$. 
First observe that 
\begin{equation}\label{Eq:abs}
|S_n(|f|) - S(|f_n|)| \leq 2^{-n}\lambda(\{f \neq 0\}).
\end{equation}
Now observe the following calculation holding for any $f$ and $n$ for which the sums make sense (including the choice $|f|$):
\begin{itemize}
    \item We observe that the sequence $S_n(f)$ is increasing in $n$: indeed, we can write \begin{align*}S_{n+1}(f) &= \sum_{i \in \Z}i2^{-n-1}\lambda(\{f(x) \in D_{n+1,i})\}) \\
    &=\sum_{i \in 2\Z}(i2^{-n-1}\lambda(\{f(x) \in D_{n+1,i})\}) + (i+1)2^{-n-1}\lambda(\{f(x) \in D_{n+1,i})\}))\\
    &\geq \sum_{i \in 2\Z}i 2^{-n-1} (\lambda(\{f(x) \in D_{n+1,i}\}\cup\{f(x) \in D_{n+1,i+1}\}) \\
    &= \sum_{i \in 2\Z}\frac{i}{2} 2^{-n} \lambda(\{f(x) \in D_{n, i/2}\}) = S_n(f).
   \end{align*}
    \item But with a similar argument one also control the opposite growth: 
    $$S_{n+1}(f) \leq S_n(f) + 2^{-n}\lambda(\{f \neq 0\}).$$
\end{itemize}
By summation of $\sum_{m \geq 0}2^{-n-m} \leq 2*2^{-n}$ we thus obtain for all $m_1, m_2 \geq n$:
$$S_{m_1}(|f|) \geq S_n(|f|) \geq S_{m_2}(|f|) - 2*2^{-n}\lambda(\{f \neq 0\})$$
and the absolute summability claim follows.

Hence also the sums $S_n(f)$ are well defined and we similarly obtain that for all $m_1, m_2 \geq n$, we have that $$S_{m_1}(f) \geq S_n(f) \geq S_{m_2}(f) - 2*2^{-n}\lambda(\{f \neq 0\}).$$
We conclude that $S_n(f)$ is Cauchy and converges.

%I am not sure I see how the last inequalities give that the sequence is Cauchy? Shouldn't it be argued that for $m>n>N$ large enough, $|S_m(f) - S_n(f)| = S_m(f) - S_n(f) \leq \sum_{k=0}^{m-n-1} 2^{-n-k} \lambda(\{f \neq 0\}) \leq 2^{-N} \lambda(\{f \neq 0\}) < \epsilon$?

%To argue the summability, we observe that the above inequalities also hold for $S_n(|f|)$ and 
%But further  and thus the absolute summability claims also follow, giving the lemma. 
\end{proof}

\begin{exo}
Argue that $f(x) = 1_{\Q \cap [0,1]}(x)$ and $f(x) = x1_{[0,1]}$ are measurable and integrable. Calculate their integrals from the definition.
\end{exo}
%\begin{rem}
%This definition of the integral might not feel quite satisfactory straight up, already because the somewhat arbitrary choice of the dyadic approximation. However, it makes things very explicit and in the end one can verify that this choice makes no difference at all.
%\end{rem}

Here are some elementary consequences from the definition and the proof above:

\begin{cor}\label{cor:imm}
Let $f: \R^n \to \R$ be measurable with $\lambda(f \neq 0) < \infty$. Then
\begin{enumerate}
\item if $f$ is bounded, i.e. $|f(x)| \leq C$ for all $x \in \R^n$, then it is integrable
\item $f$ is integrable iff $|f|$ is integrable
\item if $\lambda(f \neq 0) = 0$ then $f$ is integrable and $\int f d\lambda = 0$
\item if $f$ is integrable and $\lambda(f < 0) = 0$, then $\int f d\lambda \geq 0$


%\item If $f$ is integrable, then $S_n(f)$ monotonically increases to $\int f d\lambda$
%\item for any function $f$ that is taking values in $2^{-n}\Z$, if it is integrable, then $\int f d\lambda = S_n(f)$
\end{enumerate}
\end{cor}

This is somewhat interesting because we see that only "non-boundedness" can somehow be destructive to integrability.

\begin{proof}
The first claim follows directly from the definition of $S_n(f)$, because the sets $\{x: f(x) \in [i2^{-n}, (i+1)2^{-n})$ are just empty for $|i| > 2C2^n$ and thus each $S_n(f)$ is just a finite sum.

The second claim follows  from \eqref{Eq:abs} and the rest is left as an exercise.

\end{proof}
%The first statement follows from the monotonicity in the lemma above. 

%The second statement from \eqref{Eq:abs}, the third statement directly from the fact that both $S(f_n) = S(|f_n|) = 0$ for all $n \geq 1$. The fourth claim follows from the definition of $S_n$ and the fifth claim follows from the fact that for any $m \geq n$, we have that $S_m(f) = S_n(f)$. The final claim comes directly from the definition.
%\end{proof}

\begin{eg}
Let us check that for any function $f$ that is taking values in $2^{-n}\Z$, if it is integrable, then $\int f d\lambda = S_n(f)$. In particular $S_n(f) = \int f_n d\lambda$. Indeed, we just observe the following: if $f(x) \in 2^{-n}\Z$, then $f_m$ for $m \geq n$, we have that $f_m(x) = 2^{-m}\lfloor 2^m f(x) \rfloor = f(x)$ just because $2^mf(x)$ is always an integer! Then by definition $S_m(f) = S_n(f)$ for all $m \geq n$ and we can conclude.
\end{eg}

Let us also check that more generally the definition we give matches the natural definition of the integral for simple functions. Indeed, if we have a simple function $f = \sum_{i \geq 1} c_i 1_{E_i}(x)$ -- recall $E_i$ are asked to be disjoint --, it would be natural to define its integral to be just $\sum_{i \geq 1} c_i \lambda(E_i)$, as long as this sum is absolutely convergent. Notice that this sum is exactly equal to $S(f)$ from before (check!). The Lebesgue integral for such $f$ gives exactly this formula.  

\begin{lemma}\label{lem:intsimple}
Consider a simple function $f = \sum_{i \geq 1} c_i 1_{E_i}(x)$ such that $\sum_{i \geq 1} |c_i|\lambda(E_i) < \infty$ and $\lambda(\bigcup_{i \geq 1} E_i) < \infty$. Then $f$ is integrable and 
$$\int_{\R^n} f(x)d\lambda(x) = S(f) = \sum_{i} c_i\lambda(E_i).$$
\end{lemma}

\begin{proof}
%Denote as above $D_{n,i} := [i2^{-n}, (i+1)2^{-n})$. 
Let us admit the integrability claim, as it is similar to the argument to follow and is left as an exercise.

So suppose $f$ is integrable and thus all $S_n$ absolutely summable. We have
$$S_n(f) = \sum_{i \in \Z}i2^{-n}\lambda(\cup_{j: c_j \in D_{n,i}}E_j) = \sum_{i \in \Z}i2^{-n}\sum_{j \geq 1} 1_{c_j \in D_i}\lambda(E_j).$$
As everything is absolutely summable we can swap the sums to obtain:
$$S_n(f) = \sum_{j \geq 1} \lambda(E_j) \sum_{i \in \Z} i2^{-n}1_{c_j \in D_{n,i}}.$$
But from this it is clear that
$$ S_n(f) \leq \sum_{i} c_i\lambda(E_i) \leq S_n(f) + 2^{-n}\lambda(\bigcup_{i \geq 1}E_i).$$
We deduce that $S_n(f) \to  \sum_{i} c_i\lambda(E_i)$ and conclude the lemma.
\end{proof}

%In particular, this means that for any integrable function $f$ we have that $S_m(f) = \int_{\R^n}f_m(x)d\lambda(x)$ and we are really defining the Lebesgue integral of $f$ as the approximation of Lebesgue integrals for a particular sequence of simple functions $f_n$.

Having defined the integral on $\R^n$, we can define the integral on any Borel-measurable subset $E \subseteq \R^n$. To do this we notice that for a measurable function $f$, also $1_E(x) f(x)$ is measurable as a product of measurable functions. 
%Moreover we can directly see that $S_n(|f|1_E) \leq S_n(|f|)$ and thus if $f$ is integrable, so is $f1_E$.

\begin{defn}[Lebesgue integral on subsets]
Let $\lambda$ be the Lebesgue measure on $(\R^n, \F_B)$ and $f:\R^n \to \R$ be a measurable function with $\lambda(\{f \neq 0\}) < \infty$ and let $E \subseteq \R^n$ be a Borel set.

Suppose $f1_E$ is integrable.
We then define the integral of $f$ on $E$ by $$\int_E f(x) d\lambda(x) := \int_{\R^n} 1_E(x) f(x) d\lambda(x).$$
\end{defn}

\subsubsection{Convergence theorems}

We will now move towards less evident, but important properties of the integral. For example, soon we will want to consider the function space of all integrable functions and study its structure. As we have already seen before, we would like to answer several questions to do that: 
\begin{itemize}
\item If $f, g$ are integrable, is $f+ g$ also integrable?
\item If $(f^n)_{n \geq 1}$ are integrable, is their pointwise limit integrable? Is its integral equal to the limit of integrals?
\end{itemize}
The key results helping us answer these questions are the Dominated convergence theorem and the Monotone convergence as 

But before stating them, let us see that the answer to the second question is not quite trivial. The first part is on the exercise sheet, and for the second part we look at the following example:
\begin{eg}\label{eg:domconv}
Consider the functions $(f^n)_{n \geq 1}$ defined on $\R$ by $f^n(x) = n1_{(0,1/n)}$. They are measurable and bounded, thus integrable. Moreover as they are simple functions it is easy to check that $\int f^n d\lambda = 1$. But notice that $f^n(x)$ converge to the constant $0$ function pointwise, as for every $x \in \R$, there is some $n_x \in \N$ such that $f^n(x) = 0$ for all $n \geq n_x$. But the integral of the constant $0$ function is just $0$ and thus the integrals of $f^n$ do not converge to the integral of their pointwise limit.
\end{eg}

Before stating the dominated convergence theorem let us first check that if an integrable function dominates another function, then this function is also integrable and its integral can be bounded:

\begin{prop}\label{prop:dom}
Let $0 \leq f \leq g$ be measurable, non-zero only on some Borel set $E$ with $\lambda(E) < \infty$. Let $g$ be integrable. Then also $f$ is integrable and $\int f d\lambda \leq \int g d\lambda$.
\end{prop}

We start with a simple lemma:
\begin{lemma}\label{lem:repsmp}
For $f$ non-negative, measurable, with $\lambda(f \neq 0) < \infty$ and integrable, we can write:
$$S_n(f) = 2^{-n}\sum_{i \geq 1}\lambda(f \geq i2^{-n}).$$
\end{lemma}

\begin{proof}
Observe that 
$$\{f \geq i2^{-n}\} = \bigcup_{j \geq i}\{f \in [j2^{-n}, (j+1)2^{-n})$$
and hence
$$2^{-n}\sum_{i \geq 1}\lambda(f \geq i2^{-n}) = 2^{-n}\sum_{i \geq 1}\sum_{j \geq i}\lambda(f \in [j2^{-n}, (j+1)2^{-n}) = 2^{-n}\sum_{i,j \geq 1}1_{j \geq i}\lambda(f \in [j2^{-n}, (j+1)2^{-n}).$$
Changing the order of summation (that is allowed as all terms are non-negative), i.e. summing first over $i \geq 1$, we obtain that this equals
$$2^{-n}\sum_{j\geq 1}j\lambda(f \in [j2^{-n}, (j+1)2^{-n}) = S_n(f),$$
giving the lemma.
\end{proof}
%% NICE EXERCISE, SHOW THAT THEN \int f d\lambda = R\int \lambda(f \geq x)
The proposition now follows:
\begin{proof}[Proof of proposition]
Using the lemma we can write 
$$S_n(g) = 2^{-n}\sum_{j \geq 1}\lambda(g \geq j2^{-n}).$$
But as $f \leq g$, we have for all $j \geq 1$ that
$$\{x: f(x) \geq j2^{-n}\} \subseteq \{x: g(x) \geq j2^{-n}\}$$
and hence
$$\lambda(f \geq j2^{-n}) \leq \lambda(g \geq j2^{-n}),$$
implying that
$$S_n(g) = 2^{-n}\sum_{j \geq 1}\lambda(g \geq j2^{-n}) \geq 2^{-n}\sum_{j \geq 1}\lambda(g \geq j2^{-n}) = S_n(f)$$
as desired.
\end{proof}

We now state one of the most important theorems about the integral - the theorem of dominated convergence. Let us call the set $\{x:f(x) \neq 0\} \subseteq \R^n$ the support of $f$.

\begin{thm}[Dominated convergence theorem]
Let $E$ be a Borel set with $\lambda(E) < \infty$. Let $(f^n)_{n \geq 1}$ be measurable functions such that the support of each $f^n$ is contained in $E$ and that converge to some $f$ pointwise. 

Suppose further that for all $n \geq 1$ we have that $|f^n| \leq g$ for some $g$ that is measurable, whose support is also contained in $E$ and is integrable. 

Then all $f^n$ and also $f$ are integrable. Further $\int f^n d\lambda \to \int f d\lambda$.

\end{thm}

It may be now enlightening to check what goes wrong in the Example \ref{eg:domconv}, where the integrals of pointwise convergining functions did not converge to the integral of the limit.

\begin{comment}
We start with a lemma, that is basically the same statement but with $g = C$ for some $C > 0$.

\begin{lemma}
Let $(f^n)_{n \geq 1}, f, g$ be as in the theorem, assuming for now that $g$ is equal to a constant function $C$.

Then $f^n, f$ are integrable and $\int f^n d\lambda \to \int f d\lambda$.
\end{lemma}

\begin{proof}[Proof of lemma]
The integrability of each $f_n$ and $f$ follows from the previous proposition, and point (2) of Corollary \ref{cor:imm}.

It suffices to show that for each $\eps > 0$, there is some $N \geq 1$ such that for all $n \geq N$, we have that $|\int f^n d\lambda - \int f d\lambda | < \eps$.\\

\noindent \textbf{Step 1: reduction to dyadic approximations $S_M$} \\

Recall from the proof of Lemma \ref{lem:def} that for any integrable function $h$
$$|\int h d\lambda - S_m(h)| \leq 2*2^{-m}\lambda(h \neq 0).$$
Moreover, for all $(f^n)_{n \geq 1}, f$ of the statement the RHS is bounded uniformly by $2^{-m+1}\lambda(E)$.

Thus we can choose $M \geq 1$ large enough such that both $|\int f d\lambda - S_M(f)| \leq \eps/3$ and $|\int f^n d\lambda - S_M(f^n)| \leq \eps/3$ for all $n \geq 1$. We also choose it such that $2^{-M+2}\lambda(E)< \eps/6$ for later purposes. 

Then by triangle inequality, 
$|\int f d\lambda - \int f^n d\lambda|$ is bounded by 
$$|\int f d\lambda - S_M(f)| + |S_M(f) - S_M(f^n)| + |S_M(f^n) - \int f^n d\lambda| < 2\eps/ 3 + |S_M(f) - S_M(f^n)|$$
and it now suffices to show that for all $n \geq N$ large enough $|S_M(f) - S_M(f^n)| < \eps/3$. \\


\noindent \textbf{Step 2: convergence of the approximate sums}\\

To deal with the remaining part, we use the following simple but important claim, whose proof is left as an exercise:
\begin{claim}\label{claim:pwinproba}
Suppose $(f^n)_{n \geq 1}$ are measurable such that $\{f^n \neq 0\} \subseteq E$ for some Borel $E$ with $\lambda(E) < \infty$. Suppose further that they converge pointwise to $f$. Then for every $\eps > 0$, we have that $\lambda(|f - f^n|) > \eps$ converges to $0$ as $n \to \infty$.
\end{claim}

Now, as $|f^n|, |f| \leq C$, both $S_M(f^n)$ and $S_M(f)$ are finite sums up to some $K = C2^M$.
By the claim above we can choose $N$ such that $K2^{-M+2}\lambda(|f-f^n| > 2^{-M}) < \eps /6$ for all $n \geq N$.

Now using Lemma \ref{lem:smp} for negative and positive parts, we can write 
$$S_M(f^n) = 2^{-M}\sum_{i = 1}^{K} \lambda(f^n \in [i2^{-M}, (K+1)2^{-M})) - 2^{-M}\sum_{i = 0}^{K-1} \lambda(f^n \in [-K2^{-M}, -i2^{-M}))$$
and similarly for $S_M(f)$. 

Now for each term corresponding to positive parts, we can bound 
$$|\lambda(x: f^n(x) \in [i2^{-M}, (K+1)2^{-M})) - \lambda(x: f(x) \in [i2^{-M}, (K+1)2^{-M}))|, $$
which equals 
$$\lambda( \{f^n(x) \in [i2^{-M}, (K+1)2^{-M})\} \Delta \{f(x) \in [i2^{-M}, (K+1)2^{-M})\})$$
by 
\begin{align*}
    &|\lambda\left(x: |f^n(x) - f(x)|\leq 2^{-M} \cap \{x: f(x) \in [i2^{-M}, (i+1)2^{-M})\}\right) +\\ 
    &\lambda\left(x: |f^n(x) - f(x)| \leq 2^{-M} \cap \{x: f^n(x) \in [i2^{-M}, (i+1)2^{-M})\}\right) +\\ 
    &2\lambda\left(\{x: |f^n(x) - f(x)| > 2^{-M}\} \cap E\right)|.
\end{align*}
Summing over $i = 1 \dots K$, we obtain 
$$2(\lambda(\{|f-f^n| \leq 2^{-M}\} \cap E) + 2K\lambda(\{|f-f^n| > 2^{-M}\} \cap E),$$
Adding the similar terms over the negative parts of $S_M$, we can bound
$|S_M(f^n) - S_M(f)|$
by 
$$2^{-M+2}\left(\lambda(\{|f-f^n| \leq 2^{-M}\} \cap E) + K\lambda(\{|f-f^n| > 2^{-M}\} \cap E)\right),$$
which is bounded by $42^{-M}\lambda(E) + K2^{-M+2} \lambda(|f-f^n| > 2^{-M})$. The first term is bounded by $\eps / 6$ by assumption in Step 1, and the second term is bounded by $\eps/6$ for all $n$ large enough by the assumption just above. This gives us altogether $\eps/3$ as required.

\end{proof}
The theorem now follows
\begin{proof}[Proof of theorem]
We know that $S_n(g) < \infty$ for all $n \geq 1$, let us consider $n = 1$:
$$S_1(g) = \sum_{i \geq 1} i \lambda(g \in [i, i+1)),$$
where the sum contains just positive terms as $f$ is non-negative.

As this is absolutely summable, for all $K$ sufficiently large it holds that
\begin{equation}\label{eq:ui1}
\sum_{i \geq K} i \lambda(g \in [i, i+1)) < \eps/3.
\end{equation}

Now, observe that the RHS is exactly equal to $S_1(g_K)$.
\end{proof}
\end{comment}
The proof is somewhat lengthy, but really elementary: the idea is just to use the definition of the integral to reduce the claim to dyadic approximations, and then further 
reduce it only to finite sums. These finite sums are then dealt with by an explicit calculation. Defining the integral in other ways, would make the proof appear shorter and more elegant, as it is obtained through several intermediate results, yet it seems that this proof shows most clearly what is the mechanims behind the result. Nevertheless, we leave the proof only for those interested in the details:\\

\noindent $\star $ \textit{Start of non-examinable section} $\star $\\
\begin{proof}
The integrability of each $f_n$ and $f$ follows from the previous proposition, and point (2) of Corollary \ref{cor:imm}.

It suffices to show that for each $\eps > 0$, there is some $N \geq 1$ such that for all $n \geq N$, we have that $|\int f^n d\lambda - \int f d\lambda | < \eps$.\\

\noindent \textbf{Step 1: reduction to dyadic approximations $S_M$} \\

Recall from the proof of Lemma \ref{lem:def} that for any integrable function $h$
$$|\int h d\lambda - S_m(h)| \leq 2*2^{-m}\lambda(h \geq 0).$$
Moreover, for all $(f^n)_{n \geq 1}, f$ of the statement the RHS is bounded uniformly by $2^{-m+1}\lambda(E)$.

Thus we can choose $M \geq 1$ large enough such that both $|\int f d\lambda - S_M(f)| \leq \eps/5$ and $|\int f^n d\lambda - S_M(f^n)| \leq \eps/5$ for all $n \geq 1$. We also choose it such that $2^{-M+2}\lambda(E)< \eps/5$ for later purposes. 

By triangle inequality, 
$|\int f d\lambda - \int f^n d\lambda|$ is bounded by 
$$|\int f d\lambda - S_M(f)| + |S_M(f) - S_M(f^n)| + |S_M(f^n) - \int f^n d\lambda| < 2\eps/ 5 + |S_M(f) - S_M(f^n)|$$
and it now suffices to show that for all $n \geq N$ large enough $|S_M(f) - S_M(f^n)| < 3\eps/5$. \\

\noindent \textbf{Step 2: reduction to a finite sum}\\

As $g$ is integrable, the sum 
$$S_M(g) = \sum_{i \geq 1}i2^{-M}\lambda(g \in [i2^{-M}, (i+1)2^{-M}))$$
converges and hence we can find a $K$ sufficiently large with 
\begin{equation}\label{eq:bound1}
S_M(g1_{g \geq K}) = \sum_{i > K}i2^{-M}\lambda(g \in [i2^{-M}, (i+1)2^{-M})) < \eps/5.
\end{equation}
But now by the hypothesis both $|f|1_{|f| > K}, |f^n|1_{|f^n| > K}$ are pointwise bounded by $g1_{g > K}$ and using the Proposition \ref{prop:dom}, we obtain that
$S_M(|f^n|1_{|f^n| > K}) < \eps/5$ for all $n \geq 1$ and $S_M(|f|1_{|f| > K}) < \eps/5$ too. 

Again by the triangle inequality, 
$|S_M(f^n) - S_M(f)|$ can be bounded by
\begin{align*}
   & |S_M(f^n) - S_M(f^n1_{|f^n|\leq K})| + |S_M(f^n1_{|f^n| \leq K}) - S_M(f1_{|f| \leq K})| + |S_M(f)- S_M(f1_{|f| \leq K})| \\
   & < 2\eps/ 5 + |S_M(f^n1_{|f^n| \leq K}) - S_M(f1_{|f| \leq K})|
\end{align*}
and thus it suffices to show that for 
$n \geq N$ large enough
$|S_M(f^n1_{|f^n| \leq K}) - S_M(f1_{|f| \leq K})| < \eps/5.$\\

\noindent \textbf{Step 3: dealing with the finite sum}\\

To deal with the remaining part, i.e. the finite sum, we use the following simple but important claim, whose proof is left as an exercise:
\begin{claim}\label{claim:pwinproba}
Suppose $(f^n)_{n \geq 1}$ are measurable such that $\{f^n \neq 0\} \subseteq E$ for some Borel $E$ with $\lambda(E) < \infty$. Suppose further that they converge pointwise to $f$. Then for every $\eps > 0$, we have that $\lambda(|f - f^n|) > \eps$ converges to $0$ as $n \to \infty$.
\end{claim}

Indeed, by this claim we can choose $N$ large enough such that $K2^{-M+2}\lambda(|f-f^n| > 2^{-M}) < \eps /10$ for all $n \geq N$.
Now using Lemma \ref{lem:repsmp} for negative and positive parts, we can write 
$$S_M(f^n1_{|f^n| \leq K}) = 2^{-M}\sum_{i = 1}^{K} \lambda(f^n \in [i2^{-M}, (K+1)2^{-M})) - 2^{-M}\sum_{i = 0}^{K-1} \lambda(f^n \in [-K2^{-M}, -i2^{-M}))$$
and similarly for $S_M(f1_{|f| \leq K})$. 

Now for each term corresponding to positive parts, we can bound 
$$|\lambda(x: f^n(x) \in [i2^{-M}, (K+1)2^{-M})) - \lambda(x: f(x) \in [i2^{-M}, (K+1)2^{-M}))|, $$
which equals 
$$\lambda( \{f^n(x) \in [i2^{-M}, (K+1)2^{-M})\} \Delta \{f(x) \in [i2^{-M}, (K+1)2^{-M})\})$$
by 
\begin{align*}
    &|\lambda\left(x: |f^n(x) - f(x)|\leq 2^{-M} \cap \{x: f(x) \in [i2^{-M}, (i+1)2^{-M})\}\right) +\\ 
    &\lambda\left(x: |f^n(x) - f(x)| \leq 2^{-M} \cap \{x: f^n(x) \in [i2^{-M}, (i+1)2^{-M})\}\right) +\\ 
    %    &\lambda\left(x: |f^n(x) - f(x)| \leq 2^{-M} \cap \{x: f^n(x) \in [(K-1)2^{-M}, K2^{-M})\}\right) +\\ 
     %       &\lambda\left(x: |f^n(x) - f(x)| \leq 2^{-M} \cap \{x: f^n(x) \in [(K-1)2^{-M}, K2^{-M})\}\right) +\\ 
    &2\lambda\left(\{x: |f^n(x) - f(x)| > 2^{-M}\} \cap E\right)|.
\end{align*}

Summing over $i = 1 \dots K$, we obtain 
$$2\lambda(\{|f-f^n| \leq 2^{-M}\} \cap E) + 2K\lambda(\{|f-f^n| > 2^{-M}\} \cap E)$$
Adding the similar terms over the negative parts of $S_M$, we can bound
$|S_M(f^n1_{|f^n| \leq K}) - S_M(f1_{|f| \leq K})|$
by 
$$2^{-M+2}\left(\lambda(\{|f-f^n| \leq 2^{-M}\} \cap E) + K\lambda(\{|f-f^n| > 2^{-M}\} \cap E)\right),$$
which is bounded by $2^{-M+2}\lambda(E) + K2^{-M+2} \lambda(|f-f^n| > 2^{-M})$. The first term is bounded by $\eps / 10$ by assumption in Step 1, and the second term is bounded by $\eps/10$ for all $n$ large enough by the assumption just above. This gives us altogether $\eps/5$ as required.



%Summing over $i = 1 \dots K$, we obtain 
%$$2(\lambda(\{|f-f^n| \leq 2^{-M}\} \cap E) +K\lambda(|f^n| > K2^{-M}) + K\lambda(|f| > K2^{-M}) + 2K\lambda(\{|f-f^n| > 2^{-M}\} \cap E),$$
%and adding the similar terms over the negative parts of $S_M$, we can bound
%$|S_M(f^n1_{|f^n| < K}) - S_M(f1_{|f| < K})|$
%by the following sum
%$$2^{-M+1}\left(2\lambda(\{|f-f^n| \leq 2^{-M}\} \cap E) + K\lambda(|f^n| > K2^{-M}) + K\lambda(|f| > K2^{-M}) + 2K\lambda(|f-f^n| > 2^{-M})\right).$$
%The first term can be bounded by $2^{-M+2}\lambda(E) < \eps / 10$ by assumption in Step 1. The second and third term by $2\eps/10$ using Equation \eqref{eq:bound1}. Finally, the forth term is bounded by $\eps/10$ using the assumption just above. This gives us altogether $6\eps/10$ as required.
\end{proof}

\noindent $\star $ \textit{End of non-examinable section} $\star $\\

A direct consequence is the so called Monotone convergence theorem - here the key difference is (besides that we work with non-negative functions) that we don't assume the integrability of the limiting function, but get it thanks to monotonicity and an uniform bound on the integrals. 

\begin{thm}[Monotone convergence theorem]
Let $0 \leq f^1 \leq f^2 \dots$ be non-negative, measurable and integrable functions converging to some $f$ pointwise. Suppose that there is some $E \subseteq \R^n$ of finite Lebesgue measure with $\{f^n \neq 0\} \subseteq E$ for all $n \geq 1$. 

Suppose further that for some $C > 0$, we have that $\int f^n d\lambda < C$ for all $n \geq 1$. Then also $f$ is integrable and $\int f d\lambda = \lim_{n \to \infty} \int f^n d\lambda$.
\end{thm}

We would like to apply the Dominated convergence theorem, yet we are missing a dominating function as we don't know that $f$ is integrable. So we will show that.

\begin{proof}
Let $ f_0 = \lceil f \rceil$ be the $0$-dyadic approximations from above. 
To show that $f$ is integrable, it suffices to show that
$$\int f_0 d\lambda = S_0(f) = \sum_{i \geq 1}i \lambda(f \in [i, i+1)) < \infty,$$
which is true if and only if there is some $D > 0$ with 
$$\int f_0 1_{f_0 < N+1} d\lambda = S(f_0 1_{f_0 < N+1}) = \sum_{i = 1}^N i\lambda(f \in [i, i+1)) < D$$ for all $N \geq 1$.
Now, by assumption $f^n \to f$ pointwise. We claim that hence also for all $N \geq 1$,
the functions 
$f_0^n 1_{f_0^n < N+1}$ converge pointwise. Indeed, by a slight modification of our representation in Lemma \ref{lem:repsmp}
\begin{equation}\label{eq:mon1}
f_0^n 1_{f_0^n < N+1} = \sum_{i =1}^N 1_{\{x: f^n(x) \in [i,N+1)\}}.
\end{equation}
But as $f^n$ are pointwise increasing, this function is too; moreover it takes values in $\{0, 1, \dots, N\}$. Thus it converges pointwise to some function $\tilde f$ that also takes values in $\{0, 1, \dots, N\}$, and as it is non-zero only on the set $E$, it is in particular integrable. Notice that we do not necessarily have $\tilde f = f_0 1_{f_0 < N+1}$ 
\footnote{Here we made a mistake in class saying that we have the pointwise convergence $f_0^n 1_{f_0^n < N+1} \to  f_0 1_{f_0 < N+1}.$ This is not true as you see on the example sheet.}, yet there is still a relation between the two: 
$$1 + \tilde f \geq f_0 1_{f_0 < N+1}.$$ 
Indeed, this follows by just looking at the value of $\tilde f$ point by point using the representation \eqref{eq:mon1}: 
\begin{itemize}
    \item for any $x \in \R^n$ with $f^n(x)$ converges to $j = 1 \dots N$ from strictly below, we have that 
    $$1_{\{y: \tilde f(y) \in [j, N+1)(x)\}}(x) = 0 \leq 1 = 1_{\{y: f(y) \in [j, N+1)(x)\}}(x)$$
    \item for any $x \in \R^n$ such that $f^n(x)$ converges to $N+1$ from strictly below we have that $$1_{\{y: \tilde f \in [i, N+1)\}}(x) = 1 \geq 0 = 1_{\{y: f(y) \in [i, N+1)\}}(x);$$
    \item for all other cases $1_{\{y: \tilde f \in [i, N+1)\}}(x) = 1_{\{y:  f \in [i, N+1)\}}(x)$.
\end{itemize} 

Now, $|f_0^n 1_{f_0  < N+1}| \leq N+1$ and thus we can use Dominated convergence theorem to conclude that
$$\int \tilde f d\lambda = \lim_{n \to \infty } \int f_0^n 1_{f_0^n < N+1} d\lambda.$$
Further by non-negativity, definition of $f^n_0$ and assumption
$$\int f_0^n 1_{f_0^n < N+1} d\lambda \leq \int f_0^n d\lambda \leq \int  fd\lambda < C.$$ 
Thus by Proposition \ref{prop:dom} we conclude
$$\int f_0 1_{f_0 < N+1} d\lambda \leq \int \tilde fd\lambda < C $$
and thus indeed $f$ is integrable. We can now apply dominated convergence theorem to $f^n$ using the dominating function $f$ and conclude that their pointwise limit $f$ is integrable and that its integral is given by the limit. 
\end{proof}

Finally, let us weaken the hypothesis even further, but also obtain a weaker result:

\begin{lemma}[Fatou's lemma]
Let $(f^n)_{n \geq 1}$ be a sequence of non-negative measurable functions with $\{f^n \neq 0\} \subseteq E$ for some Borel $E$ with $\lambda(E) < \infty$. Suppose that $f^n$ are all integrable and $\liminf_{n \geq 1} \int f^n d\lambda < \infty$.

Then
$$\int \liminf_{n \geq 1} f^n d\lambda \leq \liminf_{n \geq 1} \int f^n d\lambda.$$
\end{lemma}

\begin{proof}
Notice that $f := \liminf_{n \geq \infty} f^n$ is the increasing limit of $g^n = \inf_{m \geq n} f^m$. We have seen that both $(g^n)_{n \geq 1}$ and $f$ are measurable.
Further by definition $0 \leq g^n \leq f^m$ for all $m \geq n$ and thus by Proposition \ref{prop:dom}, the functions $g^n$ are integrable and $\int g^n d\lambda \leq \inf_{m \geq n} \int f^m d\lambda < C$ for some $C > 0$ by the fact that $\liminf_{n \geq 1} \int f^n d\lambda < \infty$. 

We can now apply Monotone convergence theorem to the sequence $(g^n)_{n \geq 1}$ to see that $f$ is integrable and also 
$$\int f d\lambda = \lim_{n \geq 1} \int g^n d\lambda \leq \liminf_{n \geq 1} \int f^n d\lambda.$$
\end{proof}

These three results are important in several ways: first, it allows us to more easily prove many properties of the integral as we can use any suitable approximating sequence of functions, and second it will be a key ingredient in studying the functional space of all integrable functions. 

\begin{rem}
As said, these are the key properties upon which one builds the rest of the theory. So it is interesting to observe that both the dominated convergence theorem and the monotone convergence theorem fail for the Riemann integral! Indeed, enumerate the rationals in $[0,1]$ by $q_1, q_2, \dots$ and set $f_n = 1_{\{q_1, \dots q_n\}}$. Then all $f_n$ are integrable, $f_n$ are non-negative and increasing, they are all dominated by $f = 1_{[0,1]}$ and yet their limit is not integrable. Indeed, already the Proposition \ref{prop:dom} fails in the case of the Riemann integral, as can be seen by the same counterexample.
\end{rem}

\subsubsection{Lebesgue integral over $\R^n$ and Borel subsets}

Before further properties let us extend our definition of the integral to $\R^n$ (and thus also all Borel subsets of it).

\begin{defn}[Lebesgue integral on $\R^n$]
Let $f: \R^n \to \R$ be measurable.
We call $f$ integrable if for all $m \geq 1$, $f1_{[-m, m]^n}$ is integrable and such that $\sup_{m \geq 1}\int_{[-m,m]^n} |f| d \lambda < \infty$. 
\end{defn}

%\begin{eg}
%\item $\exp(-|x|)$ is integrable over $\R$, but the constant function $c$ for example is not integrable. Indeed, based on an earlier calculation we did on the blackboard, we can see that $\int 1_{[0, M]} \exp(-x) d\lambda < 1$ for all $M$ and thus $\int 1_{-M, M} \exp(-|x|) d\lambda < 2$ giving the integrability. On the other hand for the constant function $\int 1_{-M, M} c d\lambda = 2cM$, whose supremum over $M$ is clearly unbounded.
%\end{eg}

Let us also give an alternative definition of the Lebesgue integral, that is more common in mathematical analysis and maybe more elegant mathematically. We went for the definition with dyadic approximations, because of its concreteness and intuitive comparison with the Riemann integral.

\begin{defn}[Lebesgue integral on $\R^n$ def 2]
Let $f: \R^n \to \R$ be measurable. Write $f = f_+ - f_-$ where $f_+ := \max(f,0)$, $f_- := \max(-f,0)$ are the positive and negative parts of $f$ respectively.

We call $f$ integrable if $S^+ := \sup_{0 \leq g \leq f_+} S(g)$ and $S^- := \sup_{0 \leq g \leq f_-}S(g)$ are finite, where the sup is taken over all simple functions with finitely many different values. In this case we define
$$\int f d\lambda := S^+ - S^-$$
\end{defn}

\begin{rem}
Although we defined measurable functions with the domain of $\R^n$, there is a natural notion of measurable for functions defined on any Borel subset $E \subseteq \R^n$ - indeed, both of the given definitions generalise immediately.

Moreover a function $f$ defined on a Borel subset $E \subseteq \R^n$ will be (Borel)-measurable if and only if the extended function $\tilde f$ defined to be equal to $f$ on $E$ and zero on $E^c$ is measurable.

The integral of $f$ on $E$ can be then defined exactly as we have done for $\R^n$, but in fact it can also be defined just as $\int_E f d\lambda = \int_{\R^n}1_E \tilde f d\lambda$, so there is really nothing new here.
\end{rem}

We will assume that the three main theorems proved in the last subsection hold in this larger generality:
\begin{assum}
The dominated convergence theorem, the monotone convergence theorem and Fatou's lemma work as stated above without the assumption that $\lambda(f \neq 0) < \infty$ and for integrals over any Borel subset $E \subseteq \R^n$.
\end{assum}

While at extensions, let us also define the integral for complex-valued functions. First we say that $f: \R^n \to \CB$ is measurable if both its real and imaginary parts are measurable.

\begin{defn}[Lebesgue integral for complex valued functions]
Let $f: \R^n \to \CB$ be measurable and write $f = g + ih$ for $g, h$ real measurable. We say that $f$ is integrable if $|f|$ is integrable and define $$\int f d\lambda := \int g d\lambda + i \int h d\lambda.$$ 
\end{defn}

It is an easy check that
\begin{exo}
Let $f: \R^n \to \CB$ be measurable and write $f = g + ih$ for $g, h$ real measurable. Then $f$ is integrable if and only if both $g, h$ are integrable.
\end{exo}

Complex integrals are not only for mathematical amusement, they enter naturally when we talk about the Fourier transform, which is maybe the most common and natural integral transform. Philosophically, the Fourier transform gives us a different view on our function - it helps to look at it from the perspective of 'waves'. 

In short, the Fourier transform of an integrable function $f$, denoted either by $\mathcal{F}f(k)$ or $\hat f(k)$ evaluated at $k \in \R^n$ is just the integral of $\exp(-i k \cdot x) f(x)$ (why is it integrable?). Sometimes $\exp(-ik \cdot x) = \cos(k \cdot x) - i\sin(k \cdot x)$ is called  the complex wave.

\begin{defn}
Let $f: \R^n \to \CB$ be integrable. Then the Fourier transform $\mathcal{F}(f)$ or $\hat f$ is defined by the function $\mathcal{F}(f): \R^n \to \CB$ given by 
$$\hat f(k) := \mathcal{F}f(k) := \int_{\R^n}f(x) \exp(-i k\cdot x) d\lambda(x).$$
\end{defn}

\begin{rem}
Sometimes other conventions are used, e.g. one sometimes uses a different normalization
$$\mathcal{F}f(k) := \frac{1}{\sqrt{2\pi}}\int_{\R^n}f(x) \exp(-i k\cdot x) d\lambda(x)$$
or
$$\mathcal{F}f(k) := \int_{\R^n}f(x) \exp(-i 2\pi k\cdot x) d\lambda(x).$$
Each has some advantages / disadvantages.
\end{rem}
We will postpone the study of $\mathcal{F}(f)(k)$ as a function of $k \in \R^n$ to a later point, but just mention here that the Fourier transform is key both in solving partial differential equations but also in explaining the position-momentum duality in quantum mechanics. Mathematically, it helps to diagonalize $\Delta$ and is one of the most useful integral / linear transformations.

\begin{comment}
First, as a corollary of the dominated convergence theorem we can now approximate the integral easily by any finitely changing step functions:

\begin{cor}[Approximation by finite step functions]
Consider a measurable function $f$ whose support is contained in some Borel set $E$ that satisfies $\lambda(E) < \infty$ and is integrable. 

Let $(f^n)_{n \geq 1}$ be any simple functions of the format $f^n = \sum_{i = 1}^{M_n} c_{n,i} 1_{E_{n,i}}$, for some $c_i \in \R$, $E_{n, i}$ Borel and such that $E_{n,i} \cap E_{n,j} = \emptyset$ for $i \neq j \in 1 \dots M_n$ and $E_{n,i} \subseteq E$ for all $n \geq 1, i = 1 \dots M_n$.

Then if $f^n \to f$ pointwise and $|f_n| \leq |f| + K$ for some $K \in \N$, we have that $\int f_n d\lambda \to \int f d\lambda$.
\end{cor}
\begin{proof}
We have seen that if $f$ is integrable, then so is $|f|$. Further $|f| + 1_E$ is integrable by just the definition. Hence under the conditions above the dominated convergence theorem implies $\int f_n d\lambda \to \int f d\lambda$.
\end{proof}


Before proving some further properties let us verify that as long as Riemann and Lebesgue integral are defined for a function $f$, they agree:

\begin{prop}[Riemann vs Lebesgue easy]
Let $f$ be measurable, zero on the complement of $[a,b]$ and Riemann-integrable on $[a,b]$. Then 
$$\tiny{\circledR}\int_a^b f(x)dx = \int f d\lambda.$$
\end{prop}

\begin{proof}
As discussed at the beginning of this section, as $f$ is Riemann-integrable, we have that 
$$\tiny{\circledR}\int_a^b f(x)dx = \lim_{n \to \infty } 2^{-n} \sum_{i = 0}^{2^n} \inf_{x \in D_i} f(x).$$
But now if we set $\hat f_n(x) := \sum_{i=0}^{2^n}1_{x \in D_i} \inf_{x \in D_i} f(x)$, then $\hat f_n(x)$
is a simple function and its Lebesgue integral is equal to exactly $2^{-n} \sum_{i = 0}^{2^n} \inf_{x \in D_i} f(x)$.

\end{proof}

\begin{rem}[Non-examinable remark for enthusiasts]
Here we left aside the interesting question: is a Riemann-integrable function automatically Lebesgue-integrable? Given the proposition, the question can be restated as - is a Riemann-integrable function measurable? And the answer is: it depends. In fact with our definition of a measurable function this is not quite true (if $f$ is Riemann integrable, there is a measurable function $\tilde f$ such that $\lambda(f \neq \tilde f) = 0$), but one can define a slightly larger class of Lebesgue measurable functions by using a larger $\sigma-$algebra than the Borel $\sigma$-algebra and in that setting every Riemann integrable function is Lebesgue integrable. But even in our setting it is philosophically true. 
\end{rem}
\end{comment}

\subsection{The space of integrable functions}

Our aim is now to define the space of all integrable functions and look at its structure. We start with some further properties of the Lebesgue integral: 

\begin{prop}
Let $f, g$ be Lebesgue integrable functions. We have the following properties:
\begin{itemize}
\item Linearity: for $a, b \in \R$, then $af + bg$ is integrable and $$\int (af+bg) d\lambda = a\int f d\lambda + b\int g d\lambda$$
\item Additivity: if $E, F$ are Borel measurable and disjoint, then 
$$\int_{E \cup F} f d\lambda = \int_E f d\lambda + \int_F f d\lambda$$
\item Monotonicity: if $\lambda(f < g) = 0$, then $$\int f d\lambda \geq \int g d\lambda$$
%\item Boundedness: if $\lambda(\{f < c\} \cup \{f > C\}) = 0$, then 
$$c\lambda(f \neq 0) \leq \int f d\lambda \leq C\lambda(f \neq 0).$$
\end{itemize}
\end{prop}

\begin{proof}[Proof sketch:]
The properties 2-3 follow easily from the property 1, and are left as an exercise.

For property 1, the case that $af$ is integrable and $\int af d\lambda = a \int d\lambda$ is in the exercise sheet. Let us show that if $f,g$ integrable, then $f + g$ integrable and $\int (f +g) d\lambda = \int f d\lambda + \int g d\lambda$. 

Consider a sequence of simple functions $(f^n, g^n)_{n \geq 1}$ that take only finitely many values such that they converge pointwise to $f,g$ respectively and $|f^n| \leq |f|, |g^n| \leq |g|$ pointwise too (it is worth thinking how to construct them explicitly).

\begin{claim}
For any two simple integrable functions $f^n, g^n$ taking only finitely many values, we have that $$\int f^n d\lambda + \int g^n d\lambda = \int (f^n + g^n)d\lambda.$$
\end{claim}

\begin{proof}
Recall that by Lemma \ref{lem:intsimple} the integral of a simple function is just a sum, which in this case will be finite. E.g. if $f$ takes values $\{c_1, \dots, c_N\}$ and $g$ takes values $\{d_1, \dots, d_M\}$ we can write:
$$\int f^n d\lambda  = \sum_{i = 1}^N c_i \lambda(f^n = c_i) ,$$
$$\int g^n d\lambda  = \sum_{j = 1}^M d_j \lambda(f^n = d_j)$$
and 
$$\int (f^n + g^n) d\lambda  = \sum_{k = 1}^L d_j \lambda(f^n  + g^n= e_k)$$
where $\{e_k: k =1 \dots L\} := \{c_i + d_j: i = 1 \dots N, j = 1 \dots M\}$ is the set of all the possible values $f^n +g^n$ can take.

Thus the proof of the claim is an elementary verification for finite sums that goes by changing the order of sums a few times. The details will be on the exercise sheet.
\end{proof}
\begin{comment}
By Lemma \ref{lem:intsimple} we for any two such simple functions $f^n, g^n$ we have
$$\int f^n d\lambda + \int g^n d\lambda = \sum_{i = 1}^N c_i \lambda(f^n = c_i) + \sum_{j = 1}^M d_j \lambda(g^n = d_j).$$
We will show that this is equal to $\int (f^n + g^n) d\lambda$. This is an elementary verification again by changing a few times the order of sums.
So let $f^n + g^n$ takes values in the finite set $V = \{c_i + d_j: i = 1 \dots N, j = 1 \dots N\}$ and each such possible value $v \in V$, we have that 
$$\lambda(g^n + f^n = v) = \sum_{i=1}^N\sum_{j=1}^M1_{c_i + d_j = v}\lambda(x: \{f^n(x) = c_i \} \cap \{g^n(x) = d_j\}).$$ 
Thus by Lemma \ref{lem:intsimple} we have
$$\int f^n + g^n = \sum_{v \in V} v \sum_{i=1}^N\sum_{j=1}^M1_{c_i + d_j = v}\lambda(x: \{f^n(x) = c_i \} \cap \{g^n(x) = d_j\}),$$
which we can rewrite as
$$\sum_{v \in V} \sum_{i=1}^N\sum_{j=1}^M(c_i + d_j)1_{c_i + d_j = v}\lambda(x: \{f^n(x) = c_i \} \cap \{g^n(x) = d_j\}).$$
As these are finite sums, we can change the order of sums and sum over $v$ to obtain
$$ \sum_{i=1}^N\sum_{j=1}^M(c_i + d_j)\lambda(x: \{f^n(x) = c_i \} \cap \{g^n(x) = d_j\}).$$
But now $\cup_{j = 1}^M \{x: \{f^n(x) = c_i \} \cap \{g^n(x) = d_j\} = \{f^n = c_i\}$ and similarly $\cup_{i = 1}^N \{x: \{f^n(x) = c_i \} \cap \{g^n(x) = d_j\} = \{g^n = d_j\}$. Thus separating the sum into two parts and summing over $i$ for one of the sums and over $j$ for the other, we obtain that
$\int f^n + g^n d\lambda = \int f^n d\lambda + \int g^n d\lambda$. 
\end{comment}
Given the claim, we would like to use the fact that $f^n + g^n \to f + g$ pointwise and use the Dominated convergence theorem to conclude. Yet, it still remains to find a dominating function that is integrable.

To do this, observe that $|f^n + g^n| \leq |f^n| + |g^n| \leq |f| + |g|$ by the triangle inequality and by assumption respectively. Doing the same calculation as above for $|f^n|, |g^n|$ we obtain 
$$\int |f^n| + |g^n| d\lambda = \int |f^n| d\lambda + \int |g^n| d\lambda \leq \int |f| d\lambda + \int |g| d\lambda$$ by Proposition \ref{prop:dom}.
Thus as $|f^n| + |g^n| \to |f| + |g|$ pointwise, we can use Monotone convergence theorem to see that $|f| + |g|$ is integrable.
Thus we can apply Dominated convergence theorem to $f^n + g^n$ with the dominating function $|f| + |g|$ and conclude.

\end{proof}

We now want to study the space $L^1(E)$ of all measurable, Lebesgue integrable functions on some Borel subset of $E \subseteq \R^n$, the main examples being $E = \R^n$ and $E = [0,1]^n$.

\begin{defn}[The space $L^1$]
Let $E \subseteq \R^n$ be of positive measure. We set $L^1(E)$ to be the set of all measurable functions on $E$ that are integrable.
\end{defn}

From the proposition above, we see that this space has a linear structure. Further, dominated convergence theorem also gives a method to give sense to infinite sums. So could we hope for the structure of a Banach space like before?

First, what should be the norm? The natural candidate is $\|f\|_1 := \int_E |f| d\lambda$, as it is finite by definition for every integrable function and measures the size of $f$ in terms of its integral. Further, we saw that if $f$ is measurable and $\lambda(f \neq 0) = 0$, then $f$ is integrable and $\int f d\lambda = 0$. However for $\|f\|_1$ to be a norm, we also need to know that if $\|f\|_1 = 0$, then $f = 0$. But this is clearly not true! Indeed, we saw that for example on $[0,1]$ the function $1_{\Q \cap [0,1]}$ is (Lebesgue) integrable but its integral is equal to $0$.

The best we can do is:

\begin{lemma}
Let $f$ be measurable.
\begin{itemize}
\item If $\lambda(f \neq 0) = 0$, then $f$ is integrable and $\int f d\lambda = 0$.
\item If on the other hand $f$ is integrable, non-negative and $\int f d\lambda = 0$, then $\lambda(f \neq 0) = 0$.
\end{itemize}
\end{lemma}

\begin{proof}
Suppose that $\lambda(f \neq 0) = 0$. Then $f$ is integrable by the definition, and we can either use the definition of the integral or monotonicity from the proposition above to see that $\int f d\lambda = 0$.

For the other direction suppose that $\int f d\lambda = 0$. Now, we have that $\{x: f(x) \neq 0) = \cup_{n \geq 1} \{x:f(x) \geq 2^{-n}\}$. And thus by the union bound
$$\lambda(f \neq 0) \leq \sum_{n \geq 1}\lambda(f \geq 2^{-n}).$$
So it suffices to show that $\lambda(f \geq 2^{-n}) = 0$ for all $n \geq 1$. But now, $E = \{x: f(x) \geq 2^{-n}\}$ is Borel measurable and by the Proposition above $$\int f d\lambda = \int f1_{E} d\lambda + \int f 1_{E^c} d\lambda.$$  The second term is non-negative as $f \geq 0$, but $f1_E \geq 2^{-n}1_E$ and thus by the Proposition above the first term in the right-hand side is greater than $\int 2^{-n} 1_E d\lambda = 2^{-n} \lambda(E)$. But by assumption $int f d\lambda = 0$, and thus we conclude that $\lambda(E) = 0$. As this holds for all $n$, we conclude. 
\end{proof}

We conclude that $\|f\|_1$ can never be a real norm on the space $L^1(E)$. So what shall we do?
First, we see that it will be important to talk about properties that happen everywhere but on a set of zero measure:

\begin{defn}[Almost everywhere]
Let $f$ be measurable. We say that a property $E = \{x \in \R^n: f(x) \in F\}$ for a Borel subset $F \subseteq \R$ holds almost everywhere if $\lambda(E^c) = 0$. \footnote{In probability theory one uses the expression almost surely.}
\end{defn}

\begin{eg}
For example we say that $f = g$ almost everywhere if $\lambda(\{x: f(x) \neq g(x)\}) = 0$, or $f \leq g$ almost everywhere if $\lambda(\{x: f(x) > g(x) \}) = 0$. Or we say that a sequence of measurable functions $f_n$ converges to $f$ almost everywhere, if $\lambda(\{x: f_n(x) \to f(x)\}^c) = 0$.

Also, the second point of the previous lemma can be reworded as saying that if $f$ is non-negative and integrable and of zero integral, then $f = 0$ almost everywhere.
\end{eg}

Observe that as a Corollary of the previous lemma we obtain:

\begin{cor}
If $f$ is integrable and $f = g$ almost everywhere, then $g$ is also integrable and 
$\int g d\lambda = \int f d\lambda$. Moreover, also $|f-g|$ is integrable and has zero integral.
\end{cor}

\begin{proof}
Indeed, we have that $h = g -f = 0$ almost everywhere and hence by the Lemma above $h$ is integrable and its integral is zero. Thus by the linearity of the integral we see that $g = f + h$ is also integrable and its integral equals that of $f$. But also $|g-f| = 0$ almost everywhere, so the final claim also follows directly.
\end{proof}


Thus for the space of integrable functions we should really consider any pair of functions $f,g$ that are equal almost everywhere as the same function, as their difference has zero norm. This motivates the following definition:

\begin{defn}[The space $\Lc^1(E)$]
Let $E$ be borel with $\lambda(E) > 0$. We define $\Lc^1(E)$ the set of equivalence classes $[f]$ of integrable functions on $E$, with respect to the equivalence relation $f \sim g$ iff $f = g$ almost everywhere. 
\end{defn}

\begin{rem}
Here we are implicitly using the fact that $f \sim g$ iff $f = g$ almost everywhere really is an equivalence relation; this will be verified on the exercise sheet. Notice that for such an equivalence class it doesn't make sense to talk about the value of the function at some specific point $x$. 
\end{rem}

Also observe that by the Corollary above $\int f d\lambda = \int g d\lambda$ for all $f \sim g$ and thus we can assign an integral to any equivalence class $[f]$ by just choosing some representative $f$ and taking its integral. We will thus often abuse the notation and even if we talk of an equivalenc class $[f]$, just denote it by $f$ and its integral by $\int_E f d\lambda$.

We can now state the main theorem of this Section:

\begin{thm}[Banach space structure of $\Lc^1(E)$]
The set $\Lc^1(E)$ with addition and with the norm $\|f\|_1 := \int_E |f| d\lambda$ is a Banach space - i.e. a complete normed vector space.
\end{thm}

\begin{rem}
As mentioned, the fact that we work with equivalence classes and not functions is mathematically crucial, as otherwise we don't have a real norm. 

But it might well be that this inconvenience is prescribed to us by nature - often we can physically just measure some macroscopic things and indeed pointwise values might not be physically relevant, or not even have physical sense - e.g. in quantum mechanics / quantum field theory it also doesn't make sense to talk about pointwise values of wave functions, just like here; or does it really make sense to talk about the density of a gas at some fixed point, as we can only measure the density over some macroscopic regions?

\end{rem}

\begin{rem}
In what follows we will often denote the equivalence classes just by $f$ as if they were just functions, as everything can be worked out by taking just representatives of each equivalence class; or indeed, we work with $L^1(E)$ if the statements allow so. We will often also forget about $E$, unless in statements where it does make a difference.
\end{rem}

Now the linear structure, and the fact that $\| \cdot \|_1$ really is a norm on the space of equivalence classes follows from the work we've done so far. Also, observe that dominated convergence theorem says that if $f_n$ converges to some $f$ w.r.t. $\| \cdot \|_1$, then $f \in L^1$ as well (Exercise). The part that is really new is the completeness. 

\begin{prop}[Completeness]\label{prop:l1cmp}
Suppose that a sequence of functions $(f_n)_{n \geq 1}$ in $L^1(E)$ is Cauchy w.r.t. $\|\cdot \|_1$. Then there is some integrable function $f$ such that $f_n$ converges towards this function w.r.t $\|\cdot \|_1$, i.e. $\|f - f_n\|_1 \to 0$.
\end{prop}

This proposition is rather interesting, because how should the limiting function be constructed? 

As a first naive thought, one could imagine that maybe being Cauchy w.rt. $\| \cdot \|_1$ will tell us something about convergence pointwise or at least convergence almost everywhere. This is really not so simple:
\begin{itemize}
\item First, as we have seen pointwise convergence does not mean the integrals converge. In particular, one can find a sequence of functions $f_n$ that converge pointwise to $f$ but such that $\|f_n - f\|_1$ doesn't converge. In fact, it could even diverge!
\item Conversely, one can find a sequence of functions $(f_n)_{n \geq 1}$ in $L^1(E)$ such that $\|f_n -f \|_1 \to 0$ as $n \to \infty$, but $\lambda(E \cap \{x: f_n(x) \to f(x)\}) = 0$. 
\end{itemize}

Here is one example to illustrate the 2nd point.

\begin{eg}
For $n \in \N$ let $k_n$ be such that $2^{k_n} \leq n < 2^{k_n+1}$. We define on $[0,1]$ a sequence of functions by $f_n(x) := 1_{[2^{-k_n}(n-2^{k_n}),2^{-k_n}(n+1-2^{k_n})]}$. Then $f_n$ are all integrable and observe that $\|f_n\| = 2^{-k_n}$, which goes to zero as $n \to \infty$. On the other hand for every $x \in [0,1]$ there are infinitely many $n$ such that $f_n(x) = 0$ and infinitely many $n$ such that $f_n(x) = 1$ and hence $f_n(x)$ cannot converge.
\end{eg}

To understand why the proposition still has chances to be true, consider the following lemma explaining that functions that are close in the $\| \cdot \|_1$ norm are still also close pointwise in the sense of measure.

\begin{lemma}[Markov inequality]
Suppose $f$ is integrable and non-negative. Then $\lambda(\{x: f(x) > \beta\}) \leq \frac{\int f d\lambda}{\beta}$.

In particular, if $f, g \in L^1(E)$ satisfy $\|f-g\|_1 \leq \eps$, then $\lambda(\{x: |f(x) - g(x)| > \lambda\}) \leq \eps / \lambda$. 
\end{lemma}

\begin{proof}
%We have 
%$$\eps \geq \|f -g \|_1 \geq \|(f-g)1_{|f-g| > \lambda} > \lambda\lambda(\{x: |f(x) - g(x)| > \lambda\}),$$
%where at the last step we used that $\int_E d\lambda(x) = \lambda(E)$.
On the exercise sheet
\end{proof}

In particular, if two functions are close in the sense of $\|\cdot\|_1$ they can differ pointwise even macroscopically, but only on a small set.

This helps us prove the following key observation, which says that if a sequence of functions is Cauchy in $L^1$, then some subsequence of it converges almost everywhere. In particular, as every convergent sequence is Cauchy (why?), it means that every sequence of functions converging w.r.t. $\|\cdot\|_1$ to some $f$, has an almost everywhere convergent subsequence to a function that is equal to $f$ almost everywhere. 

\begin{lemma}
Let $(f_n)_{n \geq 1}$ be Cauchy sequence of functions in $L^1(E)$, i.e. we know that for every $\eps > 0$ there is some $n_\eps \in \N$ with $\|f_m - f_n\|_1 < \eps$ for all $m, n \geq n_\eps$. 

Then we can pick a subsequence $(f_{n_k})_{k \geq 1}$ and find a function $f \in L^1(E)$ such that $f_{n_k}$ converges to $f$ almost everywhere and w.r.t. $\| \cdot \|_1$ as $k \to \infty$. 
\end{lemma}

This lemma easily implies Proposition \ref{prop:l1cmp}. Indeed, we have $$\|f - f_n\| \leq \|f_{n_k} - f_n \| - \|f - f_{n_k}\|$$ and this can be made smaller than any $\eps$ by first taking $n_k$ large enough so that the second term is less than $\eps/2$ (Lemma) and in the Cauchy condition $\|f_n - f_m\|_1 < \eps/2$ for all $n \geq n_k$, and then choosing $n \geq n_k$. Thus we conclude that $(f_n)_{n \geq 1}$ also converges to $f$ w.r.t. $\|\cdot \|_1$.

\begin{proof}[Proof of Lemma]
Choose $n_k$ such that for all $n, m \geq n_k$, $\|f_m - f_n\| < 2^{-3k}$.

We first claim that the event
$$E_k := \{x: \exists l \geq k \text{ s.t. } |f_{n_l}(x) - f_{n_k}(x)| \geq 2^{-k}\}$$
satisfies $\lambda(E_k)  \leq 2^{-k+1}$. Indeed we can bound
$$\{|f_{n_l}(x) - f_{n_k}(x)| \geq 2^{-k}\} \subseteq \cup_{j = k}^{l-1} \{|f_{n_{j+1}}(x) - f_{n_{j}}(x)| \geq 2^{-2j}\}$$
and thus 
$$E_k \subseteq \cup_{j \geq k} \{|f_{n_{j+1}}(x) - f_{n_{j}}(x)| \geq 2^{-2j}\}.$$
Now, by the union bound 
$$\lambda(\cup_{j \geq k} \{|f_{n_{j+1}}(x) - f_{n_{j}}(x)| \geq 2^{-2j}\}) \leq \sum_{j \geq k} \lambda(\{|f_{n_{j+1}}(x) - f_{n_{j}}(x)| \geq 2^{-2j}\})$$
and by the choice of $n_k$ and the Markov inequality
$$\lambda(\{|f_{n_{j+1}}(x) - f_{n_{j}}(x)| \geq 2^{-2j}\}) \leq 2^{-j}.$$
Summing over $j \geq k$ then gives that $\lambda(E_k) \leq 2^{-k+1}$.

Consider now the set of all $x \in E$ such that $(f_{n_k}(x))_{k \geq 1}$ is Cauchy:
$$F := \{x: (f_{n_k}(x))_{k \geq q} \text{ is Cauchy}\}.$$
We have
$F^c \subseteq \cap_{n \geq 1} \cup_{k \geq n} E_k$
and hence for every $n \in \N$
$$\lambda(F^c) \leq \lambda(\cup_{k \geq n} E_k) \leq \sum_{k \geq n} \lambda(E_k) \leq 2^{-n+2},$$
which implies that $\lambda(F^c) = 0$.

But now for every $x \in F$, as $(f_{n_k}(x))_{k \geq 1}$ is Cauchy in $\R$, we can define $f(x)$ as the limit of this Cauchy sequence. On the other hand for $x \notin F$, we can just set $f(x) := 0$ (Why is such an $f$ measurable?). By definition then $(f_{n_k})_{k \geq 1}$ converges to $f$ on the event $F$, i.e. almost everywhere.

To see the convergence w.r.t $\| \cdot \|_1$ we observe that on the event $F$ we can define $f(x)$ explicitly:
$$f(x) := f_{n_1}(x) + \sum_{j \geq 1} (f_{n_{j+1}}(x) -f_{n_j}(x)).$$
Indeed for every $x \in F$, the series 
$$g(x) := |f_{n_1}(x)| + \sum_{j \geq 1} |f_{n_{j+1}}(x) -f_{n_j}(x)|$$
is absolutely convergent.

The convergence w.r.t. $\|\cdot\|_1$ is left as an exercise.
%But then $$g_k := |f_{n_k} -f|$$ converge to zero pointwise and are dominated by $g$, which itself is integrable by the monotone convergence theorem.
\end{proof}


\subsection{The space of square-integrable functions}

Similarly to the set of integrable functions, we can define the set of square integrable $L^2(E)$  as the set of all measurable functions $f: E \to \R$ (or to $\CB$) such that $|f|^2$ is integrable - if we need to be more precise, we write $L^2(E, \lambda, \R)$ or $L^2(E, \lambda, \CB)$ to also point out the measure on $E$ and the values of the functions. 

A natural candidate for a norm on this set would be 
$$\|f\|_2 := \sqrt{\int_E |f|^2 d\lambda},$$
where the square-root is necessary like in the case of $\R^n$ to have a linear scaling property.

Again, we run to the same problem as above, that this does not define a norm - indeed, again for the same non-zero function $1_{\Q}$, this expression would be zero.

Thus the space of square integrable functions is in reality again defined on the set of equivalence classes of functions, where the equivalence relation as before is given by $f \sim g$ when $f = g$ almost everywhere.


\begin{defn}[The space $\Lc^2(E)$]
Let $E$ be borel with $\lambda(E) > 0$. We define $\Lc^2(E)$ \footnote{Or $\Lc^2(E, \lambda, \R)$ or $\Lc^2(E, \lambda, \CB)$ if we want to also point out the measure and the field of values...but often we just use the shorthand} the set of equivalence classes $[f]$ of square-integrable functions on $E$, with respect to the equivalence relation $f \sim g$ iff $f = g$ almost everywhere. This space is endowed with the norm 
$$\|[f]\|_2 := (\int_E f^2 d\lambda)^{1/2},$$
with $f$ some representative of $[f]$.
\end{defn}

We will again often/mostly forget about the fact that the elements are equivalence classes. 

Firstly, the space is indeed different from $\Lc^1(E)$:

\begin{exo}
Find a function $f: \R \to \R$ that is integrable but not square integrable. Find also a function $f: \R \to \R$ that is square-integrable, but not integrable.

However, prove that if $E$ satisfies $\lambda(E) < \infty$, then $L^1(E) \supseteq L^2(E)$.
\end{exo}

It can be argued exactly as above that $\Lc^2(E)$ is a Banach space (some of it is on the exercise sheet). % linear structure! 


\begin{thm}[Banach space structure of $\Lc^2(E)$]
The set $\Lc^2(E)$ with addition and with the norm $\|f\|_2$ above is a Banach space - i.e. a complete normed vector space.
\end{thm}

However, compared to $\Lc^1(E)$, the space $\Lc^2(E)$ has even more structure. In addition to a norm, one can also define a notion of an inner product / angle, very much similar to the inner product on $\R^n$. For any $f, g \in L^2(E)$ we can set
$$\langle f, g \rangle := \int_E f\bar g d\lambda,$$
with $\bar g$ denoting the complex conjugate (which is equal to $g$ if we work with real-valued functions). This assumes that $f \bar g$ is integrable, which is clear from $|f(x)g(x)| \leq |f(x)|^2 + |g(x)|^2$.

One can verify that on $L^2(E)$ (and also $\Lc^2(E)$, the definition of $\langle f, g \rangle$ above satisfies the axioms of an inner product:

\begin{defn}[Inner product]
Let $V$ be a vector space. We call $\langle v,w \rangle$ a complex inner product if the following conditions hold:
\begin{itemize}
\item Complex-valued: $\langle v,w \rangle \in \CB$
\item Conjugate symmetry: $\langle v, w \rangle = \overline{\langle w, v \rangle}$ 
\item Linearity: for all $a, b \in \CB$, we have $\langle av + bu, w \rangle = a \langle v, w \rangle + b \langle u, w \rangle$
\item Non-negativity: $\langle v, v \rangle \geq 0$.
\end{itemize}
We call it a real inner product if it is real-valued, just symmetric and linearity holds only for $a, b \in \R$.
\end{defn}

Notice that by our definitions for $f \in L^2(E)$ we have that $\|f\|_2^2 = \langle f, f \rangle$. 

A Banach space with an inner product that gives rise to is called a Hilbert space
\begin{defn}[Hilbert space]
Let $(H, \| \cdot \|)$ be a Banach space. If $H$ can be endowed with a compatible inner product $\langle f, g \rangle$, i.e. such that $\| f \|^2 = \langle f, f \rangle$, we call $H$ an Hilbert space.
\end{defn}

A way to rephrase the results mentioned above is then: 
\begin{thm}[Hilbert space structure of $\Lc^2(E)$]
The set $\Lc^2(E)$ with addition, with the norm $\|f\|_2$ and the inner product $\langle f, g\rangle$ above is a Hilbert space.
\end{thm}

Being a Hilbert space induces some algebraic conditions on the norm. For example, the parallelogram law has to be always satisfied for a real inner product $\langle v, w\rangle$:
$$\langle v, w \rangle = \frac{1}{4}(\|v+w\|^2 -\|v-w\|^2).$$
But now $\langle v, w \rangle$ has to be linear in $v, w$ and already this implies that the space $\Lc^1$ with its norm $\| \cdot \|_1$ cannot be endowed with a compatible inner product. Indeed, working on $\Lc^1([0,1])$, one can define $f = 1_{[0,1/2]}$ and $g = a1_{[1/2,1]}$ for $a \in \R$ and check that $\|f+g\|^2 -\|f-g\|^2$ is not linear in $a$.


Sometimes the definition of Hilbert space includes another condition, called separability: a Hilbert space $H$ is called separable, if it admits a countably dense subset, i.e. a countable set $S \subseteq H$ such that its closure w.r.t. the norm is the whole of $H$. Roughly its importance is guaranteeing the existence of a countable orthonormal system, i.e. a coordinate basis for $H$. 

Before stating this, let us mention that this is true for $L^2(E)$:

\begin{prop}
$L^2(E)$ (and hence also $\Lc^2(E)$) with the $L^2$-norm admit countable dense subsets $S \subseteq L^2(E)$. The same holds for $L^1(E)$ and $\Lc^1(E)$ with their appropriate norms.
\end{prop}

We will not prove the result, as it requires a (simple) result about the Lebesgue measure that we haven't covered. But let us discuss how it works for those who are interested.\\

\noindent $\star $ \textit{Start of non-examinable section} $\star $\\

First, let us consider a simpler question and  show that there is a countable dense subset in $C([0,1], \R)$ w.r.t. the uniform norm. Indeed, for every $n \in \N$ we can define the set $S_n$ of all functions that take values in $n^{-1}\Z$ on the points $[0,1] \cap n^{-1}\Z$ and are linear between these points. Our collection $S$ is then the union of $S_n$ over all $n in \N$. To check that $S$ is dense, it suffices to show that for any $f \in C([0,1], \R)$ and any $\eps > 0$ we can find some function $g \in S$ such that $\|f - g \|_\infty \leq \eps$. To do this, observe that by uniform continuity of $f$ we can choose $n$ such that whenever $|x-y| < 1/n$, we have that $|f(x) - f(y)| < \eps/3$. Now choose $m \geq n$ such that $1/m < \eps/3$ and then the function we look for is a function $g \in S_n$ defined by $g(i/n) := n^{-1}\lfloor nf(x)\rfloor$ for all $i = 0, 1, \dots, n$. The philosophy here is that every continuous function is uniquely characterised by its values on only rationals.

In the case of integrable or square-integrable functions the same strategy wouldn't work as easily:

\begin{exo}
Find an integrable function $f$ such that the sequence of functions $f_n \in S_n$ constructed by setting $f_n(i/n) := n^{-1}\lfloor nf(x)\rfloor$ for all $i = 0, 1, \dots, n$ and interpolating linearly does not converge in $L^1$ to $f$.
\end{exo}

The problem here is that contrary to continuous functions an integrable  or square-integrable function is not well described by its values at countably many points; indeed, as we saw one can just change the value on countably many points without changing the value of the integral. 

Yet the result is true. For example one can consider all simple measurable functions with values in $2^{-n}\Z$. We then know that for example on $[0,1]$ the integrals of $|f-f_n|$ with $f_n(x) = 2^{-n}\lfloor f(x)2^n\rfloor$ is bounded by $2^{-n}$. But the set of all these functions is a priori too big. Indeed, it contains all functions of the form $$g(x) = \sum_{i \in \Z}i 2^{-n}1_{E_i},$$
where $E_i$ are disjoint Borel and the collection of Borel sets is uncountable. Yet, as mentioned in the non-examinable section under Theorem 1.20 we can approximate $E_i$ as well as we wish using rectangles, and in particular with rectangles having rational co-ordinates. This gives us the result.\\

\noindent $\star $ \textit{End of non-examinable section} $\star $\\

It also holds that 

\begin{prop}
The set of continuous functions $C([0,1])$ is dense both in $L^1([0,1])$ and in $L^2([0,1])$ w.r.t. their natural norms.
\end{prop}

Although separability might sound like a mathematical musing at first, it is actually pretty central to why we consider $Lc^2(E)$ in the first place:

\begin{thm}
A Hilbert space $H$ is separable if and only if there exists an orthonormal basis $(h_n)_{n \geq 1}$ of $H$, i.e. a set $H_B$ of elements $h_1, h_2, \dots, h_n \in H$ such that 
\begin{itemize}
    \item $\langle h_i, h_j \rangle = 1_{i = j}$
\item each $h \in H$ can be written as $h = \sum_{i \geq 1}h_i \langle h, h_i \rangle$ in a unique way.
\end{itemize} 
\end{thm}

Any collection $h_1, h_2, \dots$ satisfying the conditions above is called an orthonormal basis - here orthogonal means $\langle h_i, h_j \rangle = 0$ for $i \neq j$, normal means $\langle h_i, h_i \rangle = \|h_i\|^2 = 1$, and basis means that each $h$ can be written as a linear combination of $(h_i)_{i \geq 1}$. The uniqueness part follows nicely from orthogonality as we see in a bit

%\begin{exo}
%Let $H$ be a Hilbert space over real numbers. Show that if a collection $(h_n)_{n \in J}$ is orthonormal, then there is at most one way to write an $h \in H$ in the form $\sum_{n \in J} c_n h_n $ with $c_n \in \R$.
%\end{exo}

One should think of an orthonormal basis as a co-ordinate system, very much like a co-ordinate system for the vector space $(\R^n, +)$ - in any Hilbert space, however it arise, it allows us to encode every vector $h \in H$ uniquely by a real (or complex) valued sequence $(c_1, c_2, \dots)$. In fact, it is in bijection with such sequences satisfying $\sum_{i \geq 1} c_i^2 < \infty$.

However, as in the case of $\R^n$, there are many different choices of co-ordinate systems, i.e. many different orthonormal basis in a Hilbert space. And whereas it is often easy to check that some given collection $h_1, h_2, \dots $ is orthonormal, checking that it is actually a basis is often more tricky. For example let's check that the so called Fourier series is orthonormal on $[0,1]$: 

\begin{exo}[Fourier series]
Show that the set of functions $(\exp(2\pi i n x))_{n \in \Z}$ is orthonormal for the Hilbert space $\Lc^2([0,1], \lambda, \CB)$. Can you construct an orthonormal sequence also for $\Lc^2([0,1], \lambda, \R)$?
\end{exo}

It comes out that it is much trickier to show that this series is also complete. One way is to use the Weierstrass approximation theorem:

\begin{thm}[Weiestrass approximation theorem]
The set of polynomials is dense in the space of continuous functions on $[0,1]$ w.r.t. the uniform norm.
\end{thm}

We will see another way through the general theory of linear transformations in a bit.

In some sense any orthonormal basis of a Hilbert space is like a choice of orthogonal co-ordinate axis in $\R^n$ and this is also their utility - they allow to represent each other function in the Hilbert space just using a sequence of real (or complex) numbers! I think this is very nice.

We will see how to construct basis with special properties later on, but for now we give an algorithm how to construct one. 

\subsubsection{Constructing an orthonormal basis}

As a first step, we will extract from our dense countable set, a subset that is linearly independent. 

So consider the countable set $S = \{h_1, h_2, \dots \}$ whose closure is equal to $H$. We can now make it a bit smaller as follows:
\begin{enumerate}
\item we set $S_B^1 := \{h_1\}$;
\item at step $m$ we include $h_m$ in $S_B^m$ if and only if there is no linear combination of elements in $S_B^{m-1}\cup \{h_m\}$ that gives $0$;
\item we define $S_B = \cup_{i \geq 1} S_B^i$.
\end{enumerate}
This is called a linearly independent complete set: linearly independent because we cannot find any $h_1, \dots, h_m \in S_B$ and numbers $a_1, \dots, a_m$ such that $\sum_{i = 1}^m a_i h_i = 0$; complete because for any $h \in H$ and any $\eps > 0$, we can find $h_1, \dots, h_m$ and $a_1, \dots, a_m$ such that $\| \sum_{i=1}^m a_i h_i - h\| < \eps$.

The next step is to filter out an orthonormal set. 
%To do this, we first notice that in a Hilbert space one can give sense to projections, exactly like in $\R^n$. Indeed, given a closed subspace $H_1$, i.e. the span of some vectors $\{h_1, \dots, h_n\}$, we can define the projection $P_{H_1}(h)$ as the vector in $H_1$ that minimizes $\|h - P_{H_1}(h)\|$ over all vectors in $H_1$ (Why is this well-defined?).

\begin{prop}[Gram-Schmidt orthogonalization]
Suppose we have a complete linearly independent set $S_B = \{h_1, h_2, \dots \}$. Consider the following recursive procedure:
\begin{enumerate}
\item We set $H_B^1 := \{h_1/||h_1||\}$
\item Given $H_B^{n-1}$, we set $H_B^n := H_B^{n-1} \cup \{\tilde h_n\}$ where $\tilde h_n$ is defined by normalizing the vector $h_n - \sum_{i = 1}^{n-1} h_i \langle h_n, h_i \rangle $ to have unit norm. \footnote{Here $\sum_{i = 1}^{n-1} h_i \langle h_n, h_i \rangle$ is the projection of $h_n$ onto the subspace spanned by $h_1, \dots, h_{n-1}$.}
\item We define $H_B = \cup_{ i \geq 1} H_B^i$.
\end{enumerate}
Then $H_B$ is an orthonormal basis of $H$.
\end{prop}

The usefulness of an orthonormal basis is concluded in this exercise, which should remind you very much of your linear algebra exercises:

\begin{exo}[Why are orthonormal basis useful?]
Let $H_B = (h_1, h_2, \dots)$ be an orthonormal basis of a separable real Hilbert space $H$. Prove that:
\begin{itemize}
\item (Orthogonal projections) For every $h_1, \dots, h_n$ the norm $$\|h - \sum_{i = 1}^n c_ih_i\|$$ is minimized by $c_i = \langle h_i, h_n \rangle$.
\item (Uniqueness of expansion) Suppose that $h = \sum_{i \geq 1} c_i h_i$, in the sense that for all $\eps > 0$ there is some $n \in \N$ with $\|h - \sum_{i \geq 1}^n c_i h_i\| \leq \eps$. Show that the $(c_i)_{i \geq 1}$ are uniquely determined.
\end{itemize}

Further, show that the following are equivalent:
\begin{itemize}
\item Finite linear combinations of $H_B$ are dense in $H$: i.e. for every $h \in H$ and every $\eps > 0$ we can find $h_1, \dots, h_m \in H_B$ and real numbers $c_1, \dots c_n$ such that $\|h - \sum_{i = 1}^n c_i h_i\| < \eps$.
\item Each $h \in H_B$ admits a unique expansion $h = \sum_{i \geq 1} c_i h_i$ w.r.t. the norm $\| \cdot \|$ as above, with $c_i = \langle h, h_i \rangle$.
\end{itemize}
\end{exo}

\subsection{Fubini's theorem}

To finish off the chapter on integration, we still have to see one key result which is the integration analogue to the following result on infinite double sums (recall also Exercise 8 on sheet 5):

\begin{prop}
Let $(a_{i,j})_{i, j \geq 1}$ be an array of real numbers.
If $\sum_{i = 1}^n\sum_{j=1}^n |a_{i,j}|$ converges as $n \to \infty$, then for all $i \geq 1$, $\sum_{j=1}^n |a_{i,j}|$ converges and for all $j \geq 1$, $\sum_{i=1}^n |a_{i,j}|$ converges.

Moreover, then 
$$\lim_{n \to \infty} \sum_{i = 1}^n\sum_{j=1}^n a_{i,j} = \lim_{n \to \infty} \sum_{j \geq 1} a_{n, j} = \lim_{n \to \infty} \sum_{i \geq 1} a_{i,n}.$$

\end{prop}

The result for integrals that we state properly in a bit is very similar - we assume that $f: E_1 \times E_2 \to \R$ is integrable, and we want to say that for all fixed $x_1 \in E_1$, the function $f(x_1, y)$ is integrable over $E_2$; and that moreover the function $x_1 \to \int_{E_2} f(x_1, y) d\lambda(y)$ is then integrable over $E_1$.  

Now there are some extra aspects:
\begin{enumerate}
    \item First, why is the function $f(x_1, y): E_2 \to \R$ for every fixed $x_1$ measurable?
    \item Second, why is the integral for every $x_1$ well-defined?
    \item Third, why is the function $x_1 \to \int_{E_2} f(x_1, y) d\lambda(y)$ measurable, integrable?
\end{enumerate} 
All these aspects require some thought, although it comes out that in our setting where we work with Borel measurable functions thefirst one is simple to verify (on the sheet). Both the second and the third property do require thought: indeed, in the case of sums the finiteness of the double sum clearly implies the those of all individual sums. However, in the case of integrals this is actually not the case!

\begin{thm}[Fubini]
Let $E_1, E_2$ be Borel sets of non-zero measure and suppose that $f: E_1 \times E_2 \to \R$ is Lebesgue integrable. Then
\begin{itemize}
\item for almost every $x_1 \in E_1$, the function $f(x_1, y)$ is integrable over $E_2$;
\item the function $F(x_1) := \int_{E_2} f(x_1, y) d\lambda(y)$ if the integral is finite, and $F(x_1) = 0$ otherwise is measurable and integrable over $E_1$;
\item $\int_{E_1 \times E_2} f(x, y) d\lambda(x,y) = \int_{E_1} F(x_1) d\lambda(x_1) = \int_{E_1} (\int_{E_2} f(x,y)d\lambda(y))d\lambda(x)$;
\item and the same holds if we swap the order of integration.
\end{itemize}
Conversely, if either for almost every $x_1$ the function $|f(x_1, y)|$ is integrable over $E_2$ and further the function $F(x_1) := \int_{E_2} |f(x_1,y)|d\lambda(y)$ is integrable over $E_2$ or the same holds when $x_1$ and $x_2$ are swapped, then $f(x,y)$ is integrable over $E_1 \times E_2$.
\end{thm}

We will not prove this theorem, but we will see several applications very soon. For now let us mention that it can actually even just help to do some calculations:

\begin{exo}
The aim is to calculate $\displaystyle I = \int_{(0,\infty)}\exp(-x)\frac{\sin^2(x)}{x}d\lambda(x)$. To do this, we define $f(x,y) = \exp(-x)\sin (2xy)$ and use Fubini:
\begin{itemize}
\item Show that $f(x,y)$ is integrable over $(0, \infty) \times [0,1]$
\item Show that when first integrating $y$ over $[0,1]$ we obtain exactly $I$.
\item On the other hand, calculate explicitly the integral by first integrating over $x$. Integration by parts might be useful.
\end{itemize}
\end{exo}


%\begin{eg}[Convolutions]
%The convolution 
%We want to evaluate $I = \int_{\R} \exp(-x^2/2) d\lambda(x)$. First observe that by Fubini $I^2 = \int_{\R \times \R}\exp(-x^2/2 - y^2/2) d\lambda(x,y)$. 
%\end{eg}
% exo how interchanging limits is the same, but function not integrable on E_1 times E_2

\section{Linear operators}

In this section we will look at different linear operators defined on the spaces of functions. Here are some main examples:

\begin{itemize}
\item The differentiation operator $f \to \frac{d}{dx}f$ 
\item The Laplace operator $f \to \Delta f$ 
\item The integration operator $f \to \int_{-\infty}^xf d\lambda$
\item The Fourier transform $f \to \F(f)$
\end{itemize}

We will ask questions like on which functions they are defined and how can we best study them. Some of the main applications are the study of linear ODEs and PDEs, for example the study of

\begin{itemize}
\item The wave equation $\frac{\partial^2 f}{\partial t} = c^2 \Delta f$
\item The heat equation $\frac{\partial f}{\partial t} = K\Delta f$
\item The Sturm-Liouville problem: $\frac{d}{dx}[p(x) f'(x)] + q(x) f(x) = - \lambda w(x) f(x)$.
\end{itemize}

We are just dealing with the linear theory, as the non-linear case is much more tricky!



\subsection{Fourier transform and Fourier series}
Let's start off by revisiting the hastily defined Fourier transform and Fourier series, we will consider them on $\R$ and $[-1/2,1/2]$ respectively and use slightly different conventions than before to make it more clear that they are both very similar and yet different at the same time. 

As a general remark, one should be wary that different conventions are used throughout the literature (different signs in the exponent, different numerical constants in the exponent and in front of the integration) - it makes no difference to the content as long as one doesn't try to use formulas obtained for one convention when using a different convention.

\begin{itemize}
\item One can define the Fourier transform $\F (f) (k): \R \to \R$ for any real or complex-valued function $f \in L^1(\R)$ by 
$$\hat f(k) := \F(f)(k) := \int_\R f(x) \exp(-i 2\pi k x)d\lambda(x).$$
\item One can define the Fourier series $\F_s (f)(n): \Z \to \R$ for any real or complex valued function $f \in L^1([-1/2, 1/2])$ by
$$\hat f(n) := \F(f)(n) := \int_{[-1/2, 1/2]} f(x) \exp(-i 2\pi n x)d\lambda(x).$$
\end{itemize}

Both of these transformations offer a way to write the initial function as a superposition of waves, also both of the transformations are linear: $\F(f + g) = \F f + \F g$ by the linearity of integration.
Fourier series and Fourier transform were introduced by J. Fourier, a faithful supporter of Napoleon Bonaparte in the beginning of 19th century to study the heat flow and in particular to find solutions for the heat equation by decomposing solutions into waves. Before discussing which functions can be even decomposed, let us for the moment assume a decomposition of  $g: [-1/2, 1/2] \to \R$ or $f: \R \to \R$ respectively as
$$g(x) = \sum_{n \in \Z} \hat g(n) \exp(i 2\pi n x)$$
and
$$f(x) = \int_{\R} \hat f(k) \exp(i 2\pi k x) d\lambda(k).$$
We have already seen that for $g \in L^2([-1/2, 1/2])$ the decomposition relies on $(\exp(i 2\pi nx)_{n \in \Z}$ being an orthonormal basis, whereas the latter writing is called the Fourier inverse transform and is a non-trivial fact.


These expansions can be used to solve or at the very least guess solutions the heat equation $\frac{\partial u(t,x)}{\partial t} = D \nabla u(t,x)$ with initial condition $u_0(x) = u(0,x)$. For example on the interval $[-1/2, 1/2]$, if Fourier expansion holds, we can for any fixed time $t \geq 0$ write
$$u(t,x) = \sum_{n \in \Z} \hat u_t(n) \exp(i2\pi n x)$$
and in such a writing, when we allow ourselves to change the order of summation and differentiation, the heat equation becomes
$$\sum_{n \in \Z} \frac{d}{dt} \hat u_t(n) \exp(i 2\pi n x) = \sum_{n \in \Z} - D4\pi^2 n^2 \hat u_t(n)\exp(i 2\pi n x).$$
Notice that here on the RHS something interesting happened: we actually explicitly calculated that $\hat u'(k) = 2 \pi i n \hat u(k)$.

Thus to find a solution to the heat equation it suffices to find solutions to each of the equations $\hat u_t(n)' = - D4\pi^2 n^2 \hat u_t(n)$, which are given by just $\hat u_0(n)\exp(-D4\pi n^2t)$. In conclusion, one can propose a solution of the form
$$u(t, x) = \sum_{n \in \Z} \exp(- D4 \pi n^2 t) \hat u_0(k) \exp(i2\pi n x),$$
which at least formally satisfies the heat equation and the initial condition at time $t = 0$. To show that this is really a solution we could proceed in (at least) two ways:
\begin{itemize}
\item either make sure that all our steps were rigorous, i.e. that everything was always well defined and we could change the sums and differentiation
\item Show that the final formula gives a well-defined mathematical function that satisfies the heat equation. (This will be on the exercise sheet)
\end{itemize}

A similar argument can be made for the heat equation on the whole real line, and works also in the case of the wave equation.

However, in this sketch there are several delicate questions. For example:

\begin{itemize}
\item For which functions can we define the Fourier series and is it unique?
\item In which sense does the Fourier series present the initial function / the function $u_t$?
\item What about boundary conditions for $u_t(-1/2)$ and $u_t(1/2)$?
\end{itemize}

Maybe a bit surprisingly it comes out that some answers can be a bit subtle: for example whereas for any continuous function $f$ on $[-1/2, 1/2]$ with $f(-1/2) = f(1/2)$, we can write down its Fourier series, but it does not necessarily converge uniformly nor even at every point; yet it does converge almost everywhere. For a function in $L^1([-1/2, 1/2])$ things are even worse - Fourier coefficients are well defined, but the series expansion might diverge everywhere! Only in the case of $L^2([-1/2, 1/2])$ things work very nicely, as we will shortly see. 

\subsubsection{Which functions can we represent using Fourier series and in which sense?}

The case of square-integrable functions on an interval the Fourier series, when properly normalized, just gives one natural complete orthonormal basis. We haven't proved the completeness yet, but assuming this both the convergence and uniqueness of the Fourier series follow:

\begin{thm}[Fourier series for square-integrable functions]\label{thm:fseries}
Each $f \in L^2([-1/2,1/2], \CB)$ has a unique expansion using its Fourier series
$f(x) = \sum_{n \in \Z} \hat f(n) \exp(i2\pi n x),$
where the convergence of $S_N(f) := \sum_{|n| \leq N} \hat f(n) \exp(i2\pi n x)$ as $N \to \infty$ is w.r.t. $\| \cdot \|_2$ and 
$$\hat f(n) := \int_{[-1/2,1/2]}f(x)\exp(-2\pi i x n)d\lambda(x)$$
as before.

Moreover the map $[f] \to \hat f$ is a Hilbert space isomorphism between $\Lc^2([-1/2,1/2])$ and $l^2(\CB)$, and in particular the Plancherel identity holds $\|f\|_2^2 = \sum_{n \in \Z} |\hat f(n)|^2$.
\end{thm}
There is an analogous statement in the case of real-valued functions. 

We emphasise that here the convergence is just w.r.t the $L^2$ norm and recall in general the convergence of some functions $f_n \to f$ in $L^2$ does not imply pointwise convergence, nor even pointwise convergence almost everywhere! If one thinks of Fourier series of just as some orthonormal basis of $\Lc^2$, then a priori there is no reason that the Fourier series should also converge point-wise. And indeed this is a very subtle point!

It comes out that
\begin{itemize}
\item For square integrable functions the Fourier series does also converge almost surely (see Carleson theorem below). In particular this means that the approximation of a function $f$ using Fourier series $S_N(f)$ is still somehow better behaved than an arbitrary approximation.
\item However, Kolmogorv showed that there are integrable functions, on $[-1/2, 1/2]$, i.e. $f \in L^1([-1/2, 1/2])$, such that their Fourier series does not converge in $L^1$ and in fact diverges at every point! 
\end{itemize}

This shows how much care one needs to make general statements with Fourier series. Still, Fourier series is a very special ON basis in $L^2$ because of its relation to derivatives and smoothness of functions. For example it represents a bit smoother functions works extremely well, like this simple result illustrates:
%Notice that the first point already makes the Fourier series special, as for usual ON basis convergence in $L^2$ would only mean almost sure convergence along a subsequence. The fact that Fourier series is special comes even better out in its relation to  smoothness of functions:

\begin{thm}\label{thm:FC2}
Let $f \in C^2([-1/2,1/2])$ be twice continuously differentiable. Then its Fourier series
$$f(x) = \lim_{N \to \infty} \sum_{|n| \leq N} \hat f(n) \exp(i2\pi n x),$$
converges w.r.t. $\| \cdot \|_\infty$ and for each $x$ absolutely.
\end{thm}

The key ingredient is the following result, which we observed when guessing the solution to the heat equation and that really explains why Fourier series are so useful:
\begin{lemma}\label{lemma:diff}
Suppose that $f \in C^k([-1/2,1/2])$ is $k$ times continuously differentiable. Then for all $j = 1 \dots k$
$\F (\frac{d^j f}{dx^j}) (n) = (2\pi i)^jn^j \F(f)(n)$.

The same is true in the case of Fourier transform for $f \in C^k(\R) \cap L^1(\R)$.
\end{lemma}

\begin{proof}
On the exercise sheet
\end{proof}
Indeed, the theorem then follows: 

\begin{proof}[Proof of Theorem \ref{thm:FC2}]

By assumption $f''$ is continuous on $(-1/2, 1/2)$ and hence integrable. Thus 
$$|\F (f'')(k)| = |\int_{[-1/2, 1/2]}f''(x) \exp(2\pi i kx)d\lambda(x)| \leq \int_{[-1/2, 1/2]}|f''(x)|d\lambda(x) =: C.$$
Thus by the lemma we have that $|\hat f(n)| \leq Cn^{-2}$. 

To see that the Fourier series is uniformly convergent we now notice that for all $M > N$:
$$\|S_M(f) - S_N(f)\|_\infty \leq \sum_{N < |n| \leq M} |\hat f(n)| \leq N^{-1}.$$
The absolute convergence at any fixed $x \in [-1/2, 1/2]$ follows similarly. 
\end{proof}

This result holds actually also with less regularity, e.g. for Lipschitz functions, just the proof needs more care.

For functions with even less regularity things get quite a bit more subtle in regards to the pointwise convergence. Firstly, the positive part is that pointwise convergence for square-integrable functions does hold almost everywhere (recall that for just integrable functions it is not true at all!):

\begin{thm}[Carleson]
Let $f \in L^2([-1/2, 1/2])$ (i.e. in particular it could be a continuous function). Then its Fourier series converges almost everywhere.
\end{thm}
On the other hand this convergence cannot be improved to all points even for continuous functions: 
\begin{thm}[Katznelson]
 There exist continuous functions $f \in C([-1/2,1/2])$ with $f(-1/2) = f(1/2)$ such that  the set $\{x: S_N(f)(x) \text{ does not converge to } f(x)\}$ is uncountable. 
\end{thm}

However, despite this mess, in fact one can mathematically still give sense to the Fourier series of much less regular objects. For example, s you have seen in other courses even the Dirac delta function, that is not even a function, just a measure, has such a representation. We will come back to this after looking a bit at Fourier transforms.

\subsubsection{Which functions can we represent using Fourier transform?}

The case of square-integrable functions on $\R$ is a bit trickier - indeed, on $[-1/2, 1/2]$ each square-integrable function is also integrable, i.e. in $L^1([-1/2, 1/2])$ and thus its Fourier series is well-defined because $f(x)\exp(-2\pi i k x)$ is integrable for any $k$. But as we saw on the example sheet, there are functions in $L^2(\R)$ that are not integrable - for example the function $f(x) = (|x|+1)^{-1}$ - and hence there is no reason that $f(x)\exp(-2\pi i kx)$ should be.

Moreover, notice that in contrast to the case of the interval, none of the wave functions $\exp(i2\pi x k)$ itself is square-integrable on $\R$ (or integrable for that sake!) and moreover, we have now an uncountable collection of the wave functions. So we cannot argue exactly like in the case of the interval.

Still, things work again nicely for $\Lc^2$ spaces, explaining why they are so important:

\begin{thm}[Fourier transform on square-integrable functions]\label{thm:FTl2}
The Fourier transform 
$$\F(f)(k) := \hat f(k) := \int_{\R} f(x)\exp(-2\pi i kx)d\lambda(x)$$ 
defined on $\Lc^2(\R) \cap \Lc^1(\R)$ has a unique extensions to an operator $\tilde \F$ on the whole of $\Lc^2(\R)$ such that $\tilde \F: \Lc^2(\R) \to \Lc^2(\R)$ is an Hilbert space isomorphism.

Moreover, the inverse operator is given similarly by an extension of 
$$\F^{-1}(f)(x) := \int_{\R} \hat f(k) \exp(2\pi i kx) d\lambda(k)$$
from the subset $\hat f \in \Lc^2(\R) \cap \Lc^1(\R)$.

We will often abuse the notation and denote the-above mentioned extensions also by $\F$ and $\F^{-1}$. 
\end{thm}

Already the statement of this result looks a bit unpleasant with this extension and so! But this is necessary as for a general function $f \in \Lc^2(\R)$ the integral just doesn't make directly sense.

There are several ways to prove this theorem, We will discuss a way that goes via approximations from larger and larger intervals and thus also helps to connect the Fourier transform back to the Fourier series. 

In fact we will start checking that everything works well for even a nicer subset of functions: we consider $f \in \Lc^2(\R) \cap \Lc^1(\R)$ that are twice differentiable and such that $f', f''$ are both integrable too.

\begin{prop}
Let $f \in \Lc^2(\R) \cap \Lc^1(\R)$ be twice continuously differentiable, such that $f', f''$ are both integrable over $\R$. Then we have the following Fourier inverse transform: for almost every $x \in \R$ we can write
\begin{equation}\label{eq:ft}
f(x) := \int_{\R} \hat f(k) \exp(2\pi i xk) d\lambda(k).
\end{equation}

Moreover, we have the Plancherel formula:
$$\|f\|_2^2 := \int_{\R} |f(x)|^2 d\lambda(x) = \int_{\R} |f(k)|^2 d\lambda(k) =: \|\hat f\|_2^2.$$
\end{prop}

First, there is an extension of Lemma \ref{lemma:diff} to the case of Fourier transform, and thanks to our assumptions we can hence conclude that $|\hat f(k)| = O(k^{-2})$ and thus $\hat f(k)$ is in particular integrable.

Now observe that the theorems about Fourier series on $[-1/2,1/2]$ can be transported to Fourier series on any $[-L/2, L/2]$ by simple scaling: the (unnormalised) Fourier basis will now be $(\exp(2 \pi i L^{-1} nx))_{n \in \Z}$.

One can verify (Exercise sheet) via scaling that Theorem \ref{thm:fseries} and Theorem \ref{thm:FC2} then allow to write any $f_L \in L^2([-L/2, L/2])$
as 
\begin{equation}\label{eq:ft1}
f_L(x) = L^{-1}\sum_{n \in \Z}\hat f_L(n/L)\exp(2\pi i L^{-1}nx),
\end{equation}
where the summing is absolute for any $x \in [-L/2, L/2]$ and in the sense $L^2$, and 
\begin{equation}\label{eq:ft2}
\hat f_L(n/L) := \int_{[-L/2, L/2]}f_L(x)\exp(-2\pi i L^{-1}nx) d\lambda(x).
\end{equation}
%%% Exercise on scaling
This Fourier series can be extended to a function on $\R$ by setting $\hat f_L(x) := f_L(L^{-1}\lfloor x L \rfloor)$.
We will use these observations to prove the proposition.

\begin{proof}
Consider $f$ as in the statement and let $f_L := f 1_{[-L/2, L/2]}$. 

As $|f_L(x) \exp(-2\pi L^{-1}nx)| \leq |f(x)|$ we can first use the Dominated convergence theorem to show that for every $k \in \R$ we have that
$\hat f_L(k) \to \hat f(k)$ as $L \to \infty$, and thus the Fourier coefficient on $\R$ corresponds to the approximated Fourier coefficients. 

We will now check that under our conditions \eqref{eq:ft1} becomes in the limit $L \to \infty$ the inverse Fourier transform \eqref{eq:ft}.  We would like to write the RHS of \eqref{eq:ft1}  as the Lebesgue integral of the $\hat f_L$ extended to $\R$, i.e. 
$$L^{-1}\sum_{n \in \Z}\hat f_L(n/L)\exp(2\pi i L^{-1}nx) = \int_{\R} \hat f_L(k) \exp(2\pi iL^{-1}\lfloor k L \rfloor x)d\lambda(k).$$
But to do so, we need to check the integrability of the function $\hat f_L(k)$:
\begin{itemize}
    \item In a bounded region say $[-1,1]$ we can bound it by $|\hat f(k)| + C$ by an explicit caclulation,
    \item and further under our conditions $|\hat f_L(k)| = O(k^{-2})$  as $|k| \to \infty$,
\end{itemize}
and thus $\hat f$ is integrable.

Now again $\hat f_L(k) \exp(2\pi iL^{-1}\lfloor k L \rfloor x) \to \hat f(k) \exp( 2\pi i k x)$ pointwise, and we can again use the Dominated convergence theorem to conclude that 
$$\int_{\R} \hat f_L(k) \exp(2\pi iL^{-1}\lfloor k L \rfloor x) d\lambda(k) \to \int_{\R} \hat f(k) \exp(2 \pi i k x) d\lambda(k).$$
Finally, the Plancherel identity for $f_L$ implies that
$$\|f_L\|_2^2 = L^{-1}\sum_{n \in \Z}|\hat f_L(n/L)|^2.$$
The LHS converges to $\|f\|_2^2$ by Monotone convergence theorem. For the RHS we have to use the dominated convergence theorem as above to obtain the convergence to $\|\hat f\|_2^2$
\end{proof}

To finish the proof of Theorem \ref{thm:FTl2} it remains to do two things:
\begin{enumerate}
\item Check that the set of functions $f \in \Lc^1(\R) \cap \Lc^2(\R)$ that are twice continuously differentiable such that $f', f''$ are integrable, is dense in $\Lc^2(\R)$ w.r.t. the $\|\cdot \|_2$ norm
\item Find an argument to extend the Fourier transform using this density.
\end{enumerate}
The first argument will be covered on the exercise sheet: the idea is to use convolutions $f \star \psi_\delta$, with smooth functions $\psi_\delta$ such that $f \star \psi_\delta$ is smooth for each $\delta$, but converges to $f$ as $\delta \to 0$. Such approximations are called approximations to identiy and we will discuss them in a moment. 

The second argument uses strongly the completeness of $\Lc^2(\R)$ and gives a strong motivation both for introducing the Lebesgue integration - it is not difficult, but we will skip it for now.

\subsubsection{The Dirac delta function and its Fourier transform}

Most of these section is non-examinable, in the sense that I will not expect that you are able to rigorously work with the Dirac delta function - we don't have the time to develop the mathematics behind it. Still, there will be some exercises on the sheet, which you should look at even if you are not interested in the discussion to follow and maybe this discussion will explain you how you can work with the Dirac delta function in a mathematical way.

We have seen that giving sense to the Fourier transform can get rather tricky if we don't have functions that are sufficiently smooth. And still you have surely seen the following formula for the Dirac delta function that is not even a function - just 'something' that is zero outside of the origin and 'infinite' at the origin: $\delta_0(x) = \int_{\R} \exp(2\pi i k x)d\lambda(k)$.

If we use this, then the physics 'proof' of the inverse Fourier transform becomes rather simple: 
\begin{itemize}
    \item we just ignore the issues of integrability, and define $\F(f)$ for all functions in $L^2$ (and more);
    \item to obtain the inverse transform, we can write out the definition of $\hat f$ to obtain
$$\int_{\R} \hat f(k) \exp(2\pi i kx) d\lambda(k) =\int_{\R} \left(\int_{\R} f(y)\exp(-2\pi iyk) d\lambda(y)\right)\exp(2\pi ixk) d\lambda(k).$$
Using now Fubini to change the order of integration we obtain
$$\int_{\R} f(y)\left(\int_{\R} \exp(2\pi i(x-y)k) d\lambda(k)\right) d\lambda(y).$$
But now you recognize in 
$$\int_{\R} \exp(2\pi i(x-y)k) d\lambda(k)$$
the inverse Fourier transform of $\delta(x-y)$. Plugging this in we obtain $\int_{\R} f(y) \delta(x-y) d\lambda(y) = f(x)$.
\end{itemize}
As such, this is not a mathematically rigorous argument, but in this very case the heuristic can be backed up by arguments via approximations - so philosophically it is almost correct. What gets lost is some level of detail, which as we discuss later might even have a physical interpretation: indeed, by this argument both $f(x)$ and $\hat f(k)$ are well-defined for each $k$ and $x$, but in fact this can really not be the case when we only know that $f \in \Lc^{2}(\R)$. Also this would hint that the inverse is always true, but we saw it might not at all be true already in the case of series (for which one could give a similar proof). So we somehow lose track of the character of our functions / objects.

Now, what is this Delta function, then and when can we use such an expansion? The delta function cannot be defined as a function in any of the mentioned function spaces, as it somehow doesn't takes a finite value at the point $0$ and is characterised just by the fact that you can 'integrate continuous functions $f$ w.r.t it' obtaining $f(0)$ as an answer. 

But as mentioned already before, in $d$ dimensions it can be mathematically defined for example as a measure on $(\R^d, \F_B)$ that puts unit mass at $0$. In fact, as such one can build up the whole integration theory as we did for the Lebesgue measaure and in particular this allows to give sense to the integral $\int_{\R^d} f d\delta_0$ for all continuous functions $f$, and its value would be exactly $f(0)$ as expected. This is a way to give a mathematical meaning of 'integrating against continuous functions'. 

To work with the Dirac delta function with no fear, the most convenient way is to use approximations: 

\begin{defn}[Approximation of identity]
Let $(\psi_\eps)_{\eps > 0}$ be a family of smooth real-valued functions  on $\R^d$ such that
\begin{itemize}
\item $\psi_\eps$ is non-negative and positive only in the ball $B(0, \eps)$.
\item $\int_{\R^d} \psi_\eps = 1.$
\end{itemize}
We call such a family an approximation of identity.
\end{defn}

\begin{rem}
One way to construct such an approximation is to start with a smooth function $\psi$ that is non-negative and supported only in the unit ball and then define $\psi_\eps(x) := \eps^{-d}\psi(x/\eps)$.
\end{rem}

The idea is now that the Dirac delta function can be seen as the limit $\eps \to 0$ of such approximations of identity. It is of course not clear in which sense we can take the limit, but for example there is a notion of limit for measures:

\begin{defn}[Convergence in measure]
Let $(\mu_n)_{n \geq 1}$  and $\mu$ be finite measures defined on $(\R^d, \B)$. We say that $\mu_n$ converges to $\mu$ in measure if for every $f \in C(\R^d, \R)$ we have that $\int_{\R^d} f d\mu_n \to \int_{\R} f d\mu$. 
\end{defn}

One can now show that 
\begin{lemma}
Any approximations of identity $(\psi_\eps)_{\eps > 0}$ converge to $\delta_0$ as $\eps \to 0$ in the sense that for all continous $f: \R^d \to \R$ we have that $\int_{\R^d} f \psi_\eps d\lambda$ converges to $f(0) = \int_{\R^d} f d\delta_0$.
\end{lemma}

The whole philosophy is now that whenever you need to work with the Dirac delta function you just use the approximations and make sure that everything converges well.

For example, the Fourier transform of the Delta function at every fixed frequency $k$ is now easy to explain: we can just take an approximation to identity $(\psi_\eps)_{\eps > 0}$ and check that as $\eps \to 0$
$$\hat \psi_\eps(k) = \int_\R \psi_\eps(k) \exp(-2\pi i k x)dx \to 1.$$
However, real care is needed when we want to plug in the Fourier expansion $\delta_0(x) = \int_{\R} \exp(2\pi ik x) d\lambda(k)$, as clearly the integral is not well defined. 

Let us illustrate this via an example when we calculate $\int_{\R} 1_{(0,1)}d\delta_0$ and $\int_{\R} 1_{[0,1)}d\delta_0$ using this expansion. We know that the answers are $0$ and $1$ respectively, but if we naively plug in the expansion and use Fubini we get $$\int_{\R^2} 1_E \exp(i2\pi i k x) d\lambda(k,x)$$
with $E$ either equal to $(0,1)$ or $[0,1)$ respectively, which seems to make no difference at all for the Lebesgue integral as it doesn't see a single point! 

This doesn't mean that should not use the Fourier transform of the delta function, but that care is needed - you can really think of it as giving some non-absolutely convergent oscillating summation to represent the delta function and then one needs to be super cautious when starting to manipulate it.

\subsection{Compact operators and their spectrum}

We will now look a bit more closely at one subclass of linear operators called compact operators on Hilbert space - in our case you can keep in mind $\Lc^2([0,1])$ and $\Lc^2(\R)$ - and their diagonalisation. We will assume throughout that the Hilbert space is separable. We will start from a more general category of linear operators:

\begin{defn}[Bounded linear operator]
Let $(\Hh, \| \cdot \|)$ be a separable Hilbert space. A linear map $T : \Hh \to \Hh$, i.e. a map that satisfies $T(af + bg) = aT(f) + bT(g)$ for all $a, b \in \CB$ (or in $\R$) and $f, g \in \Hh$ is called a bounded linear operator if $\sup_{\|f\| \leq 1} \| Tf\| < \infty$. We define this supremum itself as the norm of the operator:
$\|T\|_{op} := \sup_{\|f\| \leq 1}\|Tf\|.$
\end{defn}
Some examples of bounded linear operators:
\begin{eg}[Bounded linear operators]~
\begin{itemize}
\item The identity operator $Id: \H \to \H$ defined by $Id(f) := f$. It has norm equal to $1$
\item Finite rank operators: pick $f_1, \dots, f_n \in \Hh$, $g_1, \dots, g_n \in \Hh$ and define $T(f) := \sum_{i = 1}^n  \langle f, f_i \rangle g_i.$
\item Fourier transform on $\Lc^2(\R)$ is bounded and has unit norm as we have that $\|\F (f)\|_2 = \|f \|_2$.
\item Position operator on $\Lc^2([a,b]$ defined by $T(f) := x f$ is bounded: indeed 
$$\|T(f)\|_2^2 = \int_{[a,b]}|x|^2|f(x)|^2d\lambda(x) \leq \max(|a|, |b|)^2 \|f\|_2^2.$$
\end{itemize}
\end{eg}
However, not all natural linear operators are bounded. Here are some examples (exercise sheet):
\begin{exo}
Show that the following operators are not bounded:
\begin{itemize}
\item The position operator on $\Lc^2(\R)$ defined by $T(f) := x f$ is not defined on the whole of $\Lc^2(\R)$ and is not bounded;
\item The momentum operator on $\Lc^2(\R)$
defined by $T(f) := -i \frac{df}{dx}$ is neither defined on the whole of $\Lc^2(\R)$ and is not bounded.
\end{itemize}
\end{exo}
The mathematical theory of unbounded operators is more subtle, mainly because the operators are often not defined on the whole space and thus it is not trivial to take compositions of operators or sums of them, nor adjoints which necessarily enter when one aims to diagonalise the operator. Often then one needs to extend the operators to a larger subspace, but this is not direct. It is rather direct in the case of bounded operators, and we already made use of it to define the Fourier transform:

\begin{thm}[Extension of bounded operators]
Let $T$ be a bounded operator on a subspace $D \subseteq \Hh$ of a Hilbert space $\Hh$. Then there is a extension of the operator $T$ to an operator $\tilde T: \Hh \to \Hh$ such that $\tilde T = T$ when restricted to $D$ and $\| \tilde T \|_{op} = \|T\|_{op}$.

Moreover, if $D$ is dense, this extension is unique. 
\end{thm}

We are not interested in proving this abstract theorem here, but as mentioned this somehow marks the difference between bounded and unbounded operators.

We will instead work with an even smaller class of operators called compact operators.

\begin{defn}[Compact operator]
Let $(\Hh, \| \cdot \|)$ be a Hilbert space and $T$ a bounded linear operator. We call $T$ compact if for every sequence $(f_n)_{n \geq 1}$ with $\|f_n\| \leq 1$ there is a subsequence $(f_{n_k})_{k \geq 1}$ such that $(T(f_{n_k}))_{k \geq 1}$ converges.
\end{defn}

\begin{rem}
Another equivalent way to define compactness is just to say that the image of the unit ball under $T$ has to be compact. But often the criteria via sequences is what one checks.
\end{rem}

Not all the operators that one would hope or expect to be compact are compact:

\begin{eg}[Non-compact and compact operators]~
\begin{itemize}
    \item The identity operator is compact if and only if the Hilbert space $(\Hh, \|\cdot \|)$ is finite dimensional (and hence isomorphic to $(\R^n, \|\cdot \|_2)$. Indeed, it is an application of Bolzano Weierstrass to see it is compact in case it the dimension is finite. In the opposite case consider an orthonormal basis $(\phi_n)_{n \geq 1}$. But $Id(\phi_n) = \phi_n$ and $\|\phi_n - \phi_m\|_2 = \sqrt{2}$ for all $m \neq n$. Hence $(Id(\phi_n))_{n \geq 1}$ has no subsequence that is Cauchy and hence no subsequence that is convergent.
    \item Finite rank operators are compact, again it is a matter of Bolzano Weierstrass as you can verify on the example sheet.
    \item Some further examples like the Fourier operator and the position operator on $\Lc^2([0,1])$ are on the example sheet.
\end{itemize}
\end{eg}

There is a nice characterisation of compact operators:

\begin{prop}
Let $(\Hh, \| \cdot \|)$ be a Hilbert space. Then a bounded operator $T: \Hh \to \Hh$ is compact if and only if it is a limit of finite-rank operators in the operator norm, i.e. there exist $(T_n)_{n \geq 1}$ of finite rank with $\|T_n - T\|_{op} \to 0$. 
\end{prop}
The key lemma is the following:

\begin{lemma}
Let $(T_n)_{n \geq 1}, T$ be bounded operators and suppose that $\|T_n - T\|_{op} \to 0$. If $(T_n)_{n \geq 1}$ are compact, then so is $T$.
\end{lemma}

\begin{proof}[Proof idea]
We need to prove that for any sequence $(f_n)_{n \geq 1}$ in $\Hh$ with $\|f_n \| \leq 1$ we can find a subsequence $(f_{n_k})_{k \geq 1}$ such that $T(f_{n_k})$ converges as $k \to \infty$.

To do this we use an iterative extraction procedure: 
\begin{enumerate}
\item As $T_1$ is compact, we can find a subsequence $(f_{n_{1,k}})_{k \geq 1}$ such that $(T_1(f_{n_{1,k}}))_{k \geq 1}$ converges.
\item As further $T_2$ is compact, we can extract a further subsequence $(f_{n_{2,k}})_{k \geq 1}$ from the previous one such that $(T_2(f_{n_{2,k}}))_{k \geq 1}$ also converges (notice that by definition the convergence holds for $T_1$.
\item we continue iteratively.
\end{enumerate}
The claim is then that if we consider the diagonal subsequence $(f_{n_{k,k}})_{k \geq 1}$ then $(T(f_{n_{k,k}}))_{k \geq 1}$ converges as $k \to \infty$. To do this, it suffices to verify that the sequence is Cauchy, which one can do by a $3\epsilon$ argument: we first take $n$ large enough such that $\|T_n - T\|_{op}$ is as small as we wish and then go far enough in the sequence such that for $\|T_n(f_{k,k}) - T_n(f_{l,l})\|$ is as small as we wish for all $k, l$ large enough.
\end{proof}

The key example of compact operators for us is that of Hilbert-Schmidt integral operators:

\begin{defn}[Hilbert-Schmidt integral operators]
Let $B(0,R) \subseteq \R^d$ be the ball\footnote{It could also be anything else reasonable like a box of $\R^d$ or similar} of radius $R > 0$. 

If $K(x,y): B(0,R) \times B(0,R) \to \R$ is square integrable, i.e. $\int_{B(0,R) \times B(0,R)}|K(x,y)|^2 d\lambda(x,y) < \infty$, the integral operator defined on $\Lc^2(B(0,R))$ and given by 
$$T_K(f) := \int_{B(0,R)} K(x,y)f(y)d\lambda(y)$$
is called the Hilbert-Schmidt integral operator.
\end{defn}

\begin{eg}
A known example to you is the case where $G(x,y)$ is the Green's kernel in $d = 1,2$, i.e. the symmetric kernel 
\begin{itemize}
    \item given on $[0,1]$ by $G(x,y) := x(1-y)$ for $0 \leq x \leq y \leq 1$ 
    \item and by $G(x,y) := - \log |x-y| + \log |1-x\bar y|$ on $B(0,1) \subseteq \R^2$.
\end{itemize}
Such integral kernels come up when solving more general Sturm-Liouville problems as we will shortly see.
\end{eg}
The key claim now is the following (more precise on the exercise sheet):

\begin{exo}
Hilbert-Schmidt integral operators as defined just above are compact.
\end{exo}

\subsubsection{Spectral theorem for compact operators}

We now arrive at one of the jewels of the course - the diagonalisation of a general compact operator. It's really very nice that one can do it in this (and actually in a much wider) generality.

First, we need to definite the notion of a Hermitian / symmetric operator:

\begin{defn}[Hermitian / symmetric operator]
Let $(\Hh, \| \cdot \|)$ be a Hilbert space and $T$ a bounded operator. We call $T$ Hermitian or symmetric if for all $f, g \in \Hh$ it holds that $\langle Tf, g \rangle = \langle f, Tg \rangle$.
\end{defn}

The spectral theorem now says that any Hermitian compact operator can be diagonalized:

\begin{thm}[Spectral theorem for compact operators]\label{thm:spectral}
Let $\Hh$ be a separable Hilbert space and $T: \Hh \to \Hh$ a compact Hermitian operator.

Then we can find an orthonormal basis $(\phi_n)_{n \geq 1}$ and real numbers $(\lambda_n)_{n \geq 1}$, called eigenfunctions and eigenvalues such that
\begin{itemize}
\item $T(f) = \sum_{n \geq 1}\langle f, \phi_n\rangle \lambda_n \phi_n$ and in particular $T(\phi_n) = \lambda_n \phi_n$ for all $n \geq 1$
\item For each $\eps > 0$, there are only finitely many eigenvalues $\lambda_i$ with $|\lambda_i| > \eps$.
\end{itemize}
\end{thm}

We call this the diagonalization of the operator, because the basis of eigenfunctions $(\phi_n)_{n \geq 1}$ allows us to represent the operator $T$ by a multiplication in each of the coordinates exactly like in the finite-dimensional case: if $f = \sum_{n \geq 1}c_n\phi_n$ then $T(f) = \sum_{n \geq 1} c_n \lambda_n \phi_n$.

The proof relies on the following lemmas:

\begin{lemma}\label{lem:st1}
Let $T$ be compact and Hermitian on a Hilbert space $\Hh$. Then either $\|T\|_{op}$ or $-\|T\|_{op}$ is an eigenvalue, i.e. there is some $f \in \Hh$ such that $Tf = \|T\|_{op} f$ or $Tf = -\|T\|_{op} f$.
\end{lemma}

\begin{lemma}\label{lem:st2}
Let $T$ be compact and Hermitian on a Hilbert space $\Hh$. Then all the eigenvalues $\lambda$ are real, and for each non-zero eigenvalue $\lambda$ the subspace of $f \in \Hh$ with $T f = \lambda f$ is finite-dimensional.
\end{lemma}

\begin{lemma}\label{lem:st3}
Let $T$ be compact and Hermitian on a Hilbert space $(\Hh, \| \cdot \|)$. Then any two eigenfunctions $f, g$ corresponding to different eigenvalues are orthogonal. Moreover, for every $\eps > 0$ there are only finitely many eigenvalues with $|\lambda_i| > \eps$.
\end{lemma}

Before proving them, let us see how to conclude the theorem. The proof of the theorem is not examinable as we haven't really talked about orthogonal subspaces and their complements (though everything is pretty much like in the finite-dimensional case). The proofs of some of the lemmas are examinable however.

\begin{proof}[Proof sketch of Theorem \ref{thm:spectral}]
Most of the statements in the theorem are given by the lemmas above. 
Indeed, by the first lemma the set of eigenvalues is non-empty, by the second lemma we can for each eigenvalue $\lambda$ find orthonormal vectors $\phi_{\lambda,1}, \dots \phi_{\lambda, d_\lambda}$ with $\Hh \phi_{\lambda,i} = \lambda \phi_{\lambda, i}$. Further by the third lemma we can choose all the eigenfunctions to be orthonormal.

It just suffices to prove that this set of orthonormal eigenfunctions $(\phi_n)_{n \geq 1}$ is complete: i.e. that every $f$ can be written in $\Hh$ as $f = \sum_{n \geq 1} \langle f, \phi_n \rangle \phi_n$.

Now one can verify that the linear span of all these eigenfunctions is a closed subspace of $\Hh$. Consider it's orthogonal complement $\Hh_R \subseteq \Hh$ that is itself a Hilbert space. Notice that for any $h \in \Hh_R$ we have that $Th \in \Hh_R$: indeed $(Th, \phi_n) = (h, T\phi_n) = (h, \lambda_n \phi_n) = 0$ for any eigenfunction $\phi_n$. If $\Hh_R$ is non-empty, then the restriction of $T$ to $\Hh_R$ is again a compact Hermitian operator. But by Lemma 1, it always has an eigenvalue and an eigenfunction, giving a contradiction with the fact that we already listed all of them. Thus $\Hh_R$ is empty and thus the set of orthonormal eigenfunctions indeed forms a basis.
\end{proof}

None of the lemmas is too tricky to prove and it is interesting to see how being Hermitian really forces things to be work out. 

\begin{proof}[Proof of Lemma \ref{lem:st1}]
We can assume by scaling that $\|T\|_{op} = 1$ and thus we need to show that either $1$ or $-1$ is an eigenvalue.

First we claim that there is some unit norm function $g$ with $\|Tg\| = 1$: indeed, by definition of $\|T\|_{op}$ there is some sequence of unit norm functions $g_n$ with $\|T g_n \| \to 1$. As $T$ is compact, $\|T g_n \|$ converges to some $g$. We claim that $\|T g\| = 1$: indeed, $T^2 g_n \to T g$ but 
$$\|T^2 g_n\| \|g_n\| \geq (T^2 g_n, g_n) = (T g_n, T g_n) = \| T g_n \|^2 \to 1$$
and hence as $\|g_n\| = 1$, we see that $\|T^2 g_n \| \to 1$, concluding that $\|Tg\| = 1$.

We will now show that either $Tg = g$ and thus $-1$ is an eigenvalue or $Tg + g$ is an eigenfunction with eigenvalue $1$. To do this write $Tg = cg + df$ with $f$ orthogonal to $g$ and $c, d \in \CB$ with $|c|^2 + |d|^2 = 1$.  
Then as 
$$1 = (Tg, Tg) = (T^2g, g) \leq \|T^2 g\| \|g\| = \|T^2 g\|,$$
we conclude that $T^2g = g$. But now we can conclude: either $Tg + g = 0$ or $T(Tg + g) = g + Tg$ as promised.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:st2}]
Let $(f, \lambda)$ be an eigenfunction, eiganvalue pair. Then 
$$ \lambda(f, f) = (Tf, f) = (f, Tf) = \bar \lambda (f,f),$$
and thus $\lambda$ is real.

Suppose for contradiction that $\lambda \neq 0$ but the subspace of functions $f$ with $Tf = \lambda f$ is not finite-dimensional. Then we can find unit norm orthogonal functions $f_1, f_2, \dots$ such that $Tf_n = \lambda f_n$ and in particular $\|Tf_n - Tf_m\| \geq \sqrt{2}|\lambda|$. But this is a contradiction as $T$ is compact and thus $T f_n$ should admit a convergent subsequence, in particular a subsequence that is Cauchy.  
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:st3}]
First, let $(f, \lambda_1), (g, \lambda_2)$ be two eigenfunction, eigenvalue pairs with $\lambda_1 \neq \lambda_2$. Like in the lemma above:
$\lambda_1 (f, g) = (Tf ,g) = (f, Tg) = \lambda_2 (f,g),$
which can only hold when $(f,g) = 0$, i.e. $f, g$ are orthogonal.

Now suppose there are infinitely many eigenvalues with $|\lambda| > \eps$. Then we can find orthonormal eigenfunctions $(\phi_n)_{n \geq 1}$ with eigenvalues $(\lambda_n)_{n \geq 1}$ with $|\lambda_n| > \eps$. But we can now replicate the proof of the lemma above to obtain a contradiction: as $T$ is compact the sequence $(T \phi_n)_{n \geq 1}$ should admit a convergent subsequence, yet $\|T \phi_n - T\phi_m\| \geq \sqrt{2}\eps$ thus there is no subsequence that is Cauchy. 
\end{proof}

We now look at a few corollaries. 
A first important corollary is the completeness of the Fourier basis $(\exp(2\pi i n x)_{n \in \Z})$ on $[0,1]$, as detailed on the example sheet.

As a second corollary let us bring out an abstract result about solving linear equations for compact Hermitian operators on Hilbert spaces. You should compare it to the case of system of linear equations:

\begin{cor}[Fredholm alternative]
Let $\Hh$ be a Hilbert space and $T$ compact and Hermitian. Let $(\phi_n, \lambda_n)_{n \geq 1}$ be the orthonormal sequence of eigenfunctions and eigenvalues.

Consider the equation $Tg - \lambda g = f$ for a fixed $f\in \Hh$, unknown $g \in \Hh$ and a non-zero $\lambda \in \R$. Then one of the following options holds:
\begin{itemize}
\item Either $\lambda$ is not an eigenvalue of $T$ and then there is a unique solution given by $$g = \sum_{n \geq 1}\frac{\langle f, \phi_n \rangle}{\lambda_n - \lambda}\phi_n ;$$
\item $\lambda$ is an eigenvalue of $T$ and then a solution exists if and only if $\langle f, \phi \rangle = 0$ for any eigenfunction $\phi$ of eigenvalue $\lambda$. Any solution is given by $$g = \sum_{n \geq 1}1_{\lambda_n \neq \lambda} \frac{\langle f, \phi_n \rangle}{\lambda_n - \lambda}\phi_n  + \phi,$$
where $\phi \in \Hh$ is any function that satisfies $T \phi = \lambda \phi$.
\end{itemize}

\end{cor}

It is interesting to think why the case $\lambda = 0$ has been left out. However, we will instead think what it can tells us interesting about differential equations.

This result was first invented for and is often applied to integral equations. Indeed, consider $T_K$ a Hilbert-Schmidt operator with kernel $K(x,y)$ on $\Lc^2([0,1])$. Then the Fredholm alternative would tell us when we can solve 
$$-\lambda g(x) + \int_{[0,1]}K(x,y) g(y) d(y)  = f(x)$$
for given $f \in \Lc^2([0,1])$.

\subsubsection{Sturm-Liouville equations}

We will finish the course by tying these somewhat abstract considerations to the very concrete question of Sturm-Liouville problems. We will consider here what are called regular Sturm-Liouville problems on an interval $[a,b]$. The aim is to find pairs $u: [a,b] \to \R$ and $\lambda \in \CB$ that solve on $[a,b]$ the ODE
\begin{equation*}\frac{d}{dx}\left(p(x)\frac{du}{dx}\right) + q(x) u(x) = -\lambda w(x) u(x),
\end{equation*}
(or written shorter $(pu')' + qu = \lambda w u$), together with certain boundary conditions at both endpoints. Notice that in the usual ODE theory we would work with a fixed $\lambda$ and we we wouldimpose boundary conditions only at one single endpoint.
%\begin{align*}
%&\alpha_a u(a) + \beta_au'(a) = 0 \\ 
%&\alpha_b u(b) + \beta_bu'(b) = 0,
%\end{align*}
%for some $\alpha_a, \beta_a, \alpha_b, \beta_b$ so that neiter both $\alpha_a, \beta_a$ nor both $\alpha_b, \beta_b$ are zero. Here $p(x), p'(x), q(x), w(x)$ are all assumed to be continuous on $[a,b]$ and $w, p$ further are positive throughout the interval.

The form of the equation encompasses quite a large class of ODEs - in fact, any 2nd order ODE of the form $a(x)u''(x) + b(x)u'(x) + c(x)u(x) + \lambda d(x)u(x) = 0$ with say positive $a, d$ can be brought into this form.

Such equations come up for example when finding series solutions to certain PDEs. For example, consider the inhomogeneous heat equation $\frac{\partial u}{\partial t} = k(x)\Delta u$ on the interval $[a,b]$ with heat flowing away at the boundary points, i.e. $u(a) = u(b) = 0$ and $k(x) > 0$. Then we can try to find solutions of the form $u(x,t) = v(x)w(t)$. Plugging this in we obtain $$v(x) \frac{d w(t)}{dt} = k(x)w(t)\frac{d^2 v(x)}{dx^2}.$$ and thus a solution if: $ \frac{d^2 v(x)}{dx^2}+ \lambda k^{-1}(x)v(x) = 0$ and $\frac{d w(t)}{dt} = -\lambda w(t)$. The first equation is a Sturm-Liouville equation, the second has a straightforward exponential solution. The general solution would then be given by a series expansion over all such specific solutions and in fact the main theorem would moreover prove that this gives us the most general form of the solution. This should all remind you very much like of the solution we found using Fourier series in the homogeneous case where $k(x) = const$ - it happens that Fourier series can be seen as a special case arising in the Sturm-Liouville theory.

Let us state a general (but not the most general) theorem and then discuss how to connect it  to the abstract theory developed before.

\begin{thm}[Sturm-Liouville problem]
Consider a regular Sturm-Liouville problem on $[a,b]$, i.e. finding pairs $u \in C^2([a,b], \CB)$ and $\lambda \in \CB$ such that 
\begin{equation}\label{eq:SL}\frac{d}{dx}\left(p(x)\frac{du}{dx}\right) + q(x) u(x) = -\lambda u(x),
\end{equation}
where the functions $p, p', q: [a,b] \to \R$ are continuous and further $p$ is positive throughout $[a,b]$, and that the boundary conditions
\begin{align}\label{eq:SLbnd1}
&\alpha_a u(a) + \beta_au'(a) = 0 \\ 
\label{eq:SLbnd2}
&\alpha_b u(b) + \beta_bu'(b) = 0,
\end{align}
hold for some $\alpha_a, \beta_a, \alpha_b, \beta_b$ so that neiter both $\alpha_a, \beta_a$ nor both $\alpha_b, \beta_b$ are zero. 

Then there is countable sequence of real eigenvalues $\lambda_1 < \lambda_2 < \dots $ and corresponding solutions $u_{\lambda_i}$ such that $(\lambda_i, u_{\lambda_i})_{i \geq 1}$ are the only possible pairs of solutions and moreover $(u_{\lambda_i})_{i \geq 1}$ form an orthonormal basis of $L^2([a,b])$.
\end{thm}

First there are some consequences from the general theory of ODEs:
\begin{itemize}
    \item One can always find a unique solution for Equation \eqref{eq:SL} with boundary conditions only at one endpoint of the interval - i.e. either with $u(a) = u_0(a)$, $u'(a) = u_{00}(a)$ or with $u(b) = u_0(b)$, $u'(b) = u_{00}(b)$. In particular we can for any $\lambda \in \CB$ find solutions $u_1(x)$, $u_2(x)$ that satisfy respectively either boundary conditions \eqref{eq:SLbnd1} or \eqref{eq:SLbnd2}. 
    \item Moreover for any $\lambda \in \CB$ there is at most one solution up to multiplication: indeed, if there were two solutions, one would be able to contradict the uniqueness of solutions for ODEs with boundary conditions $u(a) = u_0(a)$, $u'(a) = u_{00}(a)$.
\end{itemize}

Second, notice that if we denote $Tf := \frac{d}{dx}\left(p(x)\frac{du}{dx}\right) + q(x) u(x)$ defined on the subset $C^2([a,b], \CB)$ of $\L^2([a,b], \lambda, \CB)$ then by an integration by parts identity for any $f, g \in C^2([a,b], \CB)$ we see that 
$(Tf, g) = (Tg, f)$. Thus $T$ is Hermitian, and in particular if $\lambda_1, \lambda_2$ are two eigenvalues, then as we saw above the respective eigenfunctions are orthogonal w.r.t. $\| \cdot \|_2$. In particular there can be only countably many eigenvalues. We can also infer already from here that all eiganvalues have to be real. Yet $T$ is not compact, so we cannot conclude just as yet.

So let us show that one can define an inverse operator $T_K$ to $T$ that is compact and to which we can apply the spectral theorem to get an orthonormal basis of eigenfunctions. Showing that every eigenfunction of $T_K$ is also an eigenfunction of $T$ will then conclude the completeness of the eigenfunction basis and thus the theorem.

To do this assume WLOG that $0$ is not an eigenvalue of $T$ (otherwise we can take some $\lambda_0$ that is not an eigenvalue and relabel $\lambda \to \lambda - \lambda_0$ in the set-up).

Using the remarks above, we can then choose the solutions $u_1(x)$, $u_2(x)$ satisfying boundary conditions either only at $a$ or $b$ respectively to be linearly independent - otherwise they would satisfy both boundary conditions and $0$ would be an eigenvalue. Then we have that $W(x) := u_1(x)u_2'(x) - u_2(x)u_1'(x)$ is non-zero but by a direct computation $(pW)'(x) = 0$ and thus $p(x)W(x)$ is a constant we denote by just $pW$.

Now we define 
\begin{equation*}
K(x,y) = \begin{cases} -u_1(x)u_2(y)/pW & \text{ for } a\leq x \leq y \leq b\\
-u_1(y)u_2(x)/pW & \text{ for } a\leq y \leq x \leq b
\end{cases}
\end{equation*}
Then one can verify that the corresponding integral operator $T_K$ is Hilbert-Schmidt on $\Lc^2([a,b])$. Similarly to the case of Fourier series on the exercise sheet one can further verify that all eigenfunctions of $T$ will necessarily be eigenfunctions of $T_K$, and vice-versa, with eigenvalues given by the inverses of those of $T_K$. This concludes the theorem. It is quite nice how this slightly abstract looking spectral theorem could help us with such concrete problems - indeed, Sturm-Liouville problems were the motivation and inspirations for a large part of spectral theory, so the interplay of physics and mathematics is most certainly a fruitful one. ;)

Thank you!






\end{document}
Although workiman
\end{lemma}g with infinite sums is relatively easy as long as everything is absolutely summable, one has to verify it every time separately. Thus it will be simpler to work with finite approximations. To do this, we first study the approximations a little bit better.

To do this we consider $g_K := f1_{f \geq K}$ with $K > 0$ and $f$ non-negative. We observe that if $f$ is measurable, then so is $g_K$. Further, if $\lambda(f \geq 0) < \infty$, then also $\lambda(g_K \neq 0) < \infty$. Finally,
$S_n(g_K) \leq S_n(f)$ for every $n \in \N$ and thus $g_K$ is integrable, if $f$ is, and satisfies $\int g_K d\lambda \leq \int f d\lambda$.

The first lemma says that the tails of an integrable function are uniformly controlled:

\begin{lemma}[Markov inequality]\label{lem:Markov}
Let $f: \R^n \to \R$ be non-negative and measurable with $\lambda(f \neq 0) < \infty$. Suppose further that $f$ is integrable. 

Then $\lambda(f \geq K) \leq K^{-1}\int f d\lambda$
and hence also $\lambda(f_n \geq K) \leq K^{-1}\int f d\lambda$.
\end{lemma}

\begin{proof}
Looking more closely at the definition of $S(f)$, we have for any $n \in \N$
$$\int fd\lambda \geq S_n(f) \geq S_n(g_K) \geq K \lambda(f \geq 2^{-n}\lceil 2^n K \rceil)$$
and hence 
$$\lambda(f \geq 2^{-n}\lceil 2^n K \rceil) \leq K^{-1}\int f d\lambda. $$
Taking the limit $n \to \infty$, we have that $2^{-n}\lceil 2^n K \rceil \to K$ monotonically, and thus we obtain the claim for $f$. 

For $f_n$ it either follows from the proof or from the fact that $f \geq f_n$ pointwise.
\end{proof}
The next lemma says that in fact we can even uniformly ignore the tails of the sums:

\begin{lemma}[Uniform integrability]\label{lem:UI}
Let $f: \R^n \to \R$ be non-negative and measurable with $\lambda(f \neq 0) < \infty$. Suppose further that $f$ is integrable. 

Then for every $\eps > 0$, we can find $K > 0$ such that for all $n \geq 1$:
$$S_n(f1_{f \geq K}) < \eps$$
and in particular also $\lambda(f 1_{f \geq K}) < \eps$
\end{lemma}

\begin{proof}
We know that for any


We know that $S_n(f) < \infty$ for all $n \geq 1$, let us consider $n = 1$:
$$S_1(f) = \sum_{i \geq 1} i \lambda(f \in [i, i+1)),$$
where the sum contains just positive terms as $f$ is non-negative.

As this is absolutely summable, for all $K$ sufficiently large it holds that
\begin{equation}\label{eq:ui1}
\sum_{i \geq K} i \lambda(f \in [i, i+1)) < \eps/2.
\end{equation}

Now, observe that the RHS is exactly equal to $S_1(g_K)$. Further, from proof of lemma \ref{lem:def}, we have that for any integrable $f$ and for all $m \geq n$, also $|S_n(f) - S_m(f)| \leq 2*2^{-n} \lambda(f \neq 0)$. Applying this to $g_K = f 1_{f \geq K}$ we obtain that for all $n \geq 1$ 
$$S_n(g_K) - S_1(g_K) \leq 2 \lambda(g_K \neq 0) \leq K^{-1}\int g_K d\lambda \leq K^{-1} \int f d\lambda.$$
We can now choose $K$ such that both $K^{-1}\int fd\lambda < \eps/2$ and \eqref{eq:ui1} holds. This concludes the lemma.


\end{proof}

The following lemma will now simplify our life:

\begin{prop}[Approximation by finite sums]
Let $f: \R^n \to \R$ be measurable with $\lambda(f \neq 0) < \infty$ and integrable.
Then in fact we can approximate $\int f d\lambda$ by finite sums of the form 
$$S_n(g_{K_n}) := \sum_{i \leq K_n}i2^{-n}\lambda(f \in [i2^{-n}, (i+1)2^{-n})$$
for some sequence $(K_n)_{n \geq 1}$ of positive integers.
\end{prop}

The main ingredient is the following simple claim:



\newpage
\newpage


First, let us however check that the definition we give matches the natural definition of the integral for simple functions. Indeed, if we have a simple function $f = \sum_{i \geq 1} c_i 1_{E_i}(x)$ \footnote{recall $E_i$ are asked to be disjoint}, it would be natural to define its integral to be just $\sum_{i \geq 1} c_i \lambda(E_i)$, as long as this sum is absolutely convergent. Notice that this sum is exactly equal to $S(f)$ from before (check!). The Lebesgue integral for such $f$ gives exactly this formula.  

\begin{lemma}
Consider a simple function $f = \sum_{i \geq 1} c_i 1_{E_i}(x)$ such that $\sum_{i \geq 1} |c_i|\lambda(E_i) < \infty$ and $\lambda(\bigcup_{i \geq 1} E_i) < \infty$. Then $f$ is integrable and 
$$\int_{\R^n} f(x)d\lambda(x) = S(f) = \sum_{i} c_i\lambda(E_i).$$
\end{lemma}

\begin{proof}
Denote as above $D_{n,i} := [i2^{-n}, (i+1)2^{-n})$. Let us admit the integrability claim, as it is similar to the argument to follow.

So suppose $f$ is integrable and thus all $S_n$ absolutely summable. We have
$$S_n(f) = \sum_{i \in \Z}i2^{-n}\lambda(\cup_{j: c_j \in D_{n,i}}E_j) = \sum_{i \in \Z}i2^{-n}\sum_{j \geq 1} 1_{c_j \in D_i}\lambda(E_j).$$
As everything is absolutely summable we can swap the sums to obtain:
$$S_n(f) = \sum_{j \geq 1} \lambda(E_j) \sum_{i \in \Z} i2^{-n}1_{c_j \in D_{n,i}}.$$
But from this it is clear that
$$ S_n(f) \leq \sum_{i} c_i\lambda(E_i) \leq S_n(f) + 2^{-n}\lambda(\bigcup_{i \geq 1}E_i).$$
We deduce that $S_n(f) \to  \sum_{i} c_i\lambda(E_i)$ and conclude the lemma.
\end{proof}
In particular, this means that for any integrable function $f$ we have that $S_m(f) = \int_{\R^n}f_m(x)d\lambda(x)$ and we are really defining the Lebesgue integral of $f$ as the approximation of Lebesgue integrals for a particular sequence of simple functions $f_n$.

Having defined the integral on $\R^n$, we can define the integral on any Borel-measurable subset $E \subseteq \R^n$. To do this we notice that for a measurable function $f$, also $1_E(x) f(x)$ is measurable as a product of measurable functions. Moreover we can directly see that $S_n(|f|1_E) \leq S_n(|f|)$ and thus if $f$ is integrable, so is $f1_E$.

\begin{defn}[Lebesgue integral on subsets]
Let $\lambda$ be the Lebesgue measure on $(\R^n, \F_B)$ and $f:\R^n \to \R$ be an integrable function with $\lambda(\{f \neq 0\}) < \infty$ and let $E \subseteq \R^n$ be a Borel set.

We then define the integral of $f$ on $E$ by $$\int_E f(x) d\lambda(x) := \int_{\R^n} 1_E(x) f(x) d\lambda(x).$$
\end{defn}

Let us now state some basic but important properties of the Lebesgue integral: 
\begin{prop}
Let $f, g$ be Lebesgue integrable functions such that $\lambda(f \neq 0) < \infty$ and $\lambda(g \neq 0) < \infty$. We have the following properties:
\begin{itemize}
\item Linearity: for $a, b \in \R$, then $$\int (af+bg) d\lambda = a\int f d\lambda + b\int g d\lambda$$
\item Additivity: if $E, F$ are Borel measurable and disjoint, then 
$$\int_{E \cup F} f d\lambda = \int_E f d\lambda + \int_F f d\lambda$$
\item Monotonicity: if $\lambda(f < g) = 0$, then $$\int f d\lambda \geq \int g d\lambda$$
\item Boundedness: if $\lambda(\{f < c\} \cup \{f > C\}) = 0$, then 
$$c\lambda(f \neq 0) \leq \int f d\lambda \leq C\lambda(f \neq 0).$$
\end{itemize}
\end{prop}

The proof is a somewhat tedious verification from the definition and is non-examinable. For those interested, we e will do the first, maybe the most tricky part:\\

\noindent $\star $ \textit{Start of non-examinable section} $\star $\\

\begin{proof}
Let us prove linearity. So let $f, g$ be integrable so that in particular $S_n(|f|), S_n(|g|)$ are finite for all $n \geq 1$.
Let us first prove that $S(f_n) + S(g_n) = S(f_n+g_n).$
By definition 
$$S(f_n + g_n) = \sum_{i \in \Z}i2^{-n}\lambda(f_n + g_n = i2^{-n}).$$
By countable additivity
$$\lambda(\{f_n + g_n = i2^{-n}\}) = \sum_{j \in \Z} \lambda(\{f_n = j2^{-n}\} \cap \{g_n = (i-j)2^{-n}\}).$$
Thus we have
$$S(f_n + g_n) = \sum_{i \in \Z} \sum_{j \in \Z} (j + i-j)2^{-n}\lambda(\{f_n = j2^{-n}\} \cap \{g_n = (i-j)2^{-n}\}),$$
where we for now assume the absolute summability. 
Under this assumption we can also swap the summation
$$S(f_n + g_n) =\sum_{j \in \Z} \sum_{i \in \Z} (j + i-j)2^{-n}\lambda(\{f_n = j2^{-n}\} \cap \{g_n = (i-j)2^{-n}\}).$$
Now, for every fixed $j \in \Z$, let's swap the second summation variable $i \in \Z$ with $i :=k + j$, with $k \in \Z$. We can then write
$$S(f_n + g_n) =\sum_{j \in \Z} \sum_{k \in \Z} (j + k)2^{-n}\lambda(\{f_n = j2^{-n}\} \cap \{g_n = (k2^{-n}\}).$$
Under the absolute summability condition for $S(f_n), S(g_n)$ we can decompose the sum as 
$$\sum_{j \in \Z} \sum_{k \in \Z} j2^{-n}\lambda(\{f_n = j2^{-n}\} \cap \{g_n = k2^{-n}\}) + \sum_{j \in \Z} \sum_{k \in \Z} j2^{-n}\lambda(\{f_n = j2^{-n}\} \cap \{g_n = k2^{-n}\}).$$
But now again by countable additivity 
$$\lambda(f_n = j2^{-n}) = \sum_{k \in \Z}\lambda(\{f_n = j2^{-n}\} \cap \{g_n = k2^{-n}\}), $$
so we recognize the first term as $S(f_n)$. Similarly, after a change in order of summation we  recognize $S(g_n)$ as the second term to obtain $S(f_n + g_n) = S(f_n) + S(g_n)$. 

In the calculation we used absolute summability for $S(f_n + g_n)$, which follows by doing the same calculation with $|f_n|, |g_n|$ instead of $f_n, g_n$. 

Finally, observe that either $(f+g)_n(x) = f_n(x) + g_n(x)$, or $(f+g)_n(x) +2^{-n} = f_n(x) + g_n(x)$. Denote the former event by $E$ and thus the latter is $E^c$.
We can write 
\begin{align*}
S((f+g)_n) &= \sum_{i \in \Z}i2^{-n}\left(\lambda(\{(f+g)_n = i2^{-n}\} \cap E) + \lambda(\{(f+g)_n= i2^{-n}\} \cap E^c)\right) \\ &= \sum_{i \in \Z}i2^{-n}\left(\lambda(\{f_n+g_n = i2^{-n}\} \cap E)+\lambda(\{f_n+g_n = (i+1)2^{-n}\} \cap E^c)\right) \\
&\leq S(f_n+g_n).
\end{align*}
Similarly $$S(f_n+g_n) \leq S((f+g)_n) + 2^{-n}(\lambda(f \neq 0) + \lambda(g \neq 0))$$
and we conclude that $S_n(f+g) = S((f+g)_n)$ and $S(f_n + g_n) = S(f_n) + S(g_n)$ have the same limit as $n \to \infty$, i.e. that $\int(f+g)d\lambda = \int fd\lambda + \int g d\lambda$ as desired.

\end{proof}
\noindent $\star $ \textit{End of non-examinable section} $\star $\\
\subsection{$L^2$ function spaces and Hilbert spaces}
\section{Linear maps on function spaces}
\end{document}